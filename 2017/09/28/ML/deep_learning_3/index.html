<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>深度学习(3)：优化神经网络(1) - Cornfield Chase</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#B481BB"><meta name="application-name" content="Hugsy&#039;s Blog"><meta name="msapplication-TileImage" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><meta name="msapplication-TileColor" content="#B481BB"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hugsy&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="144x144" href="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><meta name="description" content="想提高一个深层神经网络的训练效率，需从各个方面入手，优化整个运算过程，同时预防其中可能发生的各种问题。 本文涉及优化深层神经网络中的数据划分，模型估计，预防过拟合，数据集标准化，权重初始化，梯度检验等内容。"><meta property="og:type" content="blog"><meta property="og:title" content="深度学习(3)：优化神经网络(1)"><meta property="og:url" content="https://hugsy.top/2017/09/28/ML/deep_learning_3/"><meta property="og:site_name" content="Cornfield Chase"><meta property="og:description" content="想提高一个深层神经网络的训练效率，需从各个方面入手，优化整个运算过程，同时预防其中可能发生的各种问题。 本文涉及优化深层神经网络中的数据划分，模型估计，预防过拟合，数据集标准化，权重初始化，梯度检验等内容。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411115030.png"><meta property="article:published_time" content="2017-09-27T16:00:00.000Z"><meta property="article:modified_time" content="2022-12-10T10:02:08.570Z"><meta property="article:author" content="Hugsy"><meta property="article:tag" content="Python"><meta property="article:tag" content="ML"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411115030.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hugsy.top/2017/09/28/ML/deep_learning_3/"},"headline":"深度学习(3)：优化神经网络(1)","image":["https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411115030.png"],"datePublished":"2017-09-27T16:00:00.000Z","dateModified":"2022-12-10T10:02:08.570Z","author":{"@type":"Person","name":"Hugsy"},"description":"想提高一个深层神经网络的训练效率，需从各个方面入手，优化整个运算过程，同时预防其中可能发生的各种问题。 本文涉及优化深层神经网络中的数据划分，模型估计，预防过拟合，数据集标准化，权重初始化，梯度检验等内容。"}</script><link rel="canonical" href="https://hugsy.top/2017/09/28/ML/deep_learning_3/"><link rel="alternate" href="/atom.xml" title="Cornfield Chase" type="application/atom+xml"><link rel="icon" href="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?7b65ce26b5ae8dae153d7b4d53214ba4";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.2"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/logo.svg" alt="Cornfield Chase" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/tags/CS/">CS</a><a class="navbar-item" href="/tags/EE/">EE</a><a class="navbar-item" href="/tags/ML/">ML</a><a class="navbar-item" href="/tags/ME/">ME</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411115030.png" alt="深度学习(3)：优化神经网络(1)"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2017-09-27T16:00:00.000Z" title="9/28/2017, 12:00:00 AM">2017-09-28</time>发表</span><span class="level-item"><time dateTime="2022-12-10T10:02:08.570Z" title="12/10/2022, 6:02:08 PM">2022-12-10</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></span><span class="level-item">25 分钟读完 (大约3731个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">深度学习(3)：优化神经网络(1)</h1><div class="content"><p>想提高一个深层神经网络的训练效率，需从各个方面入手，优化整个运算过程，同时预防其中可能发生的各种问题。</p>
<p>本文涉及优化深层神经网络中的数据划分，模型估计，预防过拟合，数据集标准化，权重初始化，梯度检验等内容。</p>
<span id="more"></span>

<h2 id="数据划分"><a href="#数据划分" class="headerlink" title="数据划分"></a>数据划分</h2><p>想要建立一个神经网络模型，首先，就是要设置好整个数据集中的<strong>训练集（Training Sets）</strong>、<strong>开发集（Development Sets）</strong>和<strong>测试集（Test Sets）</strong>。</p>
<p>采用训练集进行训练时，通过改变几个超参数的值，将会得到几种不同的模型。开发集又称为<strong>交叉验证集（Hold-out Cross Validation Sets）</strong>，它用来找出建立的几个不同模型中表现最好的模型。之后将这个模型运用到测试集上进行测试，对算法的好坏程度做无偏估计。通常，会直接省去最后测试集，将开发集当做“测试集”。一个需要注意的问题是，需要保证训练集和测试集的来源一致，否则会导致最后的结果存在较大的偏差。</p>
<h2 id="模型估计"><a href="#模型估计" class="headerlink" title="模型估计"></a>模型估计</h2><p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411154119.png" alt="分类问题"></p>
<p>如图中的左图，对图中的数据采用一个简单的模型，例如线性拟合，并不能很好地对这些数据进行分类，分类后存在较大的<strong>偏差（Bias）</strong>，称这个分类模型<strong>欠拟合（Underfitting）</strong>。右图中，采用复杂的模型进行分类，例如深度神经网络模型，当模型复杂度过高时变容易出现<strong>过拟合（Overfitting）</strong>，使得分类后存在较大的<strong>方差（Variance）</strong>。中间的图中，采用一个恰当的模型，才能对数据做出一个差不多的分类。</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411154147.png" alt="偏差和方差"></p>
<p>通常采用开发集来诊断模型中是否存在偏差或者时方差：</p>
<ul>
<li>当训练出一个模型后，发现训练集的错误率较小，而开发集的错误率却较大，这个模型可能出现了过拟合，存在较大方差;</li>
<li>发现训练集和开发集的错误率都都较大时，且两者相当，这个模型就可能出现了欠拟合，存在较大偏差；</li>
<li>发现训练集的错误率较大时，且开发集的错误率远较训练集大时，这个模型就有些糟糕了，方差和偏差都较大。</li>
<li>只有当训练集和开发集的错误率都较小，且两者的相差也较小，这个模型才会是一个好的模型，方差和偏差都较小。</li>
</ul>
<p>模型存在较大偏差时，可采用增加神经网络的隐含层数、隐含层中的节点数，训练更长时间等方法来预防欠拟合。而存在较大方差时，则可以采用引入更多训练样本、对样本数据<strong>正则化（Regularization）</strong>等方法来预防过拟合。</p>
<h2 id="预防过拟合"><a href="#预防过拟合" class="headerlink" title="预防过拟合"></a>预防过拟合</h2><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>向Logistic回归的成本函数中加入L2正则化（也称“L2范数”）项：$${J(w,b) = \frac{1}{m} \sum_{i=1}^m \mathcal{L}({\hat y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}||w||_2^2}$$<br>其中：$$||w||_2^2 = \sum_{j=1}^n{w_j^2} = w^T w$$<br>L2正则化是最常用的正则化类型,也存在L1正则化项：$$\frac{\lambda}{m}||w||_1 = \frac{\lambda}{m}\sum_{j=1}^n |w_j|$$<br>由于L1正则化最后得到w向量中将存在大量的0，使模型变得稀疏化,所以一般都使用L2正则化。其中的参数$\lambda$称为正则化参数，这个参数通常通过开发集来设置。<br>向神经网络的成本函数加入正则化项：<br>$${J(w^{[1]},b^{[1]},…,w^{[L]},b^{[L]}) = \frac{1}{m} \sum_{i=1}^m \mathcal{L}({\hat y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m} \sum\limits_{l=1}^L ||w^{[l]}}||^2_F$$</p>
<p>因为w是一个$n^{[l]} \times n^{[l-1]}$矩阵所以：$${||w^{[l]}||^2_F = \sum\limits_{i=1}^{n^{[l-1]}} \sum\limits_{j=1}^{n^{[l]}} (w_{ij}^{[l]})^2}$$<br>这被称为<strong>弗罗贝尼乌斯范数（Frobenius Norm）</strong>,所以神经网络的中的正则化项被称为弗罗贝尼乌斯范数矩阵。</p>
<p>加入正则化项后，反向传播时：$$dw^{[l]} = \frac{\partial \mathcal{L}}{\partial w^{[l]}} + \frac{\lambda}{m} w^{[l]}$$<br>更新参数时：$$ w^{[l]} := w^{[l]} - \alpha \frac{\partial \mathcal{L}}{\partial w^{[l]}} - \alpha \frac{\lambda}{m} w^{[l]}$$<br>有了新定义的$dw^{[l]}$，参数$w^{[L]}$在更新时会多减去一项$\alpha \frac{\lambda}{m} w^{[l]}$，所以L2正则化项也被称为<strong>权值衰减（Weight Decay）</strong>。</p>
<p>参数$\lambda$用来调整式中两项的相对重要程度，较小$\lambda$偏向于最后使原本的成本函数最小化，较大的$\lambda$偏向于最后使权值$w$最小化。当$\lambda$较大时，权值$w^{[L]}$便会趋近于$0$，相当于消除深度神经网络中隐藏单元的部分作用。另一方面，在权值$w^{[L]}$变小之下，输入样本$X$随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。</p>
<h3 id="随机失活正则化"><a href="#随机失活正则化" class="headerlink" title="随机失活正则化"></a>随机失活正则化</h3><p><strong>随机失活（DropOut）</strong>正则化，就是在一个神经网络中对每层的每个节点预先设置一个被消除的概率，之后在训练随机决定将其中的某些节点给消除掉，得到一个被缩减的神经网络，以此来到达降低方差的目的。DropOut正则化较多地被使用在<strong>计算机视觉（Computer Vision）</strong>领域。</p>
<p>使用Python编程时可以用<strong>反向随机失活（Inverted DropOut）</strong>来实现DropOut正则化：</p>
<p>对于一个神经网络第3层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keep.prob = <span class="number">0.8</span></span><br><span class="line">d3 = np.random.randn(a3.shape[<span class="number">0</span>],a3.shape[<span class="number">1</span>]) &lt; keep.prob</span><br><span class="line">a3 = np.multiply(a3,d3)</span><br><span class="line">a3 /= keep.prob</span><br><span class="line">z4 = np.dot(w4,a3) + b4</span><br></pre></td></tr></table></figure>
<p>其中的d3是一个随机生成，与第3层大小相同的的布尔矩阵，矩阵中的值为0或1。而keep.prob ≤ 1，它可以随着各层节点数的变化而变化，决定着失去的节点的个数。</p>
<p>例如，将keep.prob设置为0.8时，矩阵d3中便会有20%的值会是0。而将矩阵a3和d3相乘后，便意味着这一层中有20%节点会被消除。需要再除以keep_prob的原因，是因为后一步求z4中将用到a3，而a3有20%的值被清零了，为了不影响下一层z4的最后的预期输出值，就需要这个步骤来修正损失的值，这一步就称为反向随机失活技术，它保证了a3的预期值不会因为节点的消除而收到影响，也保证了进行测试时采用的是DropOut前的神经网络。</p>
<p>与之前的L2正则化一样，利用DropOut，可以简化神经网络的部分结构，从而达到预防过拟合的作用。另外，当输入众多节点时，每个节点都存在被删除的可能，这样可以减少神经网络对某一个节点的依赖性，也就是对某一特征的依赖，扩散输入节点的权重，收缩权重的平方范数。</p>
<h3 id="数据扩增法"><a href="#数据扩增法" class="headerlink" title="数据扩增法"></a>数据扩增法</h3><p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411154213.png" alt="数据扩增法"></p>
<p><strong>数据扩增（Data Augmentation）</strong>是在无法获取额外的训练样本下，对已有的数据做一些简单的变换。例如对一张图片进行翻转、放大扭曲，以此引入更多的训练样本。</p>
<h3 id="早停止法"><a href="#早停止法" class="headerlink" title="早停止法"></a>早停止法</h3><p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411154226.png" alt="早停止法"></p>
<p><strong>早停止（Early Stopping）</strong>是分别将训练集和开发集进行梯度下降时成本变化曲线画在同一个坐标轴内，在箭头所指两者开始发生较大偏差时就及时进行纠正，停止训练。在中间箭头处，参数w将是一个不大不小的值，理想状态下就能避免过拟合的发生。然而这种方法一方面没有很好得降低成本函数，又想以此来避免过拟合，一个方法解决两个问题，哪个都不能很好解决。</p>
<h2 id="标准化数据集"><a href="#标准化数据集" class="headerlink" title="标准化数据集"></a>标准化数据集</h2><p>对训练及测试集进行标准化的过程为：$$ \bar{x} = \frac{1}{m} \sum_{i=1}^m x^{(i)} $$ $$ x^{(i)} := x^{(i)} - \bar{x} $$ $$ \sigma^2 = \frac{1}{m} \sum_{i=1}^m {x^{(i)}}^2$$ $$x^{(i)}:= \frac{x^{(i)}}{\sigma^2} $$<br>原始数据为：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411154245.png" alt="原始数据"></p>
<p>经过前两步，x减去它们的平均值后：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411154311.png" alt="减去平均值"></p>
<p>经过后两步，x除以它们的方差后：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411154331.png" alt="除以方差"></p>
<p>数据集未进行标准化时，成本函数的图像及梯度下降过程将是：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411154351.png" alt="未进行标准化"></p>
<p>而标准化后，将变为：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411154408.png" alt="标准化后"></p>
<h2 id="初始化权重"><a href="#初始化权重" class="headerlink" title="初始化权重"></a>初始化权重</h2><p>在之前的建立神经网络的过程中，提到权重w不能为0，而将它初始化为一个随机的值。然而在一个深层神经网络中，当w的值被初始化过大时，进入深层时呈指数型增长，造成<strong>梯度爆炸</strong>；过小时又会呈指数级衰减，造成<strong>梯度消失</strong>。</p>
<p>Python中将w进行随机初始化时，使用numpy库中的np.random.randn()方法，randn是从均值为0的单位标准正态分布（也称“高斯分布”）进行取样。随着对神经网络中的某一层输入的数据量n的增长，输出数据的分布中，方差也在增大。结果证明，可以除以输入数据量n的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了，不会过大导致到指数级爆炸或过小而指数级衰减。也就是将权重初始化为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn(layers_dims[l],layers_dims[l-1]) \* np.sqrt(1.0/layers_dims[l-1])</span><br></pre></td></tr></table></figure>

<p>这样保证了网络中所有神经元起始时有近似同样的输出分布。<br>当激活函数为ReLU函数时，权重最好初始化为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn(layers_dims[l],layers_dims[l-1]) \* np.sqrt(2.0/layers_dims[l-1])</span><br></pre></td></tr></table></figure>
<p>以上结论的证明过程见参考资料。</p>
<h2 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h2><p>梯度检验的实现原理，是根据导数的定义，对成本函数求导，有：$$ J’(\theta) = \frac{\partial J(\theta)}{\partial \theta}= \lim_{\epsilon\rightarrow 0}\frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}$$</p>
<p>则梯度检验公式：$$J’(\theta) = \frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}$$</p>
<p>其中当$\epsilon$越小时，结果越接近真实的导数也就是梯度值。可以使用这种方法，来判断反向传播进行梯度下降时，是否出现了错误。</p>
<p>梯度检验的过程，是对成本函数的每个参数$\theta_{[i]}$加入一个很小的$\epsilon$，求得一个梯度逼近值$d\theta_{approx[i]}$：<br>$$d\theta_{approx[i]} = \frac{J(\theta_{[1]},\theta_{[2]},…,\theta_{[i]}+\epsilon)-J(\theta_{[1]},\theta_{[2]},…,\theta_{[i]}-\epsilon)}{2\epsilon}$$</p>
<p>以解析方式求得$J’(\theta)$在$\theta$时的梯度值$d\theta$,进而再求得它们之间的欧几里得距离：<br>$$\frac{||d\theta_{approx[i]}-d\theta||_2}{||d \theta_{approx[i]}||_2+||dθ||_2}$$</p>
<p>其中$||x||_2$表示向量x的2范数（各种范数的定义见参考资料）：<br>$$||x||_2 = \sum\limits_{i=1}^N |x_i|^2$$</p>
<p>当计算的距离结果与$\epsilon$的值相近时，即可认为这个梯度值计算正确，否则就需要返回去检查代码中是否存在bug。</p>
<p>需要注意的是，不要在训练模型时进行梯度检验，当成本函数中加入了正则项时，也需要带上正则项进行检验，且不要在使用随机失活后使用梯度检验。</p>
<h2 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h2><h3 id="三种参数初始化"><a href="#三种参数初始化" class="headerlink" title="三种参数初始化"></a>三种参数初始化</h3><p>1.zeros\random\Xavier初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#参数w,b初始化为0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters_zeros</span>(<span class="params">layers_dims</span>):</span><br><span class="line"></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(layers_dims)           </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)] = np.zeros((layers_dims[l],layers_dims[l-<span class="number">1</span>]))</span><br><span class="line">        parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="comment">#参数w初始化为random</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters_random</span>(<span class="params">layers_dims</span>):</span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">3</span>)              </span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(layers_dims)           </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)] = np.random.randn(layers_dims[l],layers_dims[l-<span class="number">1</span>]) * <span class="number">10</span></span><br><span class="line">        parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="comment">#参数w进行Xavier初始化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters_he</span>(<span class="params">layers_dims</span>):</span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(layers_dims) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)] = np.random.randn(layers_dims[l],layers_dims[l-<span class="number">1</span>]) * np.sqrt(<span class="number">1.0</span>/(layers_dims[l-<span class="number">1</span>]))</span><br><span class="line">        parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p>2.效果比较</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layers = [train_X.shape[<span class="number">0</span>], <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>] <span class="comment">#设置神经网络层数及节点数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#参数初始化为0</span></span><br><span class="line">parameters_init_zeros = model(train_X, train_Y, layers_dims = layers, initialization = <span class="string">&quot;zeros&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化random</span></span><br><span class="line">parameters_init_random = model(train_X, train_Y, layers_dims = layers, initialization = <span class="string">&quot;random&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Xavier初始化</span></span><br><span class="line">parameters_init_he = model(train_X, train_Y, layers_dims = layers, initialization = <span class="string">&quot;he&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结果为：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411154435.png" alt="zeros\random\Xavier初始化"></p>
<h3 id="无-L2-DropOut正则化"><a href="#无-L2-DropOut正则化" class="headerlink" title="无\L2\DropOut正则化"></a>无\L2\DropOut正则化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layers = [train_x.shape[<span class="number">0</span>], <span class="number">20</span>, <span class="number">3</span>, <span class="number">1</span>] <span class="comment">#设置神经网络层数及节点数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#不进行正则化</span></span><br><span class="line">parameters_no_reg = model(train_x, train_y, layers_dims = layers, learning_rate = <span class="number">0.3</span>, num_iterations = <span class="number">30000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#采用L2正则化</span></span><br><span class="line">parameters_L2_reg = model(train_x, train_y, layers_dims = layers, learning_rate = <span class="number">0.3</span>, num_iterations = <span class="number">30000</span>, lambd = <span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#采用DropOut正则化</span></span><br><span class="line">parameters_dropout_reg = model(train_x, train_y, layers_dims = layers, learning_rate = <span class="number">0.3</span>, num_iterations = <span class="number">30000</span>, keep_prob = <span class="number">0.86</span>)</span><br></pre></td></tr></table></figure>
<p>结果为：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411154503.png" alt="无\L2\DropOut正则化"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="http://mooc.study.163.com/course/deeplearning_ai-2001281003#/info">吴恩达-改善深层神经网络-网易云课堂</a></li>
<li><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/deep-neural-network/">Andrew Ng-Improving Deep Neural Networks-Coursera</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit">神经网络权重初始化-知乎专栏</a></li>
<li><a target="_blank" rel="noopener" href="https://zhihu.com/question/20473040/answer/102907063">范数的定义-知乎回答</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Hugsy19/Machine_Learning">课程代码与资料-GitHub</a></li>
</ol>
<p>注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。</p>
<hr>
<p>更新历史：</p>
<ul>
<li>2017.10.02 完成初稿</li>
<li>2018.02.13 调整部分内容</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>深度学习(3)：优化神经网络(1)</p><p><a href="https://hugsy.top/2017/09/28/ML/deep_learning_3/">https://hugsy.top/2017/09/28/ML/deep_learning_3/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hugsy</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2017-09-28</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2022-12-10</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Python/">Python</a><a class="link-muted mr-2" rel="tag" href="/tags/ML/">ML</a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210410180700.png" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210410180901.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2017/10/06/ML/deep_learning_4/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">深度学习(4)：优化神经网络(2)</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2017/09/25/ML/deep_learning_2/"><span class="level-item">深度学习(2)：神经网络</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "e1eaee115c4856c590c689ad17fa9025",
            repo: "Picbed",
            owner: "Hugsy19",
            clientID: "a321e46c668dc3d86c4f",
            clientSecret: "0ff9e81865b09e7511ef7ef17bb9596834b8fd95",
            admin: ["Hugsy19"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            proxy: "https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token",
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/logo.svg" alt="Cornfield Chase" height="28"></a><p class="is-size-7"><span>&copy; 2022 Hugsy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>