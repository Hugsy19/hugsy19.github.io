<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>深度学习(1)：Logistic回归 - Cornfield Chase</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#B481BB"><meta name="application-name" content="Hugsy&#039;s Blog"><meta name="msapplication-TileImage" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><meta name="msapplication-TileColor" content="#B481BB"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hugsy&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="144x144" href="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><meta name="description" content="深度学习（deep learning）是机器学习的一大分支，它试图在机器上建立模仿人脑机制进行分析学习的神经网络，赋予机器解释图像、声音、文本等数据的能力。 这里，首先介绍深度学习中的最基础的学习算法–Logistic回归。"><meta property="og:type" content="blog"><meta property="og:title" content="深度学习(1)：Logistic回归"><meta property="og:url" content="https://hugsy.top/2017/09/12/ML/deep_learning_1/"><meta property="og:site_name" content="Cornfield Chase"><meta property="og:description" content="深度学习（deep learning）是机器学习的一大分支，它试图在机器上建立模仿人脑机制进行分析学习的神经网络，赋予机器解释图像、声音、文本等数据的能力。 这里，首先介绍深度学习中的最基础的学习算法–Logistic回归。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411115030.png"><meta property="article:published_time" content="2017-09-11T16:00:00.000Z"><meta property="article:modified_time" content="2022-12-10T10:02:16.706Z"><meta property="article:author" content="Hugsy"><meta property="article:tag" content="ML"><meta property="article:tag" content="Python"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411115030.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hugsy.top/2017/09/12/ML/deep_learning_1/"},"headline":"深度学习(1)：Logistic回归","image":["https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411115030.png"],"datePublished":"2017-09-11T16:00:00.000Z","dateModified":"2022-12-10T10:02:16.706Z","author":{"@type":"Person","name":"Hugsy"},"description":"深度学习（deep learning）是机器学习的一大分支，它试图在机器上建立模仿人脑机制进行分析学习的神经网络，赋予机器解释图像、声音、文本等数据的能力。 这里，首先介绍深度学习中的最基础的学习算法–Logistic回归。"}</script><link rel="canonical" href="https://hugsy.top/2017/09/12/ML/deep_learning_1/"><link rel="alternate" href="/atom.xml" title="Cornfield Chase" type="application/atom+xml"><link rel="icon" href="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?7b65ce26b5ae8dae153d7b4d53214ba4";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.2"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/logo.svg" alt="Cornfield Chase" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/tags/CS/">CS</a><a class="navbar-item" href="/tags/EE/">EE</a><a class="navbar-item" href="/tags/ML/">ML</a><a class="navbar-item" href="/tags/ME/">ME</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411115030.png" alt="深度学习(1)：Logistic回归"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2017-09-11T16:00:00.000Z" title="9/12/2017, 12:00:00 AM">2017-09-12</time>发表</span><span class="level-item"><time dateTime="2022-12-10T10:02:16.706Z" title="12/10/2022, 6:02:16 PM">2022-12-10</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></span><span class="level-item">30 分钟读完 (大约4517个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">深度学习(1)：Logistic回归</h1><div class="content"><p><strong>深度学习（deep learning）</strong>是机器学习的一大分支，它试图在机器上建立模仿人脑机制进行分析学习的神经网络，赋予机器解释图像、声音、文本等数据的能力。</p>
<p>这里，首先介绍深度学习中的最基础的学习算法–Logistic回归。</p>
<span id="more"></span>
<h2 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h2><p>Logistic回归常用于解决<strong>二分类（Binary Classification）</strong>问题。在二分分类问题中，对于某个输入，输出的结果是离散的值。</p>
<p>例如：想要构建一个猫图分类器，即输入一张图片，希望该分类器准确判断出该图片是否是一张猫图，并输出它的预测结果。</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411120619.png" alt="猫图"></p>
<p>首先，考虑这个猫图分类器的输入。图片是一类非结构化的数据，一张图片在计算机中以RGB编码时，是以红、绿、蓝为三基色，每个像素点上三基色对应的量的多少（即“亮度”）编码为数据进行存储。那么在计算机上，一张图片就可以由大小与图片一致的三个矩阵来表示，三个矩阵分别表示各颜色通道，矩阵中的值则表示各像素值。由此，上图中的猫图大小为$64 \times 64$，便可以表示为三个大小为$64\times64$的矩阵。</p>
<p>然而，在<strong>模式识别（Pattern Recognition）</strong>以及机器学习中，对于处理的各种类型的数据，通常采用一些特征向量来表示。简单地将一张猫图表示为一个特征向量，可以直接把三个矩阵进行拆分重塑，最终形成维数$n_x = 64 \times 64 \times 3 = 12288 $的一个向量$x$:</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411120637.png" alt="特征向量"></p>
<p>其次，实现这个分类器，需要准备大量的猫图及少量的非猫图，并取其中大部分组成该分类器的<strong>训练样本</strong>，少部分组成<strong>测试样本</strong>。将这些样本都以上述的方式表示为特征向量的形式，一个样本由一对$(x,y)$进行表示，其中x为$n_x$维的特征向量，$y$是该特征向量的<strong>标签（Label）</strong>，根据该特征向量表示的是猫或非猫，取值为$0$或$1$。如果有$m$个训练样本对，它们将被表示为：$$ {(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})} $$</p>
<p>更进一步，可以用矩阵的形式将我们的数据表示得更为紧凑。训练集中所有特征向量$x^{(1)}$、$x^{(2)}$…以及它们的标签$y^{(1)}$、$y^{(2)}$…分别进行列组合，变成两个矩阵$X$、$Y$：$$ {X = [x^{(1)},x^{(2)},…,x^{(m)}]}$$ $$ {Y = [y^{(1)},y^{(2)},…,y^{(m)}]}$$<br>这样，$X$会是个大小为$n_x \times m$的矩阵，$Y$是个大小为$1 \times m$的矩阵。</p>
<h2 id="Logistic回归模型"><a href="#Logistic回归模型" class="headerlink" title="Logistic回归模型"></a>Logistic回归模型</h2><p>Logistic回归是一种用于解决<strong>监督学习（Supervised Learning）</strong>问题的学习算法。进行Logistic回归的目的，是使训练数据的标签值与预测出来的值之间的误差最小化。建立猫图分类器的Logistic回归模型过程如下：</p>
<p>猫图分类器中，要实现的是：对于给定的以$n_x$维特征向量$x$表示、标签为$y$的一张图片，估计出这张图片为猫图的概率$\hat{y}$，即：$$\hat{y} = p(y = 1|x), 0 \le \hat{y} \le 1$$</p>
<p>有大量猫图的数据时，考虑采用<strong>线性拟合</strong>的方法，来找到一个$\hat{y}$关于$x$的函数，从而实现这个猫分类器。规定一个$n_x$维向量$w$和一个值$b$作为参数，可得到线性回归的表达式：$${\hat{y} = w^Tx + b}$$</p>
<p>由于$\hat{y}$为概率值，取值范围为$[0,1]$，简单地进行线性拟合，得出的$\hat{y}$可能非常大，还可能为负值。此时，就需要用一个<strong>Logistic回归单元</strong>来对其值域进行约束。这里以<strong>sigmoid函数</strong>为逻辑回归单元，其表达式为：$$\sigma{(z)} = \frac{1}{1+e^{-z}}$$</p>
<p>函数图像为：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411120652.png" alt="sigmoid函数"></p>
<p>从中可以看出，sigmoid函数具有如下性质：</p>
<ul>
<li>当$z$趋近于正无穷大时，$\sigma{(z)} = 1$;</li>
<li>当$z$趋近于负无穷大时，$\sigma{(z)} = 0$;</li>
<li>当$z = 0$时，$\sigma{(z)} = 0.5$。</li>
</ul>
<p>所以可以用sigmoid函数来约束$\hat{y}$的值域，得到该分类器的Logistic回归模型：$${ \hat{y} = σ(w^Tx + b) = \frac{1}{1+e^{-(w^Tx + b)}}}$$</p>
<p>建立好Logistic回归模型后，接下来要考虑的是如何利用我们的训练集数据，找到该模型中的两个参数$w$和$b$的最优解。</p>
<h2 id="成本函数"><a href="#成本函数" class="headerlink" title="成本函数"></a>成本函数</h2><p>为了训练逻辑回归模型中的参数w和b，使得输出值$\hat{y}$与真实值y尽可能一致，即尽可能准确地判断一张图是否为猫，需要定义一个<strong>成本函数（Cost Function）</strong>作为衡量的标准。</p>
<p>单个样本的预测值（${\hat y}^{(i)}$）与其真实值（$y^{(i)}$）之间的误差大小用<strong>损失函数（Loss Function）</strong>来衡量。<strong>平方误差（Square Loss）</strong>就是一种常用的损失函数，其表达式为：$${\mathcal{L}(\hat y,y)=\frac{1}{2}(\hat y-y)^2}$$</p>
<p>但Logistic回归中，一般不采用这个损失函数，因为在训练参数过程中，使用这个损失函数将得到一个非凸函数而存在很多个局部最优解，使得后面可能无法使用<strong>梯度下降（Gradient Descent）</strong>来得到我们想要的最优解。</p>
<p>对这个Logistic回归模型，希望能够满足如下条件概率：<br>$$p(y|x) = \begin{cases} \hat{y},  &amp; \text{$(y=1)$} \\ 1 - \hat{y}, &amp; \text{$(y=0)$} \end{cases}$$</p>
<p>将上下两个式子合二为一，可写成：$$p(y|x) = \hat{y}^y (1 - \hat{y})^{(1 - y)} $$</p>
<p>对两边取对数，进一步化简为：$$log\ p(y|x) = ylog\ \hat y+(1-y)log\ (1-\hat y) $$</p>
<p>我们希望$p(y|x)$的值越大越好，而损失越小越好。所以为上式添上负号，就可以将它作为一个损失函数，这便是应用很广的<strong>交叉熵（Cross Entropy）</strong>损失函数，表达式为：$${\mathcal{L}(\hat y,y)=-[ylog\ \hat y+(1-y)log\ (1-\hat y)]}$$</p>
<p>交叉熵损失函数有如下性质：</p>
<ul>
<li>当$y^{(i)}=1$时，${\mathcal{L}({\hat y}^{(i)},y^{(i)})=-log\ {\hat y}^{(i)}}$，意味着损失越小，$\hat{y}$越接近于1；</li>
<li>当$y^{(i)}=0$时，${\mathcal{L}({\hat y}^{(i)},y^{(i)})=-log\ (1-{\hat y}^{(i)})}$，意味着损失越小，$\hat{y}$越接近0。</li>
</ul>
<p>对m个训练样本整体的成本函数，可以使用数理统计中的参数估计方法之一–<strong>最大似然估计法（Maximum Likelihood Estimation）</strong>推导出来的。</p>
<p>假设所有训练样本独立同分布，则它们的联合概率为所有样本概率的乘积，得到似然函数为:<br>$$P (x) = \prod_{i=1}^m p(y^{(i)}|x^{(i)})$$</p>
<p>两边取对数有：<br>$$log\ P(x) = \sum_{i=1}^m log\ p(y^{(i)}|x^{(i)}) = - \sum_{i=1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)})$$</p>
<p>最大似然估计中的下一步，是求解出一组使得上式取得最大值的 参数。而在这里，由于我们训练模型时，目标是使成本函数最小化，所以不直接使用上式作为成本函数，而将其负号去掉，同时为方便处理数据，而乘上一个常数$1/m$对式子适当进行放缩，最后得到下面的成本函数：<br>$${J(w,b) = \frac{1}{m} \sum_{i=1}^m \mathcal{L}({\hat y}^{(i)}, y^{(i)}) = - \frac{1}{m} \sum_{i=1}^m [ylog\ \hat y+(1-y)log\ (1-\hat y)]}$$ </p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>要找到参数的最优解，一般采用梯度下降法。</p>
<p>标量场中某一函数上某一点的梯度，指向该函数在该点处增长最快的方向，函数在该点处沿着该方向变化最快且变化率最大。</p>
<p>在空间坐标中以$w$，$b$为轴，画出的损失函数$J$的图像将类似于下图，要找到函数取得最小值的最优参数，先为$w$和$b$赋一个初始值，正如下图的最上面的红点。</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411120752.png" alt="J(w,b)图像"></p>
<p>对Logistic回归模型，因为成本函数是凸函数，无论参数的初始值是多少，最后总能到达同一个点或大致相同的点，所以几乎任何初始化方法都有效，于是通常将参数直接初始化为0。</p>
<p>所谓的梯度下降，就是从起始点开始，试图每次都沿最陡峭的下降方向下坡，尽可能快地到达最低点即凸函数取得最小值的点，而下坡的方向便是各点上的梯度值。</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411120806.png" alt="J(w,b)二维图像"></p>
<p>从一个二维上的图像看，函数的导数方向即为梯度方向，下降速度最快。即每经过一次梯度下降，参数$w$、$b$即更新为：$${w = w-\alpha\frac{\partial J(w,b)}{\partial w}}$$<br>$${b = b-\alpha\frac{\partial J(w,b)}{\partial b}}$$</p>
<p>式中的$\alpha$被称为学习率，通常为一个小于1的值，用于控制梯度下降过程中每一次移动的规格大小，好比于下降时每一次迈步的大小。$\alpha$的不宜太小也不宜过大：太小会使迭代次数增加，容易陷入局部最优解；太大容易错过最优解。</p>
<p>由梯度下降法，由训练数据经过多次迭代，就可以求得使成本函数的取得最小值的参数$w$和$b$，从而建立好Logistic模型，实现我们想要的猫图分类器。</p>
<h2 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h2><p>使用Python编写一个逻辑回归分类器来识别猫，以此来了解如何使用神经网络的思维方式来进行这项任务，识别过程如图：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411120822.png" alt="识别过程示意图"></p>
<h3 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h3><p>其中用到的Python包有：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://www.numpy.org/">numpy</a>是使用Python进行科学计算的基础包。</li>
<li><a target="_blank" rel="noopener" href="http://matplotlib.org/">matplotlib</a>是Python中著名的绘图库。</li>
<li><a target="_blank" rel="noopener" href="http://www.h5py.org/">h5py</a>在Python提供读取HDF5二进制数据格式文件的接口，本次的训练及测试图片集是以HDF5储存的。</li>
<li><a target="_blank" rel="noopener" href="http://www.pythonware.com/products/pil/">PIL</a>(Python Image Library)为Python提供图像处理功能。</li>
<li><a target="_blank" rel="noopener" href="https://www.scipy.org/">scipy</a>基于NumPy来做高等数学、信号处理、优化、统计和许多其它科学任务的拓展库。<br>几个Python包的安装及基本使用方法详见官网。</li>
</ul>
<p>1.导入要用到的所有包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入用到的包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br></pre></td></tr></table></figure>
<p>2.导入数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_dataset</span>():</span><br><span class="line">    train_dataset = h5py.File(<span class="string">&quot;train_cat.h5&quot;</span>,<span class="string">&quot;r&quot;</span>) <span class="comment">#读取训练数据，共209张图片</span></span><br><span class="line">    test_dataset = h5py.File(<span class="string">&quot;test_cat.h5&quot;</span>, <span class="string">&quot;r&quot;</span>) <span class="comment">#读取测试数据，共50张图片</span></span><br><span class="line"></span><br><span class="line">    train_set_x_orig = np.array(train_dataset[<span class="string">&quot;train_set_x&quot;</span>][:]) <span class="comment">#原始训练集（209*64*64*3）</span></span><br><span class="line">    train_set_y_orig = np.array(train_dataset[<span class="string">&quot;train_set_y&quot;</span>][:]) <span class="comment">#原始训练集的标签集（y=0非猫,y=1是猫）（209*1）</span></span><br><span class="line"></span><br><span class="line">    test_set_x_orig = np.array(test_dataset[<span class="string">&quot;test_set_x&quot;</span>][:]) <span class="comment">#原始测试集（50*64*64*3</span></span><br><span class="line">    test_set_y_orig = np.array(test_dataset[<span class="string">&quot;test_set_y&quot;</span>][:]) <span class="comment">#原始测试集的标签集（y=0非猫,y=1是猫）（50*1）</span></span><br><span class="line"></span><br><span class="line">    train_set_y_orig = train_set_y_orig.reshape((<span class="number">1</span>,train_set_y_orig.shape[<span class="number">0</span>])) <span class="comment">#原始训练集的标签集设为（1*209）</span></span><br><span class="line">    test_set_y_orig = test_set_y_orig.reshape((<span class="number">1</span>,test_set_y_orig.shape[<span class="number">0</span>])) <span class="comment">#原始测试集的标签集设为（1*50）</span></span><br><span class="line"></span><br><span class="line">    classes = np.array(test_dataset[<span class="string">&quot;list_classes&quot;</span>][:])</span><br><span class="line">    <span class="keyword">return</span> train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes</span><br></pre></td></tr></table></figure>
<p>需要说明的是，本次的训练及测试图片集是以HDF5格式储存的,train_cat.h5、test_cat.h5文件打开后结构如下：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411121647.png" alt="h5文件结构"></p>
<p>另外，也可以调用以下方法来查看训练集或测试集中的图片：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#显示图片</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image_show</span>(<span class="params">index,dataset</span>):</span><br><span class="line">    index = index</span><br><span class="line">    <span class="keyword">if</span> dataset == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">        plt.imshow(train_set_x_orig[index])</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;y = &quot;</span> + <span class="built_in">str</span>(train_set_y[:, index]) + <span class="string">&quot;, 它是一张&quot;</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class="string">&quot;utf-8&quot;</span>) +  <span class="string">&quot;&#x27; 图片。&quot;</span>)</span><br><span class="line">    <span class="keyword">elif</span> dataset == <span class="string">&quot;test&quot;</span>:</span><br><span class="line">        plt.imshow(test_set_x_orig[index])</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;y = &quot;</span> + <span class="built_in">str</span>(test_set_y[:, index]) + <span class="string">&quot;, 它是一张&quot;</span> + classes[np.squeeze(test_set_y[:, index])].decode(<span class="string">&quot;utf-8&quot;</span>) +  <span class="string">&quot;&#x27; 图片。&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>3.sigmoid函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sigmoid函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<p>4.初始化参数w，b</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化参数w,b</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_with_zeros</span>(<span class="params">dim</span>):</span><br><span class="line">    w = np.zeros((dim,<span class="number">1</span>)) <span class="comment">#w为一个dim*1矩阵</span></span><br><span class="line">    b = <span class="number">0</span>    </span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure>
<p>5.计算Y_hat,成本函数J以及dw，db</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算Y_hat,成本函数J以及dw，db</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">propagate</span>(<span class="params">w, b, X, Y</span>):</span><br><span class="line">    m = X.shape[<span class="number">1</span>] <span class="comment">#样本个数</span></span><br><span class="line">    Y_hat = sigmoid(np.dot(w.T,X)+b)                                     </span><br><span class="line">    cost = -(np.<span class="built_in">sum</span>(np.dot(Y,np.log(Y_hat).T)+np.dot((<span class="number">1</span>-Y),np.log(<span class="number">1</span>-Y_hat).T)))/m <span class="comment">#成本函数</span></span><br><span class="line"></span><br><span class="line">    dw = (np.dot(X,(Y_hat-Y).T))/m</span><br><span class="line">    db = (np.<span class="built_in">sum</span>(Y_hat-Y))/m</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost) <span class="comment">#压缩维度    </span></span><br><span class="line">    grads = &#123;<span class="string">&quot;dw&quot;</span>: dw,</span><br><span class="line">             <span class="string">&quot;db&quot;</span>: db&#125; <span class="comment">#梯度</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure>
<p>6.梯度下降找出最优解</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度下降找出最优解</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">optimize</span>(<span class="params">w, b, X, Y, num_iterations, learning_rate, print_cost = <span class="literal">False</span></span>):<span class="comment">#num_iterations-梯度下降次数 learning_rate-学习率，即参数ɑ</span></span><br><span class="line">    costs = [] <span class="comment">#记录成本值</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations): <span class="comment">#循环进行梯度下降</span></span><br><span class="line">        grads, cost = propagate(w,b,X,Y)</span><br><span class="line">        dw = grads[<span class="string">&quot;dw&quot;</span>]</span><br><span class="line">        db = grads[<span class="string">&quot;db&quot;</span>]</span><br><span class="line"></span><br><span class="line">        w = w - learning_rate*dw</span><br><span class="line">        b = b - learning_rate*db</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>: <span class="comment">#每100次记录一次成本值</span></span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>: <span class="comment">#打印成本值</span></span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;循环%i次后的成本值: %f&quot;</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    params = &#123;<span class="string">&quot;w&quot;</span>: w,</span><br><span class="line">              <span class="string">&quot;b&quot;</span>: b&#125; <span class="comment">#最终参数值</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">&quot;dw&quot;</span>: dw,</span><br><span class="line">             <span class="string">&quot;db&quot;</span>: db&#125;<span class="comment">#最终梯度值</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure>
<p>7.得出预测结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测出结果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">w, b, X</span>):</span><br><span class="line">    m = X.shape[<span class="number">1</span>] <span class="comment">#样本个数</span></span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m)) <span class="comment">#初始化预测输出</span></span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>) <span class="comment">#转置参数向量w</span></span><br><span class="line"></span><br><span class="line">    Y_hat = sigmoid(np.dot(w.T,X)+b) <span class="comment">#最终得到的参数代入方程</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y_hat.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> Y_hat[:,i]&gt;<span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[:,i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[:,i] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure>
<p>8.建立整个预测模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立整个预测模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = <span class="literal">False</span></span>): <span class="comment">#num_iterations-梯度下降次数 learning_rate-学习率，即参数ɑ</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>]) <span class="comment">#初始化参数w，b</span></span><br><span class="line"></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) <span class="comment">#梯度下降找到最优参数</span></span><br><span class="line"></span><br><span class="line">    w = parameters[<span class="string">&quot;w&quot;</span>]</span><br><span class="line">    b = parameters[<span class="string">&quot;b&quot;</span>]</span><br><span class="line"></span><br><span class="line">    Y_prediction_train = predict(w, b, X_train) <span class="comment">#训练集的预测结果</span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test) <span class="comment">#测试集的预测结果</span></span><br><span class="line"></span><br><span class="line">    train_accuracy = <span class="number">100</span> - np.mean(np.<span class="built_in">abs</span>(Y_prediction_train - Y_train)) * <span class="number">100</span> <span class="comment">#训练集识别准确度</span></span><br><span class="line">    test_accuracy = <span class="number">100</span> - np.mean(np.<span class="built_in">abs</span>(Y_prediction_test - Y_test)) * <span class="number">100</span> <span class="comment">#测试集识别准确度</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;训练集识别准确度: &#123;&#125; %&quot;</span>.<span class="built_in">format</span>(train_accuracy))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;测试集识别准确度: &#123;&#125; %&quot;</span>.<span class="built_in">format</span>(test_accuracy))</span><br><span class="line"></span><br><span class="line">    d = &#123;<span class="string">&quot;costs&quot;</span>: costs,</span><br><span class="line">         <span class="string">&quot;Y_prediction_test&quot;</span>: Y_prediction_test,</span><br><span class="line">         <span class="string">&quot;Y_prediction_train&quot;</span> : Y_prediction_train,</span><br><span class="line">         <span class="string">&quot;w&quot;</span> : w,</span><br><span class="line">         <span class="string">&quot;b&quot;</span> : b,</span><br><span class="line">         <span class="string">&quot;learning_rate&quot;</span> : learning_rate,</span><br><span class="line">         <span class="string">&quot;num_iterations&quot;</span>: num_iterations&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>
<p>9.初始化样本，输入模型，得出结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化数据</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br><span class="line"></span><br><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>] <span class="comment">#训练集中样本个数</span></span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>] <span class="comment">#测试集总样本个数</span></span><br><span class="line">num_px = test_set_x_orig.shape[<span class="number">1</span>] <span class="comment">#图片的像素大小</span></span><br><span class="line"></span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],-<span class="number">1</span>).T <span class="comment">#原始训练集的设为（12288*209）</span></span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],-<span class="number">1</span>).T <span class="comment">#原始测试集设为（12288*50）</span></span><br><span class="line"></span><br><span class="line">train_set_x = train_set_x_flatten/<span class="number">255.</span> <span class="comment">#将训练集矩阵标准化</span></span><br><span class="line">test_set_x = test_set_x_flatten/<span class="number">255.</span> <span class="comment">#将测试集矩阵标准化</span></span><br><span class="line"></span><br><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p>运行程序最终得到的结果为：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411121705.png" alt="结果"></p>
<p>训练集识别准确率接近100％，测试集的准确率有70％。由于训练使用的小数据集，而且逻辑回归是线性分类器，所以这个结果对于这个简单的模型实际上还是不错。</p>
<p>使用mathplotlib画出学习曲线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 画出学习曲线</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">&#x27;costs&#x27;</span>])</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;cost&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;iterations (per hundreds)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Learning rate =&quot;</span> + <span class="built_in">str</span>(d[<span class="string">&quot;learning_rate&quot;</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411121734.png" alt="学习曲线"></p>
<p>学习率不同时的学习曲线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;学习率: &quot;</span> + <span class="built_in">str</span>(i))</span><br><span class="line">    models[<span class="built_in">str</span>(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&#x27;\n&#x27;</span> + <span class="string">&quot;-------------------------------------------------------&quot;</span> + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[<span class="built_in">str</span>(i)][<span class="string">&quot;costs&quot;</span>]), label= <span class="built_in">str</span>(models[<span class="built_in">str</span>(i)][<span class="string">&quot;learning_rate&quot;</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">&#x27;cost&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;iterations&#x27;</span>)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">&#x27;upper center&#x27;</span>, shadow=<span class="literal">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">&#x27;0.90&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411121849.png" alt="几个不同学习率下的学习曲线"></p>
<p>说明不同的学习率会带来不同的成本，从而产生不同的预测结果。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="http://mooc.study.163.com/learn/deeplearning_ai-2001281002">吴恩达-神经网络与深度学习-网易云课堂</a></li>
<li><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/neural-networks-deep-learning/">Andrew Ng-Neural Networks and Deep Learning-Coursera</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Hugsy19/Machine_Learning">课程代码与资料-GitHub</a></li>
</ol>
<p>注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。</p>
<hr>
<p>更新历史：</p>
<ul>
<li>2017.09.13 完成初稿</li>
<li>2017.09.17 修正部分错误</li>
<li>2018.02.13 修改部分内容   </li>
<li>2019.01.10 修改部分内容</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>深度学习(1)：Logistic回归</p><p><a href="https://hugsy.top/2017/09/12/ML/deep_learning_1/">https://hugsy.top/2017/09/12/ML/deep_learning_1/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hugsy</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2017-09-12</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2022-12-10</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/ML/">ML</a><a class="link-muted mr-2" rel="tag" href="/tags/Python/">Python</a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210410180700.png" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210410180901.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2017/09/25/ML/deep_learning_2/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">深度学习(2)：神经网络</span></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "73963ad0af33ace91459aec2e9b453f9",
            repo: "Picbed",
            owner: "Hugsy19",
            clientID: "a321e46c668dc3d86c4f",
            clientSecret: "0ff9e81865b09e7511ef7ef17bb9596834b8fd95",
            admin: ["Hugsy19"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            proxy: "https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token",
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/logo.svg" alt="Cornfield Chase" height="28"></a><p class="is-size-7"><span>&copy; 2023 Hugsy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>