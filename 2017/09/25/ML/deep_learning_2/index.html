<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>【Andrew Ng】深度学习(2)：神经网络 - Cornfield Chase</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#B481BB"><meta name="application-name" content="Hugsy&#039;s Blog"><meta name="msapplication-TileImage" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><meta name="msapplication-TileColor" content="#B481BB"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hugsy&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="144x144" href="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><meta name="description" content="神经网络（Neural Network）的构筑理念是受到生物神经网络功能的运作启发而产生的。人工神经网络通常是通过一个基于数学统计学类型的学习方法得以优化，所以人工神经网络也是数学统计学方法的一种实际应用。 和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被基于传统规则的编程解决的。"><meta property="og:type" content="blog"><meta property="og:title" content="【Andrew Ng】深度学习(2)：神经网络"><meta property="og:url" content="https://hugsy.top/2017/09/25/ML/deep_learning_2/"><meta property="og:site_name" content="Cornfield Chase"><meta property="og:description" content="神经网络（Neural Network）的构筑理念是受到生物神经网络功能的运作启发而产生的。人工神经网络通常是通过一个基于数学统计学类型的学习方法得以优化，所以人工神经网络也是数学统计学方法的一种实际应用。 和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被基于传统规则的编程解决的。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://i.loli.net/2019/06/17/5d079ccf2314243451.jpg"><meta property="article:published_time" content="2017-09-24T16:00:00.000Z"><meta property="article:modified_time" content="2019-06-17T14:00:45.739Z"><meta property="article:author" content="Hugsy"><meta property="article:tag" content="ML"><meta property="article:tag" content="Coursera"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Python"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://i.loli.net/2019/06/17/5d079ccf2314243451.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hugsy.top/2017/09/25/ML/deep_learning_2/"},"headline":"【Andrew Ng】深度学习(2)：神经网络","image":["https://i.loli.net/2019/06/17/5d079ccf2314243451.jpg"],"datePublished":"2017-09-24T16:00:00.000Z","dateModified":"2019-06-17T14:00:45.739Z","author":{"@type":"Person","name":"Hugsy"},"description":"神经网络（Neural Network）的构筑理念是受到生物神经网络功能的运作启发而产生的。人工神经网络通常是通过一个基于数学统计学类型的学习方法得以优化，所以人工神经网络也是数学统计学方法的一种实际应用。 和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被基于传统规则的编程解决的。"}</script><link rel="canonical" href="https://hugsy.top/2017/09/25/ML/deep_learning_2/"><link rel="alternate" href="/atom.xml" title="Cornfield Chase" type="application/atom+xml"><link rel="icon" href="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?7b65ce26b5ae8dae153d7b4d53214ba4";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/logo.svg" alt="Cornfield Chase" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/tags/CS/">CS</a><a class="navbar-item" href="/tags/EE/">EE</a><a class="navbar-item" href="/tags/ML/">ML</a><a class="navbar-item" href="/tags/Weekly/">Weekly</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2017-09-24T16:00:00.000Z" title="9/25/2017, 12:00:00 AM">2017-09-25</time>发表</span><span class="level-item"><time dateTime="2019-06-17T14:00:45.739Z" title="6/17/2019, 10:00:45 PM">2019-06-17</time>更新</span><span class="level-item">31 分钟读完 (大约4593个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">【Andrew Ng】深度学习(2)：神经网络</h1><div class="content"><p><strong>神经网络（Neural Network）</strong>的构筑理念是受到生物神经网络功能的运作启发而产生的。人工神经网络通常是通过一个基于数学统计学类型的学习方法得以优化，所以人工神经网络也是数学统计学方法的一种实际应用。</p>
<p>和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被基于传统规则的编程解决的。</p>
<span id="more"></span>

<h3 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h3><p>机器学习领域所说的神经网络，指的是一种模仿生物神经网络的结构和功能而建立的数学或计算模型，用于对函数进行估计或近似。下面通过一个房价预测模型的例子，来对所谓的神经网络有一个感性的认识。</p>
<p>给定的一些市面上房子的面积及其对应价格的数据，希望据此建立一个房价预测模型，即输入一个房子的面积，希望通过这个模型输出该房价的预测值。因为一般情况下房价和房子的面积都成正相关，所以这是可以作为一个线性回归问题，并且可以将已知数据之间的关系表现在平面坐标系中：</p>
<p><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjtjyyfqxrj20f10b00sy.jpg" alt="房价线性模型"></p>
<p>对这些数据进行线性拟合，因为房价永远不会是负数，将得到图中绘制的<strong>ReLU函数（Rectified Linear Unit，修正线性单元）</strong>，这个函数将在后面多次用到。</p>
<p>将这个房价预测模型构建成一个神经网络，便以房子的面积作为输入，房价作为输出，ReLU函数则充当其中的神经元的作用，来产生输出，得到下面一个简单的神经网络模型：</p>
<p><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjtj8ibkakj20b105jdft.jpg" alt="房价神经网络模型"></p>
<p>更进一步地，考虑到房价除了受房子的面积影响之外，还会受卧室的数量、房子的位置以及地区的财富水平等其他因素的影响，搜集到这些因素的相关数据，要将这些因素也考虑在内的话，就需要构建一个更为复杂的神经网络。</p>
<p><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjtjl5miuuj20le0a2wfj.jpg" alt="神经网络模型"></p>
<p>如上图所示，综合了几种因素作为神经网络的输入，就构成了稍为复杂的神经网络。该网络将ReLU等函数作为<strong>隐藏层（Hidden Units）</strong>来处理输入数据，并输出我们想要的结果。</p>
<p>在这个预测房价的例子中，只要拥有足够量的训练数据，就能够生成一个较好的神经网络模型，从而对于任意的输入，都能得到较为精确的预测结果。</p>
<p>包括此在内的大多数机器学习问题，都是通过输入带有正确输出结果的数据来训练模型，在机器学习领域这种方法被称为<strong>监督学习（Surprised Learning）</strong>。对于不同领域的监督学习问题，需要用不同神经网络进行解决，而神经网络大致可分为如上所示的标准神经网络、擅长处理图像数据的<strong>卷积神经网络（CNN）</strong>以及处理序列数据的<strong>循环神经网络（RNN）</strong>，前者处理的常是数据库中的<strong>结构化数据（Structured Data）</strong>，而后两者则是图像、音频、文字序列等<strong>非结构化数据（Unstructured Data）</strong>。</p>
<p>简单而言，深度学习便是更为复杂的神经网络。</p>
<p>在逻辑回归中，通过建立一个简单的神经网络模型，输入训练样本(x,y)，希望得出一个预测值$\hat{y}$，使得$\hat{y}$尽可能等于y。训练的流程如下：</p>
<p><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjtl1m6yfhj20cp0a90sx.jpg" alt="逻辑回归流程"></p>
<p>在这个模型中，先建立了一个损失函数，进而不断采用梯度下降法找到参数w和b的最优解。采用这种算法编写的猫识别器最终的准确率只有70%，想要进一步提高识别的精准度，就需要建立起一个多层的神经网络来训练样本。</p>
<h3 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h3><p>如图所示的神经网络中，前面为输入层，中间为隐藏层 ，最后为输出层。中间层被称为隐藏层的原因是因为在训练过程中，将看到输入的样本有哪些，输出的结果是什么，中间层中的神经节点产生的真实值无法被观察到。所以中间层被称为隐藏层，只是因为你不会在训练集中看到它。</p>
<p><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjthu97petj20ix0dimyp.jpg" alt="两层神经网络"></p>
<p>前面的逻辑回归中，用$X$表示输入，这里用符号$a^{[0]}$代替，上标“[ ]”括号中的数字表示神经网络中的第几层，且符号$a$代表着<strong>激活（Activation）</strong>，指的是不同层次的神经网络传递给后续层次的值。</p>
<p>将输入集传递给隐藏层后，隐藏层随之产生激活表示为$a^{[1]}$，而隐藏层的第一节点生成的激活表示为$a^{[1]}_1$，第二个节点产生的激活为$a^{[1]}_2$，以此类推，则：$${ a^{[1]} = \begin{bmatrix} a^{[1]}_1 \\ a^{[1]}_2 \\ a^{[1]}_3 \\ a^{[1]}_4 \end{bmatrix}\quad}$$<br>最后，输出层输出的值表示为$a^{[2]}$，则$\hat{y} = a^{[2]}$。</p>
<p>图中的这个神经网络也被称为两层神经网络，原因是计算神经网络的层数时，通常不考虑输入层。所以这个神经网络中，隐藏层是第一次层，输出层是第二层，而输入层为第零层。</p>
<p>图中的隐藏层中，将存在参数w和b，它们将分别表示为$w^{[1]}$和$b^{[1]}$，$w^{[1]}$将会是个4×3矩阵，$b^{[1]}$将会是个4×1矩阵。输出层中，也会存在参数$w^{[2]}$和$b^{[2]}$，$w^{[2]}$是个1×4矩阵，$b^{[2]}$是个1×1矩阵。</p>
<h3 id="神经网络的表示"><a href="#神经网络的表示" class="headerlink" title="神经网络的表示"></a>神经网络的表示</h3><p><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjufdpan63j20ex09imxx.jpg" alt="神经网络的表示"></p>
<p>如图所示，将样本输入隐藏层中的第一个节点后，可得；$$ z^{[1]}_1 = w^{[1]T}_1X + b^{[1]}_1, a^{[1]}_1 = σ(z^{[1]}_1)$$<br>以此类推：$$ z^{[1]}_2 = w^{[1]T}_2X + b^{[1]}_2, a^{[1]}_2 = σ(z^{[1]}_2)$$ $$ z^{[1]}_3 = w^{[1]T}_3X + b^{[1]}_3, a^{[1]}_3 = σ(z^{[1]}_3)$$ $$ z^{[1]}_4 = w^{[1]T}_4X + b^{[1]}_4, a^{[1]}_4 = σ(z^{[1]}_4)$$<br>将它们都表示成矩阵形式：$${ z^{[1]} = \begin{bmatrix} w^{[1]}_1 &amp;  w^{[1]}_1 &amp; w^{[1]}_1\\ w^{[1]}_2 &amp; w^{[1]}_2 &amp; w^{[1]}_2 \\ w^{[1]}_3 &amp; w^{[1]}_3 &amp; w^{[1]}_3 \\ w^{[1]}_4 &amp; w^{[1]}_4 &amp; w^{[1]}_4 \end{bmatrix}\quad}\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}\quad + \begin{bmatrix} b^{[1]}_1 \\ b^{[1]}_2 \\ b^{[1]}_3 \\ b^{[1]}_4 \end{bmatrix} = \begin{bmatrix} z^{[1]}_1 \\ z^{[1]}_2 \\ z^{[1]}_3 \\ z^{[1]}_4 \end{bmatrix}\quad\quad$$<br>即：$$ z^{[1]} = w^{[1]}X + b^{[1]} $$ $$a^{[1]} = σ(z^{[1]})$$<br><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjug7o5ldfj20fh0e8my3.jpg" alt="神经网络的表示"><br>进过隐藏层后进入输出层，又有:$$ z^{[2]} = w^{[2]}a^{[1]} + b^{[2]}$$ $$a^{[2]} = σ(z^{[2]})$$</p>
<p>可以发现，在一个的共有l层，且第l层有$n^{[l]}$个节点的神经网络中，参数矩阵 $w^{[l]}$的大小为$n^{[l]}$*$n^{[l-1]}$，$b^{[l]}$的大小为$n^{[l]}$*1。</p>
<p>逻辑回归中，直接将两个参数都初始化为零。而在神经网络中，通常将参数w进行<strong>随机初始化</strong>，参数b则初始化为0。</p>
<p>除w、b外的各种参数，如学习率$\alpha$、神经网络的层数$l$，第$l$层包含的节点数$n^{[l]}$及隐藏层中用的哪种激活函数，都称为<strong>超参数（Hyper Parameters）</strong>，因为它们的值决定了参数w、b最后的值。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>建立一个神经网络时，需要关心的一个问题是，在每个不同的独立层中应当采用哪种激活函数。逻辑回归中，一直采用sigmoid函数作为激活函数，此外还有一些更好的选择。</p>
<p><strong>tanh函数（Hyperbolic Tangent Function，双曲正切函数）</strong>的表达式为：$$tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$<br>函数图像为：<br><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjuhv55rcsj208w05m744.jpg" alt="tanh函数"></p>
<p>tanh函数其实是sigmoid函数的移位版本。对于隐藏单元，选用tanh函数作为激活函数的话，效果总比sigmoid函数好，因为tanh函数的值在-1到1之间，最后输出的结果的平均值更趋近于0，而不是采用sigmoid函数时的0.5，这实际上可以使得下一层的学习变得更加轻松。对于二分类问题，为确保输出在0到1之间，将仍然采用sigmiod函数作为输出的激活函数。</p>
<p>然而sigmoid函数和tanh函数都具有的缺点之一是，在z接近无穷大或无穷小时，这两个函数的导数也就是梯度变得非常小，此时梯度下降的速度也会变得非常慢。</p>
<p>线性修正单元，也就是上面举例解释什么是神经网络时用到的ReLU函数也是机器学习中常用到的激活函数之一，它的表达式为：$$g(z) = max(0,z) =\begin{cases} 0,  &amp; \text{($z$ $\le$ 0)} \\ z, &amp; \text{($z$ $\gt$ 0)} \end{cases} $$<br>函数图像为：<br><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjujlni8kyj20dp07d744.jpg" alt="ReLU函数"></p>
<p>当z大于0时是，ReLU函数的导数一直为1，所以采用ReLU函数作为激活函数时，随机梯度下降的收敛速度会比sigmoid及tanh快得多，但负数轴的数据都丢失了。</p>
<p>ReLU函数的修正版本，称为<strong>Leaky-ReLU</strong>，其表达式为：$$g(z) = max(0,z) =\begin{cases} \alpha z,  &amp; \text{($z$ $\le$ 0)} \\ z, &amp; \text{($z$ $\gt$ 0)} \end{cases} $$<br>函数图像为：<br><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjujlnn9tgj20de07m3ye.jpg" alt="Leaky-ReLU"></p>
<p>其中$ \alpha $是一个很小的常数，用来保留一部非负数轴的值。</p>
<p>可以发现，以上所述的几种激活函数都是非线性的，原因在于使用线性的激活函数时，输出结果将是输入的线性组合，这样的话使用神经网络与直接使用线性模型的效果相当，此时神经网络就类似于一个简单的逻辑回归模型，失去了其本身的优势和价值。</p>
<h3 id="前向传播和反向传播"><a href="#前向传播和反向传播" class="headerlink" title="前向传播和反向传播"></a>前向传播和反向传播</h3><p>如图，通过输入样本$x$及参数$w^{[1]}$、$b^{[1]}$到隐藏层，求得$z^{[1]}$，进而求得$a^{[1]}$；再将参数$w^{[2]}$、$b^{[2]}$和$a^{[1]}$一起输入输出层求得$z^{[2]}$，进而求得$a^{[2]}$，最后得到损失函数$\mathcal{L}(a^{[2]},y)$，这样一个从前往后递进传播的过程，就称为<strong>前向传播（Forward Propagation）</strong>。<br><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjupgnkcbtj20r4089q32.jpg" alt="前向传播"><br>前向传播过程中：$$ z^{[1]} = w^{[1]T}X + b^{[1]} $$ $$a^{[1]} = g(z^{[1]})$$  $$ z^{[2]} = w^{[2]T}a^{[1]} + b^{[2]}$$ $$a^{[2]} = σ(z^{[2]}) = sigmoid(z^{[2]})$$ $${\mathcal{L}(a^{[2]}, y)=-(ylog\ a^{[2]} + (1-y)log(1-a^{[2]}))}$$</p>
<p>在训练过程中，经过前向传播后得到的最终结果跟训练样本的真实值总是存在一定误差，这个误差便是损失函数。想要减小这个误差，当前应用最广的一个算法便是梯度下降，于是用损失函数，从后往前，依次求各个参数的偏导，这就是所谓的<strong>反向传播（Back Propagation）</strong>，一般简称这种算法为<strong>BP算法</strong>。<br><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjupevz9aaj20r00aqt99.jpg" alt="反向传播"><br>sigmoid函数的导数为：$${a^{[2]’} = sigmoid(z^{[2]})’ =  \frac{\partial a^{[2]}}{\partial z^{[2]}} = a^{[2]}(1 - a^{[2]})}$$</p>
<p>由复合函数求导中的链式法则，反向传播过程中：$$ da^{[2]} = \frac{\partial \mathcal{L}(a^{[2]}, y)}{\partial a^{[2]}} = -\frac{y}{a^{[2]}} + \frac{1 - y}{1 - a^{[2]}}$$ $$ dz^{[2]} = \frac{\partial \mathcal{L}(a^{[2]}, y)}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}} = a^{[2]} - y$$ $$ dw^{[2]} = \frac{\partial \mathcal{L}(a^{[2]}, y)}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}}\cdot \frac{\partial z^{[2]}}{\partial w^{[2]}} = dz^{[2]}\cdot a^{[1]T}$$ $$ db^{[2]} = \frac{\partial \mathcal{L}(a^{[2]}, y)}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}}\cdot \frac{\partial z^{[2]}}{\partial b^{[2]}} = dz^{[2]}$$ $$ da^{[1]} = \frac{\partial \mathcal{L}(a^{[2]}, y)}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} = dz^{[2]} \cdot w^{[2]} $$ $$ dz^{[1]} = \frac{\partial \mathcal{L}(a^{[2]}, y)}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}}  \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}= dz^{[2]} \cdot w^{[2]} × g^{[1]’}(z^{[1]}) $$ $$ dw^{[1]} = \frac{\partial \mathcal{L}(a^{[2]}, y)}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}}  \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}} \cdot \frac{\partial z^{[1]}}{\partial w^{[1]}}= dz^{[1]} \cdot X^T $$ $$ db^{[1]} = \frac{\partial \mathcal{L}(a^{[2]}, y)}{\partial a^{[2]}} \cdot \frac{\partial a^{[2]}}{\partial z^{[2]}} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}}  \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}} \cdot \frac{\partial z^{[1]}}{\partial b^{[1]}}= dz^{[1]}$$<br>这便是反向传播的整个推导过程。</p>
<p>在具体的算法实现过程中，还是需要采用逻辑回归中用到梯度下降的方法，将各个参数进行向量化、取平均值，不断进行更新。</p>
<h3 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h3><p>深层神经网络含有多个隐藏层，构建方法如前面所述，训练时根据实际情况选择激活函数，进行前向传播获得成本函数进而采用BP算法，进行反向传播，梯度下降缩小损失值。</p>
<p>拥有多个隐藏层的深层神经网络能更好得解决一些问题。如图，例如利用神经网络建立一个人脸识别系统，输入一张人脸照片，深度神经网络的第一层可以是一个特征探测器，它负责寻找照片里的边缘方向，<strong>卷积神经网络（Convolutional Neural Networks，CNN）</strong>专门用来做这种识别。</p>
<p><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjxyv40x0kj20kj09l419.jpg" alt="深层神经网络"></p>
<p>深层神经网络的第二层可以去探测照片中组成面部的各个特征部分，之后一层可以根据前面获得的特征识别不同的脸型的等等。这样就可以将这个深层神经网络的前几层当做几个简单的探测函数，之后将这几层结合在一起，组成更为复杂的学习函数。从小的细节入手，一步步建立更大更复杂的模型，就需要建立深层神经网络来实现。</p>
<h3 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h3><p>之前使用逻辑回归实现过一个猫分类器，最终用测试集测试最高能达到的准确率有70%，现在用一个4层神经网络来实现。</p>
<p>1.L层神经网络参数初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span>(<span class="params">layer_dims</span>):</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(layer_dims)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)] = np.random.randn(layer_dims[l], layer_dims[l-<span class="number">1</span>]) / np.sqrt(layer_dims[l-<span class="number">1</span>]) <span class="comment">#初始化为随机值</span></span><br><span class="line">        parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)] = np.zeros((layer_dims[l], <span class="number">1</span>)) <span class="comment">#初始化为0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p>2.前向传播和后向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#部分代码此处省略，详细代码见参考资料-Github</span></span><br><span class="line"><span class="comment">#L层神经网络模型的前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span>(<span class="params">X, parameters</span>):</span></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span>                 </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A</span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)], parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)], activation = <span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line"></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(L)], parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(L)], activation = <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"></span><br><span class="line"><span class="comment">#L层神经网络模型的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span>(<span class="params">AL, Y, caches</span>):</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(caches)</span><br><span class="line">    m = AL.shape[<span class="number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape)</span><br><span class="line"></span><br><span class="line">    dAL = -(np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</span><br><span class="line">    current_cache = caches[L-<span class="number">1</span>]</span><br><span class="line">    grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(L)], grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(L)], grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(L)] = linear_activation_backward(dAL, current_cache, activation = <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(L-<span class="number">1</span>)):</span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(l + <span class="number">2</span>)], current_cache, activation = <span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p>3.L层神经网络模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#L层神经网络模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span>(<span class="params">X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    costs = []                  </span><br><span class="line"></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):</span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;循环%i次后的成本值: %f&quot;</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;cost&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;iterations (per tens)&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Learning rate =&quot;</span> + <span class="built_in">str</span>(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p>4.输入数据，得出结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()</span><br><span class="line"></span><br><span class="line">m_train = train_x_orig.shape[<span class="number">0</span>] <span class="comment">#训练集中样本个数</span></span><br><span class="line">m_test = test_x_orig.shape[<span class="number">0</span>] <span class="comment">#测试集总样本个数</span></span><br><span class="line">num_px = test_x_orig.shape[<span class="number">1</span>] <span class="comment">#图片的像素大小</span></span><br><span class="line"></span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>],-<span class="number">1</span>).T <span class="comment">#原始训练集的设为（12288*209）</span></span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>],-<span class="number">1</span>).T <span class="comment">#原始测试集设为（12288*50）</span></span><br><span class="line"></span><br><span class="line">train_x = train_x_flatten/<span class="number">255.</span> <span class="comment">#将训练集矩阵标准化</span></span><br><span class="line">test_x = test_x_flatten/<span class="number">255.</span> <span class="comment">#将测试集矩阵标准化</span></span><br><span class="line"></span><br><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>]</span><br><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class="number">2500</span>, print_cost = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">pred_train = predict(train_x, train_y, parameters)</span><br><span class="line">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure>
<p>得到的结果为：<br><img src="https://ws1.sinaimg.cn/large/82e16446ly1fjzarivaoej20g4097glx.jpg" alt="成本值"><br><img src="https://ws1.sinaimg.cns/large/82e16446ly1fjzarhqjegj20au07qq2z.jpg" alt="成本变化曲线"><br>样本集预测准确度: 0.985645933014<br>测试集预测准确度: 0.8</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol>
<li><a target="_blank" rel="noopener" href="http://mooc.study.163.com/learn/deeplearning_ai-2001281002">吴恩达-神经网络与深度学习-网易云课堂</a></li>
<li><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/neural-networks-deep-learning/">Andrew Ng-Neural Networks and Deep Learning-Coursera</a></li>
<li><a target="_blank" rel="noopener" href="https://www.deeplearning.ai/">deeplearning.ai</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/BinWeber/Machine-Learning">课程代码与资料-GitHub</a></li>
</ol>
<p>注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。</p>
<hr>
<p>更新历史：</p>
<ul>
<li>2017.09.28 完成初稿</li>
<li>2018.02.13 调整部分内容</li>
<li>2019.01.01 二刷课程后修改内容</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>【Andrew Ng】深度学习(2)：神经网络</p><p><a href="https://hugsy.top/2017/09/25/ML/deep_learning_2/">https://hugsy.top/2017/09/25/ML/deep_learning_2/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hugsy</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2017-09-25</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2019-06-17</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/ML/">ML</a><a class="link-muted mr-2" rel="tag" href="/tags/Coursera/">Coursera</a><a class="link-muted mr-2" rel="tag" href="/tags/Deep-Learning/">Deep Learning</a><a class="link-muted mr-2" rel="tag" href="/tags/Python/">Python</a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210410180700.png" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210410180901.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2017/09/28/ML/deep_learning_3/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">【Andrew Ng】深度学习(3)：优化神经网络(1)</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2017/09/12/ML/deep_learning_1/"><span class="level-item">【Andrew Ng】深度学习(1)：Logistic回归</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "d80bd51ddee6f75b10598ba9b4fd4150",
            repo: "Picbed",
            owner: "Hugsy19",
            clientID: "a321e46c668dc3d86c4f",
            clientSecret: "0ff9e81865b09e7511ef7ef17bb9596834b8fd95",
            admin: ["Hugsy19"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/logo.svg" alt="Cornfield Chase" height="28"></a><p class="is-size-7"><span>&copy; 2021 Hugsy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>