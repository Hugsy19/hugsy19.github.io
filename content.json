{"pages":[{"title":"关于本站","text":"更新历史 2017.09.12 以binweber.top为域名，‘binweber’为自己瞎编的名字，博客破壳日(●ˇ∀ˇ●) 2018.03.11 历时7个月，深度学习系列课程/笔记完结(●’◡’●) 2018.03.29 准备EE跨CS考研，博客暂停更新(づ￣ 3￣)づ 2018.12.24 考研初试结束，准备以后二战/(ㄒoㄒ)/~~ 2019.04.01 做毕设，打算重新整理原来的深度学习系列笔记（⊙ｏ⊙） 2019.06.26 本科毕业(ง •_•)ง 2019.07.01 更换域名为hugsy.top，‘hugsy’为老友记中Joe最爱的玩具企鹅名┗|｀O′|┛ 2021.04.10 各种原因，时隔近两年没怎么更新过，已化身社畜解决&lt;( _ _ )&gt; 花了一天多时间重新整理（升级Hexo、主题，换图床…），重新出发( •̀ ω •́ )y 常见问题 图片无法显示： 由于第三方图床的不稳定性，图片全都迁移到了github上； 由于DNS被污染，导致raw.githubusercontent.com无法访问； 到IPAddress搜索raw.githubusercontent.com，查询其最新IP地址； 将查询到的地址加入hosts文件即可。（推荐使用SwitchHosts） 遇到其他问题欢迎在此处留言~~","link":"/about/index.html"}],"posts":[{"title":"单片机原理(1)：基本结构","text":"单片微型计算机（Single Chip Microcomputer）简称单片机，是把组成微型计算机的主要功能部件（CPU、RAM、ROM、I/O口、定时/计数器、串行口等）集成在一块芯片中，构成一个完整的微型计算机。 单片机主要面对测控对象，突出的是控制功能，所以它的芯片内集成了很多面向测控对象的接口电路，如ADC（Analog to Digital Converter，模数转换器）、DAC（Digital to Analog Converter，数模转换器）、高速I/O口、PWM（Pulse Width Modulator，脉冲宽度调制器）、WDT（Watch Dog Timer，监视定时器，俗称“看门狗”）等。这些接口电路已经突破了传统的微型计算机体系结构，所以单片机也称MCU（Micro-Controller Unit，微控制器）。 单片机的种类十分广泛，现在市场上主流的有PIC系列、STM32系列、AVR等等。51系列单片机是对所有兼容Intel 8031指令系统的单片机的统称，它们都是Intel公司1981年制造8位MCU Intel 8051衍生兼容品，其指令集称为MCS-51。这里以此为例介绍单片机工作原理。 引脚及功能 51单片机的通常采用的是40引脚的双列直插式封装（Dual In-line Package，DIP），引脚功能的定义如上图所示。除去基本的电源引脚，其中： VSS：电源引脚，需要接地（GND）。 **XTAL1、XTAL2(External Crystal)**：时间引脚，在片内XTAL1是振荡器反相放大器和时钟发生器的输入端，XTAL2是振荡器反相放大器的输出。分别外接由两个电容与晶振的并联谐振时钟电路时，XTAL1接外部晶振和微调电容的一端，XTAL2接外部晶振和微调电容的另一端；使用外部时钟时，XTAL1接地或悬空，XTAL2引脚接外部时钟的输入。 **RST(Reset)**：复位引脚，出现持续两个机器周期的高电平将导致单片机复位。 **ALE/PROG(Address Latch Enable/)**：作ALE，为地址锁存允许控制，用于在访问外部存储器时锁存P0口输出的低8位地址，脉冲的频率大约为振荡器频率的1/6。作PROG，当内部程序存储器（EPROM）编程时，由此输入编程脉冲信号。 **PSEN(Programme Store Enable)**：外部程序存储器选通信号。 **EA/Vpp(External Enable)**：作EA功能，为外部程序存储器使能信号，低电平时，只能访问外部程序存储器。作Vpp，对EPROM编程时，用以施加编程电压。 P0、P1、P2、P3：四个并行I/O口，每并行口8个引脚。 I/O口结构及功能51单片机的32个I/O引脚，组成P0到P3四个8位的并行双向I/O口，内部的特殊功能寄存器（Special Function Register，SFR）P0-P3分别是它们的端口锁存器，此外每个口还分别包含一个输出驱动器和输入缓冲器。四个并行口可以按字节操作，也可以按位操作。 P0可作一般I/O口使用，应用系统采用外部总线结构时，也可以分时复用作为双向数据总线（Data Bus）和低8位地址总线（Address Bus）。 上图为P0口某一位的结构原理图。 当P0作I/O口时，CPU发出控制信号C=0，封锁与门，使场效应管V2截至，同时将下面的数据选择器（Multiplexer，MUX）拨到下方，将锁存器的$\\overline Q$端与场效应管（Field Effect Transistor）V1的栅极接通。 输出数据时，内部总线数据经锁存器、MUX、V1输出到引脚；输入数据后，上下两个三态输入缓冲器用以内部的读操作，分别实现读引脚和读锁存器。 要注意的是，P0口作为I/O口输出时，由于输出级为漏极开路电路，需要外接上拉电阻，才能输出高电平；输入后读取数据时，V1导通将输入的高电平拉为低电平造成误读，所以在进行输入操作前，要先向端口输出锁存器写“1”。 当P0口作地址/数据总线时，CPU发出控制信号C=1。由P0输出地址/数据信息时，与门打开，MUX将CPU内部地址/数据总线反向后与V1的栅极接通，V1和V2两个FET管构成推拉式输出电路，负载能力大大加强；输入数据时，是由下面的三态输入缓冲器进入内部数据的。 P1四个I/O口中功能最简单，可做一般I/O口使用。其某一位的结构原理图如下。 其内部已有上拉电阻，可直接输出高电平，驱动拉电流负载。和P0一样，端口输入后读取数据时，要先向端口输出锁存器写“1”。 P2可作一般I/O口使用，在应用系统采用外部总线结构时，仅用作高8位地址总线。其某一位的结构原理如下图： MUX打向左边时，作一般I/O口使用，此时于前面介绍的I/O口工作原理基本一致。Q输出为0时V1导通，外部引脚输出低电平；输出1时V1截止，由于存在内部上拉电阻，外部引脚输出高电平。输入后也可分为读引脚状态和读锁存器状态。而且端口输入后读取数据时，要先向端口输出锁存器写“1”。 P3可作一般I/O口使用外，每个引脚都具有第二功能。其某一位的结构原理如下图： 作一般I/O口用，原理同P2。使用引脚的第二功能时，对应的锁存器里必须为“1”，否则图中的与非门输出始终为1，V1导通，引脚始终在低电平，不能正常工作。 每个引脚的第二功能如下表： 引脚 第二功能 说明 P3.0 RXD(Receive External Data) 串行输入端 P3.1 TXD(Transmit External Data) 串行输出端 P3.2 INT0(Interrupt) 外部中断0 P3.3 INT1 外部中断1 P3.4 T0(Timer) 计数器0外部输入 P3.5 T1 计数器1外部输入 P3.6 WR(Write Enable) 外部数据存储器写选通 P3.7 RD(Read Enable) 外部数据存储器读选通 四个I/O口中，P1、P2、P3口可驱动4个LS TTL（Low-power Schottky TTL，低功耗肖特基TTL）负载，即输出电流不小于400uA； P0口的输出缓冲器可驱动8个LS TTL负载，作一般I/O使用时为开漏输出，需要外加上拉电阻，做数据/地址总线使用时不需要外加上拉电阻。 内部微体系结构 51系列单片机内部包含： 运算器（包括逻辑运算器ALU（Arithmetic and Logic Unit）、累加器ACC（Accumulator）、寄存器B、程序状态字寄存器PSW（Program Status Word））、程序计数器PC（Program Counter）、指令寄存器IR（Instruction Register)、指令译码器ID（Instruction Decoder）、数据指针DPTR（Data Pointer）等组成的8位CPU。 4KB ROM 程序存储器、128B RAM 数据存储器 可寻址64KB外部数据存储、控制电路 21个特殊功能寄存器SFR 32条可编程I/O端口线 2个16位定时/计数器 1个可编程全双工串行口 5个中断源、两个优先级嵌套中断结构 1个片内振荡器及时钟电路 CPUCPU用以产生各种控制信号，是单片机的核心。 ALU完成数据的算数运算、逻辑运算，并将运算结果的状态送往PSW。 PC是一个16位具有自动加一功能的寄存器，用来存放即将取出的指令地址，可对64KB程序存储器直接寻址。 IR是用来暂存待执行指令的8位寄存器。ID对其中的指令进行译码，将指令转变为执行此指令所需要的电信号。 DPTR是一个16位专用地址指针寄存器，用来存放16位地址，作间址寄存器使用。 存储器51单片机的存储器按功能可分为数据存储器（RAM）和程序存储器（ROM），按位置分又可以分为片外存储器和片内存储器 。 ROM51单片机通常包含只4KB的程序存储器，而以PC作为地址指针，通过16位地址总线，最大可寻址$2^{16}$B即64KB的地址空间，所以一般进行外部存储器进行拓展。 引脚EA}决定了程序存储器的0000-0FFFH 4KB地址范围在单片机内部还是外部。当EA=1时，程序存储器的地址分为片内的0000-0FFFH 4KB地址范围和片外程序存储器的1000-FFFFH 60KB地址范围；否则，当EA=0时，只能寻址外部程序存储器，0000-FFFFH全部64KB地址空间都在片外。 系统复位后PC的内容为0000H，也就是说单片机上电后都是从程序存储器的0000H单元开始取指令执行程序，一般在此地址单元设置转移指令，使之转向用户主程序处。此外，0003-0023H单元被保留用作5个中断服务程序的入口地址： 入口地址 所属中断 0003H 外部中断0入口地址 000BH 定时器0中断入口地址 0013H 外部中断1入口地址 001BH 定时器1中断入口地址 0023H 串口中断入口地址 故中断服务程序个主程序一般都被放置在0030H单元以后。 RAM128B的内部数据存储器是用得最多的地址空间，所有的操作指令的操作数据只能在此地址空间或SFR中。 RAM区结构如上图。其中，00H-1FH共32个单元为工作寄存器区，分为4组，每组8个单元组成通用寄存器，都用R0-R7表示。可以通过改变PSW的RS1和RS0、RS1两位的状态来选择CPU当前使用的工作寄存器组，如下表所示。这样可以提高CPU的操作效率和响应中断的速度，利于现场的保护及恢复。 20H-2FH共16个字节128位组成位寻址区，可用位寻址方式访问，位地址为00H-7FH。位寻址区的位地址如下表： 最顶层的30H-7FH共80个单元为用户RAM区，作为堆栈或者数据缓存器。 普通51子系列单片机中，地址为00H-7FH的低128B区域为RAM区，而地址为80H-FFH的高128B区域即为SFR。增强型52子系列中，有地址为00H-FFH的256B的RAM，而SFR的地址和RAM的高128字节地址80H-FFH是重合的，需要通过不同的寻址方式来区分它们。 SFRSFR是专门用于控制、选择、管理、存放单片机内部各部分的工作方式、条件、状态、结果的寄存器，不同的SFR用于管理不同的硬件模块。 51单片机内的21个SFR中，有5个是16位寄存器 ，11个可以进行位寻址。它们的地址分布及功能如下表： 其中PSW用于存放程序运行是的各种状态信息，其各位定义如下： 位地址 | D7H | D6H | D5H | D4H | D3H | D2H | D1H | D0H——|——|——|——|——|——|——|——|——|——定义 | CY | AC | F0 | RS1 | RS0 | OV | F1 | P **CY(Carry)**：进位标志位，运算过程最高为产生进位或借位，则CY=1。 **AC(Assistant Carry)**：辅助进位标志位，运算过程中低四位向高位有进位或借位，则AC=1。 F0/F1：软件标志位，可作为用户自定义的标志位，由用户置位或复位。 **RS0和RS1(Register Select)**：RAM中的4组工作寄存器选择位。 **OV(Overflow)**：溢出标志位，运算过程中产生溢出，OV=1。 **P(Parity)**：奇偶标志位，执行指令是会根据ACC中1的个数的奇偶自动令P置位或清零，奇为1，偶为0；串行通信时，可以由此位验证传输的可靠性。 时钟和复位单片机内部的时钟信号用来提供单片机内部各种操作的时间基准，复位操作使单片机内的电路初始化。 时钟电路、单位51单片机的时钟信号通常由内部振荡或外部振荡方式来获得。接法如图所示： 在XTAL1和XTAL2引脚外接晶体振荡器或者陶瓷谐振器，即构成内部振荡方式。晶振通常选用6、12或24MHz，单片机内部有一个高增益反向放大器，外接晶振将构成自激振荡，产生振荡时钟脉冲，电容C1、C2可以稳定振荡频率，电容值一般为5-30pF。 外部振荡就是把已有的时钟信号引入单片机内。 单片机以晶振的振荡周期（或外部输入的时钟周期）为最小的时序单位，片内的各种微操作都以此周期为时序基准；振荡频率经单片机内的二分频器分频后形成状态周期，所以，1个状态周期包含2个振荡周期；而6个状态周期组成1个机器周期，所以，1个机器周期包含12个振荡周期；执行一条指令需要1-4个机器周期，这个时间便称为指令周期。 例如，单片机外接12MHz的晶振时：$$ 振荡周期 = \\frac{1}{f_{osc}} = \\frac{1}{12Mhz} = 0.0833 us $$ $$ 状态周期 = \\frac{2}{f_{osc}} = \\frac{2}{12Mhz} = 0.167 us $$ $$ 机器周期 = \\frac{12}{f_{osc}} = \\frac{12}{12Mhz} = 1 us $$ $$ 指令周期 = 1–4个机器周期 = 1–4 us $$ 时序图51单片机的指令执行及存储器读取时序图如下： 取指时序： 外部ROM读时序： 外部RAM读取时序： 复位电路当51单片机的复位引脚RST出现两个机器周期以上高电平时，单片机即完成复位操作。复位操作通常有上电复位和开关复位两种形式，接法如图所示： 单片机复位后，PC=0000H，P0-P3=FFH，SP=07H，其他寄存器都回到初始的0状态。 更新历史： 2017.11.22 完成初稿","link":"/2017/11/19/EE/msc51_1/"},{"title":"单片机原理(2)：程序设计","text":"指令系统是计算机硬件的语言系统，也叫机器语言，它是软件和硬件的主要界面，从系统结构的角度看，它是系统程序员看到的计算机的主要属性。指令系统表征了计算机的基本功能，决定了机器所要求的能力，也决定了指令的格式和机器的结构。 51系列单片机一般使用汇编语言（Assembly Language）直接编程，其指令系统中，有进行数据传送、算术运算、逻辑运算、位操作、控制传递等功能的111条基本指令。此外也可以采用C语言进行程序设计。 寻址方式寻址方式是CPU寻找操作数或操作数地址的方法，存放在不同位置的数据具有需要采用不同的方式进行寻址，不同类型计算机的寻址方式也不同，它是计算机重要的性能指标之一。MSC-51单片机有7种寻址方式。 立即寻址指令中直接给出参与操作的数据，称立即数，用data表示。在汇编语言中，为标明立即数，为data加前缀”#”。立即数可以是8位和16位二进制数，分别用#data和#data16表示。 汇编指令格式：MOV A, #data 如： 12MOV A, #30H ;8位立即数30H放入累加器ACC中MOV DPTR, #2000H ;16位立即数2000H放入数据指针DPTR 直接寻址指令中直接给出参与操作的数据的地址，直接地址一般用direct表示。 汇编指令格式：MOV A, direct 如： 1MOV A, 80H ;将80H单元，即P0口的内容放入累加器ACC中 寄存器寻址参与操作的数据存放在寄存器中，汇编指令中直接以寄存器名来表示参与操作的数据地址，寄存器包括工作寄存器R0～R7、累加器ACC、寄存器B、数据指针DPTR。 汇编语言格式：MOV A, Rn ;n=0~7 如： 1MOV A, R1 ;将R1中的内容放入累加器ACC中 寄存器间接寻址二次寻址，寻址中寄存器的内容为操作数所存放的地址。第一次寻址得到寄存器的内容为(R0)、(R1)或(DPTR)，第二次寻址是将第一次寻址得到的寄存器内容作为地址，在其中存、取参与操作的数据。汇编语言中，寄存器前缀@是寄存器间接寻址的标志，有@R0、@R1、@DPTR等。 汇编语言格式：MOV A, @R0/R1/DPTR 如： 1MOV A, @DPTR ;将DPTR所指示的地址单元中的内容放入累加器ACC中 变址寻址由两个寄存器提供地址。若由ACC、PC提供，在汇编语言指令中寻址地址表示为@A+PC；若由ACC和DPTR提供，在汇编语言指令中寻址地址为@A+DPTR。其中，PC或DPTR被称为基址寄存器，A被称为变址寄存器，基址与变址相加为16位无符号加法。 若变址寄存器ACC中的内容加基址寄存器DPTR(或PC)中内容时，低8位有进位，则该进位直接加到高位，不影响进位标志。因变址寻址指令多用于查表，故常称为查表指令。 汇编语言格式：MOV A, @A+DPTP 如： 1MOV A, @A+DPTR ;将A+DPTR所指示的地址单元中的内容放入累加器ACC中 相对寻址以相对寻址指令的下一条指令的程序计数器PC的内容为基值，加上指令机器代码中的“相对地址”，形成新的PC值（要转移的指令地址）。指令机器代码中“相对地址”指的是用一个带符号的8位二进制补码表示的偏移字节数，其取值范围为-128～+127，负数表示向后转移，正数表示向前转移。rel代表一个8位带符号的偏移量，要转移的指令地址=(PC)+相对寻址指令字节数+rel，“( )”代表存储单元的内容。 汇编语言格式：SJMP rel 如： 1SJMP 08H ;指令代码为双字节，该指令将转到地址为(PC) + 02H + 08H 位寻址参与操作的数据为“位”，而不是字节，是对片内RAM中的位寻址区20H~2FH、SFR中11个可位寻址单元的位进行操作。bit代表内数据存储器RAM或SFR的直接寻址位。 汇编语言格式：MOV C, bit 如： 12MOV C, 00H ;或者写成MOV C, 20H.0 ;将字节地址20H的D0位内容放到位累加器C中 位寻址的位地址与直接寻址的字节地址形式完全一致，主要由操作码进行区分。 所有寻址方式的寻址范围总结如下表： 寻址方式 寻址存储器范围 立即寻址 程序存储器ROM 直接寻址 片内RAM低128KB，特殊功能寄存器SFR 寄存器寻址 工作寄存器R0~R7，A，C，DPTR，AB 寄存器间接寻址 片内RAM低128KB，片外RAM 变址寻址 程序存储器ROM （@A+DPTR，@A+PC） 相对寻址 程序存储器ROM（相对寻址指令的下一指令PC值加 -128~+127） 位寻址 片内RAM的20H到2FH字节地址的所有位，可位寻址SFR 指令系统数据传送指令 指令 功能 MOV (Move) 传送内部RAM和SFR的数据 MOVX (Move External RAM) 传送外部RAM的数据（外部RAM只能和累加器ACC之间进行传送数据） MOVC (Move Code) 读取并传送ROM数据表格中的数据 PUSH (Push onto Stack) 入栈 POP (Pop from Stack) 出栈 XCH (Exchange) 全字节交换 XCHD (Exchange low-order Digit) 低半字节交换 SWAP 高、低字节互换 如： 12345678MOV DPTR, #2000H ;外部RAM单元地址2000H传入DPTRMOVX A, @DPTR ;外部RAM 2000H单元的数据送入AMOVC A, @A + PC ;ROM A+PC单元的数据送入APUSH A ;将A中的数据放入堆栈POP A ;将堆栈顶层的数据放入AXCH A, R0 ;交换A和R0中的数据XCHD A, R0 ;交换A和R0中的低4字节数据SWAP A ;交换A的高、低4字节数据 算数运算指令 指令 功能 ADD (Addition) 加法 ADDC (Add with Carry) 带进位加法 SUBB (Subtract with Borrow) 带借位减法 INC (Increment) 加1 DEC (Decrement) 减1 MUL (Multiply) 乘法 DIV (Divide) 除法 DA (Decimal Adjust) 十进制调整 逻辑运算指令 指令 功能 ANL (AND Logic) 逻辑与 ORL (OR Logic) 逻辑或 XRL (Exclusive-OR Logic) 逻辑异或 CLR (Clear) 清零 CPL (Complement) 取反 RL (Rotate Left) 循环左移 RLC (Rotate Left throught the Carry flag) 带进位循环左移 RR (Rotate Right) 循环右移 RRC (Rotate Right throught the Carry flag) 带进位循环右移 控制转移指令 指令 功能 SJMP (Short Jump) 短转移，后接偏移量rel AJMP (Absolute Jump) 绝对转移，最大转移范围为2KB LJMP (Long Jump) 长转移，转移目的地址在0~64KB空间范围 ACALL (Absolute subroutine Call) 子程序绝对调用，最大调用范围为2KB LCALL (Long subroutine Call) 子程序长调用，调用64KB空间范围内的子程序 RET (Return from subroutine) 子程序返回 RETI (Return from Interruptio) 中断返回 CJNE (Compare Jump if Not Equal) 比较不相等则转移 DJNZ (Decrement Jump if Not Zero) 减１后不为０则转移 JZ (Jump if Zero) 结果为０则转移 JNZ (Jump if Not Zero) 结果不为０则转移 JC (Jump if the Carry flag is set) 有进位则转移 JNC (Jump if Not Carry) 无进位则转移 JB (Jump if the Bit is set) 位为１则转移 JNB (Jump if the Bit is Not set) 位为０则转移 JBC(Jump if the Bit is set and Clear the bit) 位为１则转移，并清除该位 NOP (No Operation) 空操作 位操作指令 指令 功能 SETB (Set Bit) 将某个位置1 伪指令伪指令在汇编时不产生机器码，不影响程序执行，仅指明在汇编时执行一些特殊的操作。 ORG 16位地址：起始指令，用在原程序或数据块的开始，指明此语句后面目标程序或数据块存放的起始地址。 如： 1ORG 1000H ;后面目标程序或数据块存放的起始地址为1000H [标号:]DB 字节数据项表：字节定义，将项表中的字节数据存放到从标号开始的连续字节单元中。 如： 1SEG: DB 32, 'A', 25H ;SEG~SEG+2地址单元依此存放20H、41H、25H [标号:]DW 双字节数据项表：字定义，定义16位地址表，16地址按低位地址存低位字。 如： 1TAB: DW 1234H, 25H ;TAB~TAB+3地址单元依此存放12H、34H、00H、25H [标号:]DS 数值表达式：保留字节，指示在程序存储器中保留与标号为起始地址的若干字节单元，单元个数由数值表达式指定。 如： 1L1: DS 32 ;L1地址开始保留32个存储单元 名字 EQU 表达式：等值指令，用与给一个表达式赋值或给字符串起名字。之后名字可用做程序地址，数据地址或立即数地址使用。名字必须是一字母开头的字母数字串。 如： 1SPACE EQU 10H ;程序中出现SPACE的地方都用10H代替 名字 DATA 直接字节地址：给8位内部RAM单元起个名字，名字必须是一字母开头的字母数字串，同一单元可起多个名字。 如： 1ERROR DATA 80H ;内部RAM 80H单元名为ERROR 名字 XDATA 直接字节地址：给8位外部RAM起个名字，名字规定同DATA伪指令。 如： 1IO_PORT XDATA 0CF04H ;外部RAM 0CF04H单元名为IO_PORT 名字 BIT 位指令：给一可位寻址的位单元起名，规定同DATA伪指令。 如： 1SWT BIT 30H ; 30H单元名为SWT [标号：] END：结束指令，指出源程序到此结束。 C语言编程51单片机采用C语言编程后，通过C51编译器进行编译后产生目标代码，下载到单片机内部即可运行。编程时，不同存储器的寻址和数据类型等细节问题都由编译器解决，无需像使用汇编语言那样都必须考虑到，而且其中还有丰富的子程序库可以直接调用，大大减小编程工作量。此外，编程过程中还可以将C语言和汇编语言交叉使用，提高开发效率。 数据类型C51中定义的数据类型比C语言多几种扩展数据类型： bit用于定义为变量的名字，编译器会对其分配地址，位变量分配在内部RAM的20H～2FH单元相应的位区域，位地址范围是00～7FH，共128个。 sbit用于定义位变量的名字和地址，地址是确定的且不用编译器分配。它是SFR中的可以进行位寻址的确定位，也可是内部RAM的20H～2FH单元中确定的位。 sfr用于访问一个内存单元，利用它可以访问51单片机内部的所有SFR，一般用来声明SFR。 存储器类型C51是面向8051单片机的程序语言，应用程序中使用的任何变量或常量必须以一定的存储器类型定位于单片机的相应的存储区域中，因此在定义变量类型时，还需要定义它的存储器类型： 对于单片机来说，访问片内RAM比访问片外RAM的速度要快得多，所以对于经常使用的变量应该置于片内RAM中，要用bdata、data、idata来定义；对于不经常使用的变量或规模较大的变量应该置于片外RAM中，要用pdata、xdata来定义。 中断函数使用C51编写中断程序时，需要遵循如下格式： 123void 函数名(void) interrupt 中断号 [using 工作寄存器组号] { ……} 其中，51单片机中的每个中断的中断号定义如下表： 中断号 中断源 中断入口地址 0 外部中断0 0003H 1 定时器0 000BH 2 外部中断1 0013H 3 定时器1 0018H 4 串口中断 0023H 后面用using声明RAM中的工作寄存器组号为0~4，用于暂存当前工作状态，也可以不予声明，使用默认组。 中断函数不带有任何参数，不能有返回值，不能被其他程序调用，同时还要做到尽量精简。 插入汇编想要在C程序里插入汇编代码，只要在汇编代码段前声明#pragma asm，段后声明#pragma endasm即可，格式如下： 12345#pragma asm 汇编语句行1 汇编语句行2 ……#pragma endasm 更新历史： 2017.11.23 完成初稿","link":"/2017/11/22/EE/msc51_2/"},{"title":"单片机原理(4)：系统扩展、外围接口","text":"当单片机内部功能不能满足应用系统的要求，经需要在片外连接相应的外围芯片以满足应用系统的要求的过程，叫做系统扩展。 通过外围接口技术，可以通过单片机来控制LED数码管、键盘、LCD显示屏等外部设备以及进行A/D、D/A转换，使单片机应用在更为广泛的领域。 系统扩展51单片机中集成了CPU、I/O口、定时器、中断系统、存储器等计算机的基本部件，外加电源、复位电路和时钟单路等简单的辅助电路即构成一个能够正常工作的最小系统，电路如下图所示： 51单片机有很强外部拓展能力，大部分常规芯片都可作为单片机的外围扩展电路，可进行的拓展有存储器扩展、I/O口扩展、串行总线接口存储器扩展等。 总线（Bus）是计算机内部CPU、内存、输入、输出等设备传递信息的公用通道，它是由导线组成的传输线束， 主机的各个部件通过它相连接，外部设备通过相应的接口电路再与总线相连接，从而形成了计算机硬件系统。按照计算机所传输的信息种类，计算机的总线可以划分为地址总线（Address Bus）、数据总线（Data Bus）及控制总线（Control Bus），分别用来传输数据、数据地址和控制信号。 单片机的系统扩展法有并行扩展法及串行扩展法，并行扩展法是用单片机的地址总线、数据总线及控制总线进行系统扩展，而串行扩展法是用SPI（Serial Peripheral Interface）总线或者I2C（Inter－Integrated Circuit）总线进行系统扩展。 系统总线扩展总线信号 对应引脚 扩展总线信号名 信号含义 P0口锁存输出 A0~A7 地址总线低8位 P2口 A8~A15 地址总线高8位 P0口 D0~D7 8位数据总线 ALE ALE 控制信号，地址锁存使能 PSEN PSEN 控制信号，程序存储器ROM使能，低电平有效 EA EA/VPP 控制信号，外部访问使能，低电平有效 RD RD(P3.7) 控制信号，读信号，低电平有效 WR WR(P3.8) 控制信号，写信号，低电平有效 51单片机包含的系统总线信号如上表所示，为了减少引脚数量，51系列单片机的扩展总线中，数据线和地址线采用了分时复用技术。 P0口除了作一般I/O口外，还可以分时复用传送地址总线信号的低8位（A0A7）和数据总线信号（D0D7），它在某一时刻传送的是低8位地址信号还是数据信号由ALE引脚的电平状态指明。 P2口除了作一般I/O口外，还可传输地址总线信号的高8位（A8~A15）。其他系统总线信号都为控制信号，在执行不同指令时，随硬件产生。 实际使用时，通过外接一个8位锁存器，可以实现地址信号和数据信号分离，如下图中使用74LS373实现信号分离电路原理图： 进行总线扩展时，由于地址总线的宽度为16位，故外部ROM或RAM的最大直接寻址范围都为64KB，同时它们的地址可以重叠使用。 地址译码法进行总线扩展时，首先要进行的是分配地址空间，就是把64KB的寻址空间通过地址译码的方法分成若干个大小相同的页面，其中低位地址线用来选择页内单元，高位地址线则用于页面的选择，不同的外部设备占用不同的页面。分配完成后，就要想办法进行地址译码，以方便单片机进行寻址。常用的地址译码方法有全地址译码法及“部分地址译码法”。 全地址译码是指所有的地址线都参与译码，所得到的地址空间是连续的，每一个数据单元与地址是一一对应的，其电路的结构一般比较复杂。例如一个存储页面大小为8KB，要把64KB的存储空间分成8个页面，则所有高位地址A13~A15都必须参与译码，产生8个独立的页面选择信号，形成一个连续的地址段，一般采用3-8译码器来实现，如下图： 部分译码是指只有一部分地址参与译码，所得到的地址空间是非连续的地址段，没有覆盖整个可寻址空间，一个数据单元可能与几个地址对应。如下图： 还有一种线选法是部分译码法的特殊形式，即对地址线不进行译码，直接用地址线来选通数据单元，其得到的地址空间也是非连续的。比如，不用外加译码电路，仅用高位地址线就把64KB的寻址空间区分成若干区，如图下图所示： 存储器扩展ROM扩展51单片机访问外部ROM时，其控制总线仅由ALE、PSEN和EA组成。当EA = 1，单片机要访问的地址的超出片内ROM的范围时，将自动转向进行片外ROM寻址。可以通过“MOVC A, @A+DPTR”这条指令访问外部ROM。指令执行过程中控制信号的逻辑关系和时序如下图： 采用2764扩展32KB ROM时，接线图如下： RAM扩展51单片机访问外部RAM时，控制总线由ALE、PSEN、RD及WR组成。当执行“MOVX A, @DRTP”、“MOVX @DPTR, A”指令时，进行读、写外部RAM的操作，指令执行过程中控制信号的逻辑关系和时序如下图： 采用SRAM芯片61128扩展32KB RAM时，接线图如下： 并行扩展I/O口的方法，与扩展RAM的方法基本一致。 外围接口技术LED显示器LED（Light Emitting Diode）显示器是若干个发光二极管组成的显示字段的显示器件。常用的LED显示器有七段数码显示器。 七段LED数码显示器由8个发光二极管组成，根据内部LED的连接形式不同，可分为共阴极和共阳极两种。共阴极发光二极管的阴极连接在一起，共阳极则阳极连接在一起，其电路连接如下图： 选用共阴极的数码管时，所有LED的阴极连接在一起接地，当某个LED的阳极接高电平则对应的LED便点亮。共阳极数码管则相反，当某个LED的阴极接低电平则对应的LED便点亮。每次把某些特定的LED点亮，就能使数码管用来显示一些数字或符号，LED数码管共8位，正好是一个字节，习惯上以“a”段对应段码字节为最低位，这样，只需要输入不同的段码，就能获得不同的显示。 LED数码管的显示方式一般都采用动态显示，这种方法节省I/O口，然而在这种方法在任意时刻只有一位显示器能被点亮，显示位数较多时，需要采用动态扫码，动态扫描的频率有一定要求，要使人眼无法察觉。频率过低的话，LED将会出现闪烁现象，而频率太高，每个LED点亮的时间太短，LED的亮度太低，肉眼无法看清。程序上常采用的是调用延时子程序的方法，选通某一位LED使其点亮并保持几个ms左右的时间。 键盘在单片机应用系统中，往往需要向单片机输入一些指令或参数，而单片机的运行结果有时也需要通过外部显示器或打印机输出出来，以供操作者及时了解和掌握单片机的运行状况。这样就构成了一种人机的交互接口。由于单片机本身的特点决定了其无法具备键盘、显示器、打印机等人机交互部件，所以只能通过其I/O口来扩展这些功能。 键盘可分为编码键盘和非编码键盘。编码键盘上闭合键的识别由专门的硬件实现，非编码键盘则通过软件来即时实别。单片机一般都采用的是非编码键盘。 单片机系统中所使用的键盘都是机械式的弹性按键，因为存在机械触点的弹性作用，在按键闭合和弹起的瞬间都会出现抖动，按键抖动一般会持续5~10ms，为使一次按键仅被处理一次，必须消除按键抖动。消除按键抖动可以采用软件消抖或硬件消抖。 硬件消抖通常采用RS触发器来实现，需要在电路上改进，较为复杂。软件消抖更为简单，在检测到有按键闭合时，延时一小段时间之后再次检测，如果仍然检测到按键闭合，则认为按键真正闭合。 键盘的连接单片机接口的方式有独立式和矩阵式。独立式键盘的每个键都单独与一个I/O口相连，各键的输入状态互不影响。单片机通过检测对应I/O口的电平高低就可以判断出是哪个键被按下，然而当按键数目较多时，占用的I/O口也较多。 需要的按键数量较多时，通常都采用矩阵式的连接方式。矩阵式键盘由行线和列线组成，所以有时也称行列式键盘。按键位于行、列线的交叉点上，行、列线又分别与I/O端口相连。其连接方式如下图： 矩阵式键盘的识别方法通常采用扫描法。先令某根列线，例如0号列线输出为“0”，其余三根列线输出为“1”。再依次扫描行线的状态，如有某根行线为“0”则表示该行线与0号列线交叉处的键被按下。如果行线都为“1”，则没有键被按下。同样，可以依次将下一根列线置“0”，同时其余列线“1”，并扫描行线，这样就可以判断出被按键的位置。 更新历史： 2017.11.28 完成初稿","link":"/2017/11/27/EE/msc51_4/"},{"title":"树莓派的基本配置方法","text":"入手一块树莓派3B+，准备用它完成毕业设计，顺便深入学习一波Linux。这里先学习官方Raspbian系统的烧录和基本配置过程。 系统烧录树莓派要用SD卡启动，所以最好有一张16G以上的内存卡，将系统烧录在其中，它也将作为树莓派的硬盘。 基于开源特性，树莓派支持很多系统，如Ubuntu、Chrome OS、Android、Window IOT甚至是与PC功能一致的Window 10 ARM。官方提供的系统是32位的基于Debian的Raspbian，虽然从3B开始就有了64位的CPU，且据说64位的系统将使树莓派的性能最高提升30倍，但官方一直都没有要发布64位Raspbian的意思。第三方的64位的系统就留着以后折腾，现在先装上官方出品的稳定Raspbian。 首先到树莓派官网上下载Raspbian系统镜像，里面有三个版本可选： 第一个是带有桌面环境和一些推荐安装的软件的版本，第二个则不带有推荐安装的软件，第三个不带桌面环境。根据自己的需要，选择下载。下载期间，安装好SDFormatter和Win32DiskImager软件，并使用SDFormatter将SD卡进行格式化： 下载完成相应的压缩包并将系统镜像文件（.img）解压出来后，使用Win32DiskImager选择镜像文件并Write到SD卡： 系统写入完成后，系统就烧录到了SD卡中。SD卡的空间将被分成两部分，一部分是Windows文件系统下无法识别的Linux磁盘分，另一部分是系统的开机引导分区boot，所以不要理会Windows的格式化提示而将烧录到SD的系统又给格式化了。 网络配置烧录完系统后，如果身边有网线以及支持HDMI输入的显示器（部分高端笔记本也支持），则可以直接将弹出SD卡并放入树莓派的SD卡卡槽中，启动系统。否则，先不急着弹出SD卡，而是进行一些简单的网络配置，以方便远登录我们的树莓派。 首先开启系统的ssh，只需要在boot分区下，新建一个文件，并重命名为”ssh“，不带任何后缀名： ssh（Secure Shell）是专为远程会话和其他网络服务提供安全性的应用层协议，可以用它来远程访问树莓派上Raspbian系统的命令行。在Windows上大家都推荐使用带有ssh功能的PuTTy来连接树莓派，个人发现git也可以用来建立ssh连接，且操作上更为方便，建议下载使用。 接下来配置树莓派的wifi网络，还是在boot分区下，新建文件并重命名为“wpa_supplicant.conf”，使用编辑器打开并填写内容如下： 1234567891011121314151617country=CNctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1network={ssid=&quot;wifi名称-1&quot;psk=&quot;密码-1&quot;key_mgmt=WPA-PSKpriority=1}network={ssid=&quot;wifi名称-2&quot;key_mgmt=NONEpriority=2scan_ssid=1} 其中network中各参数的含义如下： ssid：网络名称 psk：密码 key_mgmt：加密方式，一般为“WPA—PSK”，无密码则为“NONE” priority：连接优先级，数字越大优先级越高 scan_ssid：连接隐藏的wifi时需要将其指定为“1” 想要通过PC远程登录树莓派，PC和树莓派需要连接在相同的局域网中。对于笔记本，在连接到其他有线/无线网络后可开启移动热点，并在“wpa_supplicant.conf”中填写该热点的相关信息；对于不带无线网卡的台式PC，则另外需要一个wifi路由器，设置树莓派开机自动连接到该热点并用网线将PC连接到路由器上。 完成以上配置后，就可以将SD卡弹出，并放入树莓派的SD卡卡槽中。 ssh登录树莓派采用的是Micro USB电源接口，3B+上标识的是5V/2.5A。其实在不挂载大功率设备的情况下，可以直接用手机的充电头或PC的USB接口进行供电。 树莓派上有两个指示灯，接入电源后，电源相关的红色的指示灯将常亮，否则说明电源方面出现供电不足等问题。绿色的是内存卡读取指示灯，闪烁时说明系统在读取内存卡。 在同一局域网下想用ssh远程登录树莓派，首先需要知道树莓派IP地址。如果让树莓派开机自动连接的是笔记本开启的移动热点，Windows10下直接在“移动热点”的设置界面下即可获得树莓派的IP地址，其设备名称为“raspberrypi”： 如果树莓派连接的是路由器，则需要用浏览器访问路由器的管理界面（访问地址一般为“192.168.1.1”，因路由器不同而异），查看连接到路由器的所有设备信息，找到名为”raspberrypi”的设备并记下它的IP地址。 因为树莓派的IP地址是通过DHCP协议自动分配到的，所以每次重新打开移动热点/路由器时该IP地址都会发生变化，后面会讲到静态IP的设置方法。 获得树莓派的IP地址后，就可以打开安装好的git bash，输入命令： ssh pi@树莓派IP地址 其中，“pi”是树莓派的默认用户名。随后根据提示，输入”yes”确认连接密钥，输入初始密码“raspberry”（输入时不会显示），即可ssh远程登录到Raspbian的命令行： ssh远程登录到命令行后，就可以通过raspi-config对系统进行一些基本的设置。使用如下命令打开raspi-config: sudo raspi-config 随后进入如下界面： 首先依次操作：Boot Options -&gt; Splash Screen -&gt; Yes，打开欢迎界面。 随后再：Interfacing Options -&gt; VNC -&gt; Yes，打开VNC服务，方便之后远程登录桌面。 接着：Adcanced Options -&gt; Expend Filesystem，将树莓派的分区扩展到整个SD卡。 完成这几项设置后，就可以退出raspi-config，并选择确认进行重启。 远程桌面在没有显示器的情况下，就要通过远程访问Raspbian桌面。Windows下进行远程桌面连接，一种方法是先通过ssh连接在Raspbian上安装xrdp软件并进行一些设置，这样就可以用Windows自带的远程桌面软件（mstsc）实现远程桌面连接。另外一种方法是使用官方推荐的VNC方式连接到远程桌面。 使用VNC方式进行远程桌面连接需要用到VNC Viewer软件，它支持包含树莓派在内的多种平台，甚是方便。 在官网上下载安装好该软件后，输入树莓派的IP地址，按提示输入用户名和密码： 即可远程登录Raspbian桌面： 基本设置更换源在使用欢迎程序进行设定前，由于官方源在国内的速度不够理想，所以最好将树莓派的源更换为国内的清华大学镜像源。打开终端，输入： sudo leafpad /etc/apt/sources.list 编辑sources.list，注释掉原来的内容，并加入以下内容： deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi 再输入： sudo leafpad /etc/apt/sources.list.d/raspi.list 编辑raspi.list，注释掉原来的内容，并加入以下内容： deb http://mirror.tuna.tsinghua.edu.cn/raspberrypi/ stretch main ui 最后在终端执行apt-get update、upgrade即可。 分辨率如果按前面的设置开启了欢迎界面，那么第一次登录到Raspbian桌面是将会进入到欢迎界面。其中可以方便地更改“pi”账户的密码，以及设置国家、语言等，它将自动下载相应的语言包，按提示一步一步完成即可重启系统即可显示中文。 之后，可以通过终端再次进入raspi-config，通过：Advanced Options -&gt; Resolution 选择你满意的桌面显示分辨率，重启后即完成相关设置。 解锁root可以设置root密码，以获得root用户权限。root账户默认没有密码，但是账户是锁定的。输入命令： sudo passwd root 之后两次输入想要设置的root密码，再输入命令： sudo passwd –unlock root 即可解锁root账户。 静态IP树莓派连接到无线网时，默认由路由器采用DHCP协议自动分配IP地址，且每次路由器重启后树莓派分配到的IP地址都会发生变化。要设置静态IP地址，只要在终端中输入： sudo leafpad /etc/dhcpcd.conf 编辑dhcpcd.conf，在其中添加如下内容： 1234567891011interface eth0static ip_address=192.168.x.x/24static routers=192.168.x.1static domain_name_servers=192.168.x.1interface wlan0static ip_address=192.168.x.x/24static routers=192.168.x.1static domain_name_servers=192.168.x.1 其中： ip_address：静态ip地址 router：网关地址 domain_name_servers：DNS地址 其中上面的连接有线网下的静态IP，下面则是无线网下的。其中的x需要根据具体情况设置，静态IP一定要和路由器的网段一致，否则无法连接到网络。 例如，我的树莓派连接的是笔记本开的移动热点，获知该移动热点的网段为192.168.137.x后，我将静态IP设置如下； 这样就能够通过一个固定的IP地址来访问远程桌面，另外在Windows10开启移动热点后的已连接的设备中，不会显示IP地址固定的设备的信息。 到这里，树莓派的开发环境也就基本配置完成了。 更新历史： 2019.2.28 完成初稿","link":"/2019/02/27/EE/raspi/"},{"title":"YOLOv3模型详解","text":"2015年华盛顿大学的Joseph Redmon等人提出的YOLO算法中，将目标检测直接作为回归问题来解决，而使用一个一体化的CNN实现了端到端（End to End）检测，为目标检测问题提供了另外一种解决思路的同时，基于深度学习的目标检测算法也自此有了单步和多步之分。到2018年，YOLO已经更新到了v3版本，这里对该模型进行分析。 主干架构YOLOv3中用来对输入进行特征提取的主干架构为Darknet-53，它是YOLOv2中用到的Darknet-19的升级版本。正如其名，Darknet-53共包含53个卷积层，该网络在Darknet-19的基础上进一步引入了更多的残差结构，网络层数量虽然只有ResNet-101的一半，但它在性能上能够媲美ResNet-152的同时，运算速度也远超这两者。整个Darknet-53的网络结构如下表。 Darknet-53中主要包含卷积层（Convolutional）和残差块（Residual）这两种网络层。在每个卷积层中，使用各种卷积核完成卷积运算后，都进行了一次批标准化，之后再使用Leaky ReLU函数进行激活；表格中的各个方框即代表一类残差块，方框右边标识的是相应残差块的数量，每个残差块中均包含两个卷积层。它们的具体结构如下图所示： 卷积过程中，除了一直使用$3\\times3$大小的卷积核来提取特征外，还延续了Darknet-19中使用$1\\times1$大小的卷积核来增加特征图维度的思想，两类卷积核的数量都随着网络层数的增加而呈$2$的幂次方增长。网络中没有引入全局平均池化层，而采用简单的增大卷积步幅策略来压缩特征。每次将卷积步幅从$1$增加$2$，特征图的大小就会成原来的一半，经过$5$次这样的下采样（Subsampled）操作后，最后输出的特征图大小被压缩到$13\\times13$，即输入的$1/32$。 多尺寸预测利用主干结构提取到输入的特征图后，YOLOv3模型的后续检测流程下图： 图中的卷积层（Convolutional）与主干架构中用到的卷积层结构完全一致，每个卷积集（Convolutional Set）内则包含了$5$个这样的卷积层。在后续的处理过程中，用这些卷积层来进一步提取特征，以得到更高层次的抽象特征。卷积集的具体结构如下图： 在输入的尺寸为$416\\times416$的情况下，Darknet-53的第三、四、五种残差块中，最后将分别提取到$52\\times52$、$26\\times26$、$13\\times13$这三种尺寸的特征图。YOLOv3借鉴了FPN中的思想，使用了这三种大小的特征图来分别预测不同尺寸的包围盒。对于第五种残差块最后输出的大小为$13\\times13$的特征图，由于该特征图中的各网格（像素点）具有最大（$32\\times32$）的感受野，所以该特征图适合用来检测图像中包含的较大目标。将$13\\times13$大小的特征图输入一个卷积集中，得到更高层次的抽象特征，再由结构图所示的右边分支，将特征输入一个$3\\times3$的卷积层，就能得到第一组预测结果。 第四种残差块输出大小为$26\\times26$的特征图，它具有中等大小（$16\\times16$）的感受野，适合用来检测图像中包含的中等大小的目标。为充分利用提取到的各类特征，对$13\\times13$的特征图经过卷积集后得到的特征利用插值算法进行上采样，而输出大小为$26\\times26$的特征图。将该特征图与残差块输出的特征图堆叠（Concatenate）到一起，再输入一个卷积集中和一个$3\\times3$的卷积层，就能得到第二组预测结果。同理，第三种残差块输出的特征图（$52\\times52$）具有的感受野最小（$8\\times8$），适合用来检测图像中包含的小目标。对前面得到的特征进行上采样后，与残差块输出的特征图堆叠在一起，再经过一系列的卷积层就能得到第三组预测结果。 Anchor Box机制YOLOv3和YOLOv2一样，仍采用了Faster R-CNN中提出的Anchor Box机制从特征图中检测各类目标。Anchor Box机制相当于给模型提供了更多了先验知识，拥有了这些先验知识，模型只要学会预测真实包围盒各参量相对于Anchor Box的偏移大小，而非Bounding Box的各个具体参数，该机制使模型的学习过程容易了很多。此外，YOLOv2中还使用了K-Means聚类算法对训练集中真实Bounding Box的大小进行聚类运算，这样得到几种Anchor Box尺寸要比原Faster R-CNN手工选取的尺寸具有更优的先验性。 使用K-Means对真实Bounding Box进行聚类的过程中，为减小误差，采用各Bounding Box之间的交并比（IOU）来代替传统的欧几里得距离作为聚类标准，具体的距离度量公式为：$$d(box,centrid)=1-IOU(box,centrid)$$ 在YOLOv2中，利用了上述度量标准，从$K=1$开始对COCO数据集中所有的Bounding Box进行了多次K-Means运算，进行各方面权衡后，最终采用了$K=9$时聚类结果，得到了$9$种尺寸的Anchor Box。YOLOv2中最后只输出一个张量，因此直接将这$9$个Anchor Box运用到特征图的每一个网格上，即特征图中的每一个网格最后都要输出$9$个Bounding Box预测结果。YOLOv3中依然采用这些尺寸的Bounding Box，但把其中最大尺寸的$3$个Bounding Box分配到了$13\\times13$大小的特征图上，中等尺寸大小的$3$个分配给大小为$26\\times26$特征图，剩下的$3$个小尺寸Anchor Box则分配到$52\\times52$大小的特征图上。Anchor Box的具体大小及分配情况见下表： 如图下图所示，将多尺寸预测与Anchor Box机制结合，不同大小的物体分别由不同特征图中的Anchor Box负责检测，这有效提高了目标的检测精度。 结果输出YOLOv3检测模型输出的是三个尺度不一的张量，需要把它们进行拆分，才能得到想要的检测结果。从前面的流程图可知，在输入大小为$416\\times416$的情况下，输出张量分别包含$13\\times13$、$26\\times26$、$52\\times52$个网格。除网格数量外，这些张量的结构都是相同的，这里仅以其中大小为$13\\times13$张量为例进行分析。 如下图，由于各尺度的特征图都使用了$3$种不同的Anchor Box来实现包围盒回归，因此输出张量的各网格中均包含了$3$个Bounding Box预测结果，各预测结果均由Bounding Box的$4$个位置（$x$、$y$、$w$、$h$）、$1$个置信度评分（bbox score）以及$C$类目标的$C$个存在概率值（classes probs）组成。作者使用了包含$80$类常见物体的COCO数据集来训练YOLOv3，因此一个预测结果是一个$85$维的向量，整个张量的深度则为$255$。 和YOLOv2中一样，为使Anchor Box机制下的模型易于训练，训练样本中各Bounding Box的真实参数都使用了强约束方法进行编码。用$G_x$、$G_y$、$G_w$、$G_h$来表示某个Bouding Box的实际中心坐标、长度及宽度，则它们的编码结果$t_x$、$t_y$、$t_w$、$t_h$将由下列公式求得：$$t_x = G_x - c_x$$ $$t_y = G_y - c_y$$ $$t_w = \\log⁡ \\frac{G_w}{p_w}$$ $$t_h = \\log⁡\\frac{G_h}{p_h}$$ 其中$c_x$、$c_y$是把输入的图像划分为$S\\times S$个网格后，某个目标的中心点所在网格的左上角坐标；$p_w$、$p_h$则表示Anchor Box的大小。因此，编码后得到的$t_x$、$t_y$表示的是Bounding Box的中心坐标相对于其所在网格的偏移量，$t_h$、$t_w$则表示Bounding Box的大小相对于Anchor Box的放缩尺度。使用这些编码后的数据作为训练样本的标记，YOLOv3模型将通过这些样本，学会如何通过微调Anchor Box的大小或位置来得到真实的Bounding Box参数。 因此，从输出的张量中拆分出各个结果后，还需要对它们进行解码。用$b_x$、$b_y$、$b_w$、$b_h$表示某个网格中某个Bounding Box的实际参数，由前面的编码公式，它们与其对应的预测值$t_x$、$t_y$、$t_w$、$t_h$之间存在如下转换关系：$$b_x = \\sigma(t_x) + c_x$$ $$b_y =\\sigma(t_y) + c_y$$ $$b_w = p_w e^(t_w)$$ $$ b_h = p_h e^(t_h)$$ 其中，$\\sigma$表示是Sigmoid函数，用它来把$t_x$、$t_y$的值缩放到$0$和$1$之间。对$t_x$、$t_y$放缩后的结果再加上对应网格的左上角坐标$c_x$、$c_y$，就能得到预测的实际中心坐标$b_x$、$b_y$，Anchor Box的宽度$p_w$及高度$p_h$分别乘以它们对应的放缩值则得到预测的Bounding Box 大小$b_w$、$b_h$。具体的转换过程如下图： 对于输出张量中的Bounding Box的置信度评分以及类别概率，将它们拆分出来后只需要使用Sigmoid函数将它们的值约束在0到1的范围内即可。把输出的三个张量全部拆分后，将得到多达10647个Bounding Box预测结果。对于这些结果，需要采用非极大值抑制滤除其中无效或冗余的Bounding Box，才能得到最终的检测结果。 损失函数由于均方误差的优化过程较为简单，因此所有的YOLO算法都采用了由多个均方误差组成的损失函数。在YOLOv3中，用到的损失函数如下：$$\\mathcal{L}= \\frac12 \\sum_{i=1}^{10647} \\lambda_\\text{obj}\\cdot[(2 - w_i \\cdot h_i) \\cdot \\sum_{r\\in(x,y,w,h)}(r_i-\\hat{r_i})^2 + \\sum_{c\\in classes}(p_i(c) - \\hat{p_i} (c))^2] +(C_i - \\hat{C_i})^2 $$ 整个损失函数可被分为Bounding Box位置损失（$x_i$、$y_i$、$w_i$、$h_i$）、置信度评分损失（$C_i$）以及分类损失（$p_i(c)$）这三大部分。只有当网格中存在某类目标时，$\\lambda_{obj}$的值才为$1$，否则$\\lambda_{obj}$的值为$0$，即对于不存在目标的网格，只计算其置信度评分损失。考虑到Bounding Box的大小会造成损失计算上的不均衡，损失函数中增加的额外项$(2 - w_i\\cdot h_i)$可以增加小Bounding Box的位置损失，同时降低大Bounding Box的损失，从而解决该问题。 参考资料 YOLO v3网络结构分析-CSDN yolo系列之yolo v3【深度解析】-CSDN 史上最详细的Yolov3边框预测分析-知乎专栏 更新历史： 2019.6.20 完成初稿","link":"/2019/06/18/ML/cnn_yolov3/"},{"title":"【Coursera】深度学习(10)：自然语言处理","text":"自然语言处理（Natural Language Processing, NLP）是人工智能和语言学领域的学科分支，它研究实现人与计算机之间使用自然语言进行有效通信的各种理论和方法。 词嵌入前面介绍过，处理文本序列时，通常用建立字典后以one-hot的形式表示某个词，进而表示某个句子的方法。这种表示方法孤立了每个词，无法表现各个词之间的相关性，满足不了NLP的要求。 词嵌入（Word Embedding）是NLP中语言模型与表征学习技术的统称，概念上而言，它是指把一个维数为所有词的数量的高维空间（one-hot形式表示的词）“嵌入”到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。 如上图中，各列分别组成的向量是词嵌入后获得的第一行中几个词的词向量的一部分。这些向量中的值，可代表该词与第一列中几个词的相关程度。 使用2008年van der Maaten和Hinton在论文[Visualizing Data using t-SNE]中提出的t-SNE数据可视化算法，将词嵌入后获得的一些词向量进行非线性降维，可到下面的映射结果： 其中可发现，各词根据它们的语义及相关程度，分别汇聚在了一起。 对大量词汇进行词嵌入后获得的词向量，可用来完成**命名实体识别（Named Entity Recognition)**等任务。其中可充分结合迁移学习，以降低学习成本，提高效率。 好比前面讲过的用Siamese网络进行人脸识别过程，使用词嵌入方法获得的词向量可实现词汇的类比及相似度度量。例如给定对应关系“男性（Man）”对“女性（Woman）”，要求机器类比出“国王（King）”对应的词汇，通过上面的表格，可发现词向量存在数学关系“Man - Woman $\\approx$ King - Queen”，也可以从可视化结果中看出“男性（Man）”到“女性（女性）”的向量与“国王（King）”到“王后（Queen）”的向量相似。词嵌入具有的这种特性，在2013年Mikolov等发表的论文[Linguistic Regularities in Continuous Space Word Representations]中提出，成为词嵌入领域具有显著影响力的研究成果。 上述思想可写成一个余弦（cos）相似度函数：$$ sim(u, v) = \\frac{u^T v}{\\mid\\mid u \\mid\\mid_2 \\mid\\mid v \\mid\\mid_2} $$以此度量词向量的相似度。 词嵌入方法词嵌入的方法包括人工神经网络、对词语同现矩阵降维、概率模型以及单词所在上下文的显式表示等。以词汇的one-hot形式作为输入，不同的词嵌入方法能以不同的方式学习到一个嵌入矩阵（Embedding Matrix），最后输出某个词的词向量。 将字典中位置为$i$的词以one-hot形式表示为$o_i$，嵌入矩阵用$E$表示，词嵌入后生成的词向量用$e_i$表示，则三者存在数学关系：$$E \\cdot o_i = e_i$$ 例如字典中包含10000个词，每个词的one-hot形式就是个大小为$10000 \\times 1$的列向量，采用某种方法学习到的嵌入矩阵大小为$300 \\times 10000$的话，将生成大小为$300 \\times 1$的词向量。 神经概率语言模型采用神经网络建立语言模型是学习词嵌入的有效方法之一。2003年Bengio等人的经典之作[A Neural Probabilistic Language Model]中，提出的神经概率语言模型，是早期最成功的词嵌入方法之一。 模型中，构建了了一个能够通过上下文来预测未知词的神经网络，在训练这个语言模型的同时学习词嵌入。例如将下图中上面的句子作为下面的神经网络的输入： 经过隐藏层后，最后经Softmax将输出预测结果。其中的嵌入矩阵$E$与$w$、$b$一样，是该网络中的参数，需通过训练得到。训练过程中取语料库中的某些词作为目标词，以目标词的部分上下文作为输入，训练网络输出的预测结果为目标词。得到了嵌入矩阵，就能通过前面所述的数学关系式求得词嵌入后的词向量。 Word2VecWord2Vec（Word To Vectors）是现在最常用、最流行的词嵌入算法，它由2013年由Mikolov等人在论文[Efficient Estimation of Word Representations in Vector Space]中提出。 Word2Vec中的Skip-Gram模型，所做的是在语料库中选定某个词（Context），随后在该词的正负10个词距内取一些目标词（Target）与之配对，构造一个用Context预测输出为Target的监督学习问题，训练一个如下图结构的网络： 该网络仅有一个Softmax单元，输出Context下Target出现的条件概率：$$p(t \\mid c) = \\frac{exp(\\theta_t^T e_c)}{\\sum_{j=1}^m exp(\\theta_j^T e_c)} $$上式中$\\theta_t$是一个与输出的Target有关的参数，其中省略了用以纠正偏差的参数。训练过程中还是用交叉熵损失函数。 选定的Context是常见或不常见的词将影响到训练结果，在实际中，Context并不是单纯地通过在语料库均匀随机采样得到，而是采用了一些策略来平衡选择。 Word2Vec中还有一种CBOW（Continuous Bag-of-Words Model）模型，它的工作方式是采样上下文中的词来预测中间的词，与Skip-Gram相反。 以上方法的Softmax单元中产生的计算量往往过大，改进方法之一是使用分级Softmax分类器（Hierarchical Softmax Classifier），采用霍夫曼树（Huffman Tree）来代替隐藏层到输出Softmax层的映射。 此外，Word2Vec的作者在后续论文[Distributed Representations of Words and Phrases and their Compositionality]中提出了负采样（Negative Sampling）模型，进一步改进和简化了词嵌入方法。 负采样模型中构造了一个预测给定的单词是否为一对Context-Target的新监督学习问题，采用的网络结构和前面类似： 训练过程中，从语料库中选定Context，输入的词为一对Context-Target，则标签设置为1。另外任取$k$对非Context-Target，作为负样本，标签设置为0。只有较少的训练数据，$k$的值取520的话，能达到比较好的效果；拥有大量训练数据，$k$的取值取25较为合适。 原网络中的Softmax变成多个Sigmoid单元，输出Context-Target（c,t）对为正样本（$ y = 1 $)的概率：$$p(y = 1 \\mid c, t) = \\sigma(\\theta_t^T e_c) $$其中的$\\theta_t$、$e_c$分别代表Target及Context的词向量。通过这种方法将之前的一个复杂的多分类问题变成了多个简单的二分类问题，而降低计算成本。 模型中还包含了对负样本的采样算法。从本质上来说，选择某个单词来作为负样本的概率取决于它出现频率，对于更经常出现的单词，将更倾向于选择它为负样本，但这样会导致一些极端的情况。模型中采用一下公式来计算选择某个词作为负样本的概率：$$p(w_i) = \\frac{f(w_i)^{\\frac{3}{4}}}{\\sum_{j=0}^m f(w_j)^{\\frac{3}{4}}} $$其中$f(w_i)$代表语料库中单词$w_i$出现的频率。 GloVeGloVe（Global Vectors）是另一种现在流行的词嵌入算法,它在2014年由Pennington等人在论文[GloVe: Global Vectors for Word Representation]中提出。 Glove模型中，首先基于语料库统计了词的共现矩阵$X$，$X$中的元素为$X_{i,j}$，表示整个语料库中单词$i$和单词$j$彼此接近的频率，也就是它们共同出现在一个窗口中的次数。之后要做的，就是优化以下代价函数：$$J=\\sum_{i,j}^N f(X_{i,j})(\\theta_i^T e_j + b_i + b_j - log(X_{i,j}))^2$$其中$\\theta_i$、$e_j$分是单词$i$和单词$j$的词向量，$b_i$、$b_j$是两个偏差项，$f()$是一个用以防止$X_{i,j} = 0$时$log(X_{i,j})$无解的权重函数，词汇表的大小为$N$。 （以上优化函数的推导过程见参考资料中的“理解GloVe模型”） 最后要说明的是，使用各种词嵌入方法学习到的词向量，并不像最开始介绍词嵌入时展示的表格中Man、Woman、King、Queen的词向量那样，其中的值能够代表着与Gender、Royal等词的的相关程度，实际上它们大都超出了人们的能够理解范围。 词嵌入应用：情感分类器NLP中的情感分类，是对某段文字中所表达的情感做出分类，它能在很多个方面得到应用。训练情感分类模型时，面临的挑战之一可能是标记好的训练数据不够多。然而有了词嵌入得到的词向量，只需要中等数量的标记好的训练数据，就能构建出一个表现出色的情感分类器。 如上图，要训练一个将左边的餐厅评价转换为右边评价所属星级的情感分类器，也就是实现$x$到$y$的映射。有了用词嵌入方法获得的嵌入矩阵$E$，一种简单的实现方法如下： 方法中计算出句中每个单词的词向量后，取这些词向量的平均值输入一个Softmax单元，输出预测结果。这种简单的方法适用于任何长度的评价，但忽略了词的顺序，对于某些包含多个正面评价词的负面评价，很容易预测到错误结果。 采用RNN能实现一个表现更加出色的情感分类器，此时构建的模型如下： 这是一个“多对一”结构的循环神经网络，每个词的词向量作为网络的输入，由Softmax输出结果。由于词向量是从一个大型的语料库中获得的，这种方法将保证了词的顺序的同时能够对一些词作出泛化。 词嵌入除偏在词嵌入过程中所使用的语料库中，往往会存在一些性别、种族、年龄、性取向等方面的偏见，从而导致获得的词向量中也包含这些偏见。比如使用未除偏的词嵌入结果进行词汇类比时，“男性（Man）”对“程序员（Computer Programmer）”将得到类似“女性（Woman）”对“家务料理人（Homemaker）”的性别偏见结果。2016年Bolukbasi等人在论文[Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings]中提出了一些消除词嵌入中的偏见的方法。 这里列举消除词向量存在的性别偏见的过程，来说明这些方法。（摘自第二周课后作业） 1.中和本身与性别无关词汇 中和（Neutralize）“医生（doctor）”、“老师（teacher）”、“接待员（receptionist）”等本身与性别无关词汇中的偏见，首先计算$g = e_{woman}-e_{man}$，用“女性（woman）”的词向量减去“男性（man）”的词向量，得到的向量$g$就代表了“性别（gender）”。假设现有的词向量维数为50，那么对某个词向量，将50维空间分成两个部分：与性别相关的方向$g$和与$g$正交的其他49个维度$g_{\\perp}$。如下左图： 除偏的步骤，是将要除偏的词向量，左图中的$e_{receptionist}$，在向量$g$方向上的值置为$0$，变成右图所示的$e_{receptionist}^{debiased}$。所用的公式如下:$$e^{bias}_{component} = \\frac{e \\cdot g}{||g||_2^2} \\times g$$ $$e_{receptionist}^{debiased} = e - e^{bias}_{component}$$ 2.均衡本身与性别有关词汇 对“男演员（actor）”、“女演员（actress）”、“爷爷（grandfather）”等本身与性别有关词汇，如下左图，假设“女演员（actress）”的词向量比“男演员（actor）”更靠近于“婴儿看护人（babysit）”。中和“婴儿看护人（babysit）”中存在的性别偏见后，还是无法保证它到“女演员（actress）”与到“男演员（actor）”的距离相等。对一对这样的词，除偏的过程是均衡（Equalization）它们的性别属性。 均衡过程的核心思想是确保一对词（actor和actress）到$g_{\\perp}$的距离相等的同时，也确保了它们到除偏后的某个词（babysit）的距离相等，如上右图。 对需要除偏的一对词$w1$、$w2$，选定与它们相关的某个未中和偏见的单词$B$之后，均衡偏见的过程如下公式：$$\\mu = \\frac{e_{w1} + e_{w2}}{2}$$ $$\\mu_{B} = \\frac {\\mu \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} \\times \\text{bias_axis}$$ $$\\mu_{\\perp} = \\mu - \\mu_{B}$$ $$e_{w1B} = \\frac {e_{w1} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} \\times \\text{bias_axis}$$ $$e_{w2B} = \\frac {e_{w2} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} \\times \\text{bias_axis}$$ $$e_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} \\times \\frac{e_{\\text{w1B}} - \\mu_B} {||(e_{w1} - \\mu_{\\perp}) - \\mu_B)||_2} $$ $$e_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} \\times \\frac{e_{\\text{w2B}} - \\mu_B} {||(e_{w1} - \\mu_{\\perp}) - \\mu_B)||_2} $$ $$e_1 = e_{w1B}^{corrected} + \\mu_{\\perp}$$ $$e_2 = e_{w2B}^{corrected} + \\mu_{\\perp}$$ 参考资料 吴恩达-序列模型-网易云课堂 Andrew Ng-Sequence Model-Coursera Deep Learning in NLP（一）词向量和语言模型 从SNE到t-SNE再到LargeVis word2vec前世今生 Word2Vec导学第二部分-负采样-csdn 理解GloVe模型-csdn 课程代码与资料-GitHub 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2018.03.08 完成初稿","link":"/2018/03/01/ML/deep_learning_10/"},{"title":"【Coursera】深度学习(3)：优化神经网络(1)","text":"想提高一个深层神经网络的训练效率，需从各个方面入手，优化整个运算过程，同时预防其中可能发生的各种问题。 本文涉及优化深层神经网络中的数据划分，模型估计，预防过拟合，数据集标准化，权重初始化，梯度检验等内容。 数据划分想要建立一个神经网络模型，首先，就是要设置好整个数据集中的训练集（Training Sets）、开发集（Development Sets）和测试集（Test Sets）。 采用训练集进行训练时，通过改变几个超参数的值，将会得到几种不同的模型。开发集又称为交叉验证集（Hold-out Cross Validation Sets），它用来找出建立的几个不同模型中表现最好的模型。之后将这个模型运用到测试集上进行测试，对算法的好坏程度做无偏估计。通常，会直接省去最后测试集，将开发集当做“测试集”。一个需要注意的问题是，需要保证训练集和测试集的来源一致，否则会导致最后的结果存在较大的偏差。 模型估计 如图中的左图，对图中的数据采用一个简单的模型，例如线性拟合，并不能很好地对这些数据进行分类，分类后存在较大的偏差（Bias），称这个分类模型欠拟合（Underfitting）。右图中，采用复杂的模型进行分类，例如深度神经网络模型，当模型复杂度过高时变容易出现过拟合（Overfitting），使得分类后存在较大的方差（Variance）。中间的图中，采用一个恰当的模型，才能对数据做出一个差不多的分类。 通常采用开发集来诊断模型中是否存在偏差或者时方差： 当训练出一个模型后，发现训练集的错误率较小，而开发集的错误率却较大，这个模型可能出现了过拟合，存在较大方差; 发现训练集和开发集的错误率都都较大时，且两者相当，这个模型就可能出现了欠拟合，存在较大偏差； 发现训练集的错误率较大时，且开发集的错误率远较训练集大时，这个模型就有些糟糕了，方差和偏差都较大。 只有当训练集和开发集的错误率都较小，且两者的相差也较小，这个模型才会是一个好的模型，方差和偏差都较小。 模型存在较大偏差时，可采用增加神经网络的隐含层数、隐含层中的节点数，训练更长时间等方法来预防欠拟合。而存在较大方差时，则可以采用引入更多训练样本、对样本数据正则化（Regularization）等方法来预防过拟合。 预防过拟合L2正则化向Logistic回归的成本函数中加入L2正则化（也称“L2范数”）项：$${J(w,b) = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}({\\hat y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}||w||_2^2}$$其中：$$||w||_2^2 = \\sum_{j=1}^n{w_j^2} = w^T w$$L2正则化是最常用的正则化类型,也存在L1正则化项：$$\\frac{\\lambda}{m}||w||_1 = \\frac{\\lambda}{m}\\sum_{j=1}^n |w_j|$$由于L1正则化最后得到w向量中将存在大量的0，使模型变得稀疏化,所以一般都使用L2正则化。其中的参数$\\lambda$称为正则化参数，这个参数通常通过开发集来设置。向神经网络的成本函数加入正则化项：$${J(w^{[1]},b^{[1]},…,w^{[L]},b^{[L]}) = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}({\\hat y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum\\limits_{l=1}^L ||w^{[l]}}||^2_F$$ 因为w是一个$n^{[l]} \\times n^{[l-1]}$矩阵所以：$${||w^{[l]}||^2_F = \\sum\\limits_{i=1}^{n^{[l-1]}} \\sum\\limits_{j=1}^{n^{[l]}} (w_{ij}^{[l]})^2}$$这被称为弗罗贝尼乌斯范数（Frobenius Norm）,所以神经网络的中的正则化项被称为弗罗贝尼乌斯范数矩阵。 加入正则化项后，反向传播时：$$dw^{[l]} = \\frac{\\partial \\mathcal{L}}{\\partial w^{[l]}} + \\frac{\\lambda}{m} w^{[l]}$$更新参数时：$$ w^{[l]} := w^{[l]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w^{[l]}} - \\alpha \\frac{\\lambda}{m} w^{[l]}$$有了新定义的$dw^{[l]}$，参数$w^{[L]}$在更新时会多减去一项$\\alpha \\frac{\\lambda}{m} w^{[l]}$，所以L2正则化项也被称为权值衰减（Weight Decay）。 参数$\\lambda$用来调整式中两项的相对重要程度，较小$\\lambda$偏向于最后使原本的成本函数最小化，较大的$\\lambda$偏向于最后使权值$w$最小化。当$\\lambda$较大时，权值$w^{[L]}$便会趋近于$0$，相当于消除深度神经网络中隐藏单元的部分作用。另一方面，在权值$w^{[L]}$变小之下，输入样本$X$随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。 随机失活正则化随机失活（DropOut）正则化，就是在一个神经网络中对每层的每个节点预先设置一个被消除的概率，之后在训练随机决定将其中的某些节点给消除掉，得到一个被缩减的神经网络，以此来到达降低方差的目的。DropOut正则化较多地被使用在计算机视觉（Computer Vision）领域。 使用Python编程时可以用反向随机失活（Inverted DropOut）来实现DropOut正则化： 对于一个神经网络第3层 12345keep.prob = 0.8d3 = np.random.randn(a3.shape[0],a3.shape[1]) &lt; keep.proba3 = np.multiply(a3,d3)a3 /= keep.probz4 = np.dot(w4,a3) + b4 其中的d3是一个随机生成，与第3层大小相同的的布尔矩阵，矩阵中的值为0或1。而keep.prob ≤ 1，它可以随着各层节点数的变化而变化，决定着失去的节点的个数。 例如，将keep.prob设置为0.8时，矩阵d3中便会有20%的值会是0。而将矩阵a3和d3相乘后，便意味着这一层中有20%节点会被消除。需要再除以keep_prob的原因，是因为后一步求z4中将用到a3，而a3有20%的值被清零了，为了不影响下一层z4的最后的预期输出值，就需要这个步骤来修正损失的值，这一步就称为反向随机失活技术，它保证了a3的预期值不会因为节点的消除而收到影响，也保证了进行测试时采用的是DropOut前的神经网络。 与之前的L2正则化一样，利用DropOut，可以简化神经网络的部分结构，从而达到预防过拟合的作用。另外，当输入众多节点时，每个节点都存在被删除的可能，这样可以减少神经网络对某一个节点的依赖性，也就是对某一特征的依赖，扩散输入节点的权重，收缩权重的平方范数。 数据扩增法 数据扩增（Data Augmentation）是在无法获取额外的训练样本下，对已有的数据做一些简单的变换。例如对一张图片进行翻转、放大扭曲，以此引入更多的训练样本。 早停止法 早停止（Early Stopping）是分别将训练集和开发集进行梯度下降时成本变化曲线画在同一个坐标轴内，在箭头所指两者开始发生较大偏差时就及时进行纠正，停止训练。在中间箭头处，参数w将是一个不大不小的值，理想状态下就能避免过拟合的发生。然而这种方法一方面没有很好得降低成本函数，又想以此来避免过拟合，一个方法解决两个问题，哪个都不能很好解决。 标准化数据集对训练及测试集进行标准化的过程为：$$ \\bar{x} = \\frac{1}{m} \\sum_{i=1}^m x^{(i)} $$ $$ x^{(i)} := x^{(i)} - \\bar{x} $$ $$ \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m {x^{(i)}}^2$$ $$x^{(i)}:= \\frac{x^{(i)}}{\\sigma^2} $$原始数据为： 经过前两步，x减去它们的平均值后： 经过后两步，x除以它们的方差后： 数据集未进行标准化时，成本函数的图像及梯度下降过程将是： 而标准化后，将变为： 初始化权重在之前的建立神经网络的过程中，提到权重w不能为0，而将它初始化为一个随机的值。然而在一个深层神经网络中，当w的值被初始化过大时，进入深层时呈指数型增长，造成梯度爆炸；过小时又会呈指数级衰减，造成梯度消失。 Python中将w进行随机初始化时，使用numpy库中的np.random.randn()方法，randn是从均值为0的单位标准正态分布（也称“高斯分布”）进行取样。随着对神经网络中的某一层输入的数据量n的增长，输出数据的分布中，方差也在增大。结果证明，可以除以输入数据量n的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了，不会过大导致到指数级爆炸或过小而指数级衰减。也就是将权重初始化为： 1w = np.random.randn(layers_dims[l],layers_dims[l-1]) \\* np.sqrt(1.0/layers_dims[l-1]) 这样保证了网络中所有神经元起始时有近似同样的输出分布。当激活函数为ReLU函数时，权重最好初始化为： 1w = np.random.randn(layers_dims[l],layers_dims[l-1]) \\* np.sqrt(2.0/layers_dims[l-1]) 以上结论的证明过程见参考资料。 梯度检验梯度检验的实现原理，是根据导数的定义，对成本函数求导，有：$$ J’(\\theta) = \\frac{\\partial J(\\theta)}{\\partial \\theta}= \\lim_{\\epsilon\\rightarrow 0}\\frac{J(\\theta+\\epsilon)-J(\\theta-\\epsilon)}{2\\epsilon}$$ 则梯度检验公式：$$J’(\\theta) = \\frac{J(\\theta+\\epsilon)-J(\\theta-\\epsilon)}{2\\epsilon}$$ 其中当$\\epsilon$越小时，结果越接近真实的导数也就是梯度值。可以使用这种方法，来判断反向传播进行梯度下降时，是否出现了错误。 梯度检验的过程，是对成本函数的每个参数$\\theta_{[i]}$加入一个很小的$\\epsilon$，求得一个梯度逼近值$d\\theta_{approx[i]}$：$$d\\theta_{approx[i]} = \\frac{J(\\theta_{[1]},\\theta_{[2]},…,\\theta_{[i]}+\\epsilon)-J(\\theta_{[1]},\\theta_{[2]},…,\\theta_{[i]}-\\epsilon)}{2\\epsilon}$$ 以解析方式求得$J’(\\theta)$在$\\theta$时的梯度值$d\\theta$,进而再求得它们之间的欧几里得距离：$$\\frac{||d\\theta_{approx[i]}-d\\theta||_2}{||d \\theta_{approx[i]}||_2+||dθ||_2}$$ 其中$||x||_2$表示向量x的2范数（各种范数的定义见参考资料）：$$||x||_2 = \\sum\\limits_{i=1}^N |x_i|^2$$ 当计算的距离结果与$\\epsilon$的值相近时，即可认为这个梯度值计算正确，否则就需要返回去检查代码中是否存在bug。 需要注意的是，不要在训练模型时进行梯度检验，当成本函数中加入了正则项时，也需要带上正则项进行检验，且不要在使用随机失活后使用梯度检验。 Python实现三种参数初始化1.zeros\\random\\Xavier初始化 12345678910111213141516171819202122232425262728293031323334#参数w,b初始化为0def initialize_parameters_zeros(layers_dims): parameters = {} L = len(layers_dims) for l in range(1, L): parameters['W' + str(l)] = np.zeros((layers_dims[l],layers_dims[l-1])) parameters['b' + str(l)] = np.zeros((layers_dims[l],1)) return parameters#参数w初始化为randomdef initialize_parameters_random(layers_dims): np.random.seed(3) parameters = {} L = len(layers_dims) for l in range(1, L): parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1]) * 10 parameters['b' + str(l)] = np.zeros((layers_dims[l],1)) return parameters#参数w进行Xavier初始化def initialize_parameters_he(layers_dims): np.random.seed(3) parameters = {} L = len(layers_dims) - 1 for l in range(1, L + 1): parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1]) * np.sqrt(1.0/(layers_dims[l-1])) parameters['b' + str(l)] = np.zeros((layers_dims[l],1)) return parameters 2.效果比较 12345678910layers = [train_X.shape[0], 10, 5, 1] #设置神经网络层数及节点数#参数初始化为0parameters_init_zeros = model(train_X, train_Y, layers_dims = layers, initialization = &quot;zeros&quot;)#初始化randomparameters_init_random = model(train_X, train_Y, layers_dims = layers, initialization = &quot;random&quot;)#Xavier初始化parameters_init_he = model(train_X, train_Y, layers_dims = layers, initialization = &quot;he&quot;) 结果为： 无\\L2\\DropOut正则化12345678910layers = [train_x.shape[0], 20, 3, 1] #设置神经网络层数及节点数#不进行正则化parameters_no_reg = model(train_x, train_y, layers_dims = layers, learning_rate = 0.3, num_iterations = 30000)#采用L2正则化parameters_L2_reg = model(train_x, train_y, layers_dims = layers, learning_rate = 0.3, num_iterations = 30000, lambd = 0.7)#采用DropOut正则化parameters_dropout_reg = model(train_x, train_y, layers_dims = layers, learning_rate = 0.3, num_iterations = 30000, keep_prob = 0.86) 结果为： 参考资料 吴恩达-改善深层神经网络-网易云课堂 Andrew Ng-Improving Deep Neural Networks-Coursera 神经网络权重初始化-知乎专栏 范数的定义-知乎回答 课程代码与资料-GitHub 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2017.10.02 完成初稿 2018.02.13 调整部分内容","link":"/2017/09/28/ML/deep_learning_3/"},{"title":"【Coursera】深度学习(4)：优化神经网络(2)","text":"想提高一个深层神经网络的训练效率，需从各个方面入手，优化整个运算过程，同时预防其中可能发生的各种问题。 本文涉及优化深层神经网络中的几种梯度下降法，梯度下降中的Momentum、RMSProp、Adam优化算法及学习率衰减，批标准化等内容。 梯度下降法批梯度下降法（BGD）批梯度下降法（Batch Gradient Descent，BGD）是最常用的梯度下降形式，前面的Logistic回归及深层神经网络的构建中所用到的梯度下降都是这种形式。其在更新参数时使用所有的样本来进行更新，具体过程为：$${X = [x^{(1)},x^{(2)},…,x^{(m)}]}$$ $$z^{[1]} = w^{[1]}X + b^{[1]}$$ $$a^{[1]} = g^{[1]}(z^{[1]})$$ $$… \\ …$$ $$z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}$$ $$a^{[l]} = g^{[l]}(z^{[l]})$$ $$ {J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}({\\hat y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum\\limits_{l=1}^L ||w^{[l]}}||^2_F$$ $$ {\\theta_j:= \\theta_j -\\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_j}} $$示例图： 优点：最小化所有训练样本的损失函数，得到全局最优解；易于并行实现。缺点：当样本数目很多时，训练过程会很慢。 随机梯度下降法（SGD）随机梯度下降法（Stochastic Gradient Descent，SGD）与批梯度下降原理类似，区别在于每次通过一个样本来迭代更新。其具体过程为：$${X = [x^{(1)},x^{(2)},…,x^{(m)}]}$$ $$ for\\ \\ \\ i=1,2,…,m\\ \\{ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$ $$z^{[1]} = w^{[1]}x^{(i)} + b^{[1]}$$ $$a^{[1]} = g^{[1]}(z^{[1]})$$ $$… \\ …$$ $$z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}$$ $$a^{[l]} = g^{[l]}(z^{[l]})$$ $$ {J(\\theta) = \\mathcal{L}({\\hat y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2} \\sum\\limits_{l=1}^L ||w^{[l]}}||^2_F$$ $$ \\theta_j:= \\theta_j -\\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_j} \\} $$示例图： 优点：训练速度快。缺点：最小化每条样本的损失函数，最终的结果往往是在全局最优解附近，不是全局最优；不易于并行实现。 小批量梯度下降法（MBDG）小批量梯度下降法（Mini-Batch Gradient Descent，MBGD）是批量梯度下降法和随机梯度下降法的折衷,对用m个训练样本，，每次采用t（1 &lt; t &lt; m）个样本进行迭代更新。具体过程为：$${X = [x^{\\{1\\}},x^{\\{2\\}},…,x^{\\{k = \\frac{m}{t}\\}}]}$$ 其中： $$x^{\\{1\\}} = x^{(1)},x^{(2)},…,x^{(t)}$$ $$x^{\\{2\\}} = x^{(t+1)},x^{(t+2)},…,x^{(2t)}$$ $$… \\ …$$之后：$$ for\\ \\ \\ i=1,2,…,k\\ \\{ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$ $$z^{[1]} = w^{[1]}x^{\\{i\\}} + b^{[1]}$$ $$a^{[1]} = g^{[1]}(z^{[1]})$$ $$… \\ …$$ $$z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}$$ $$a^{[l]} = g^{[l]}(z^{[l]})$$ $$ {J(\\theta) = \\frac{1}{k} \\sum_{i=1}^k \\mathcal{L}({\\hat y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2k} \\sum\\limits_{l=1}^L ||w^{[l]}}||^2_F$$ $$ \\theta_j:= \\theta_j -\\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_j} \\} $$示例图： 样本数t的值根据实际的样本数量来调整，为了和计算机的信息存储方式相适应，可将t的值设置为2的幂次。将所有的训练样本完整过一遍称为一个epoch。 梯度下降优化指数加权平均指数加权平均（Exponentially Weight Average）是一种常用的序列数据处理方式，其计算公式为：$$S_t = \\begin{cases} Y_1, &amp; t=1 \\\\ \\beta S_{t-1} + (1-\\beta)Y_{t}, &amp; t&gt;1 \\end{cases}$$其中$Y_t$为t下的实际值，$S_t$为t下加权平均后的值，$\\beta$为权重值。给定一个时间序列，例如伦敦一年每天的气温值： 其中蓝色的点代表了真实的数据值。对于一个即时的温度值，取权重值$\\beta$为0.9，则有：$$ v_0 = 0 $$ $$v_1 = 0.9v_0 + 0.1\\theta_1$$ $$… \\ … $$ $$v_{100} = 0.1\\theta_{100}+0.1 \\times 0.9\\theta_{99} +0.1 \\times 0.9^2\\theta_{98} \\ …$$ $$ v_t = 0.9v_{t-1} + 0.1\\theta_t $$根据：$$\\lim_{\\epsilon \\to 0} (1-\\epsilon)^{\\frac{1}{\\epsilon}} = \\frac{1}{e} \\approx 0.368$$ $\\beta=1 - \\epsilon = 0.9$时相当于把过去 $\\frac{1}{\\epsilon} = 10$天的气温值指数加权平均后，作为当日的气温，且只取10天前的气温值的$0.368$，也就是$\\frac{1}{3}$多一些。 由此求得的值即得到图中的红色曲线，它反应了温度变化的大致趋势。 当取权重值$\\beta=0.98$时，可以得到图中更为平滑的绿色曲线。而当取权重值$\\beta=0.5$时，得到图中噪点更多的黄色曲线。$\\beta$越大相当于求取平均利用的天数就越多，曲线自然就会越平滑而且越滞后。 当进行指数加权平均计算时，第一个值$v_o$被初始化为$0$，这样将在前期的运算用产生一定的偏差。为了矫正偏差，需要在每一次迭代后用以下式子进行偏差修正：$$v_t := \\frac{v_t}{1-\\beta^t}$$ Momentum梯度下降动量梯度下降（Gradient Descent with Momentum）是计算梯度的指数加权平均数，并利用该值来更新参数值。具体过程为：$$v_{dw} = \\beta v_{dw} + (1-\\beta)dw$$ $$v_{db} = \\beta v_{db} + (1-\\beta)db$$ $$w := w-\\alpha v_{dw}$$ $$ b := b-\\alpha v_{db}$$其中的动量衰减参数$\\beta$一般取0.9。 进行一般的梯度下降将会得到图中的蓝色曲线，而使用Momentum梯度下降时，通过累加减少了抵达最小值路径上的摆动，加快了收敛，得到图中红色的曲线。 当前后梯度方向一致时，Momentum梯度下降能够加速学习；前后梯度方向不一致时,Momentum梯度下降能够抑制震荡。 RMSProp算法**RMSProp(Root Mean Square Prop，均方根支)**算法在对梯度进行指数加权平均的基础上，引入平方和平方根。具体过程为：$$s_{dw} = \\beta s_{dw} + (1-\\beta)dw^2$$ $$s_{db} = \\beta s_{db} + (1-\\beta)db^2$$ $$w := w-\\alpha \\frac{dw}{\\sqrt{s_{dw}+\\epsilon}}$$ $$b := b-\\alpha \\frac{db}{\\sqrt{s_{db}+\\epsilon}}$$其中的$\\epsilon=10^{-8}$，用以提高数值稳定度，防止分母太小。 当$dw$或$db$较大时，$dw^{2}$、$db^{2}$会较大，造成$s_{dw}$、 $s_{db}$也会较大，最终使$\\frac{dw}{\\sqrt{s_{dw}}}$、 $\\frac{db}{\\sqrt{s_{db}}}$较小，减小了抵达最小值路径上的摆动。 Adam优化算法**Adam(Adaptive Moment Estimation，自适应矩估计)**优化算法适用于很多不同的深度学习网络结构，它本质上是将Momentum梯度下降和RMSProp算法结合起来。具体过程为：$$v_{dw} = \\beta_1 v_{dw} + (1-\\beta_1)dw, \\ v_{db} = \\beta_1 v_{db} + (1-\\beta_1)db$$ $$s_{dw} = \\beta_2 s_{dw} + (1-\\beta_2)dw^2,\\ s_{db} = \\beta_2 s_{db} + (1-\\beta_2)db^2$$ $$v^{corrected}_{dw} = \\frac{v_{dw}}{(1-\\beta_1^t)},\\ v^{corrected}_{db} = \\frac{v_{db}}{(1-\\beta_1^t)}$$ $$s^{corrected}_{dw} = \\frac{s_{dw}}{(1-\\beta_2^t)},\\ s^{corrected}_{db} = \\frac{s_{db}}{(1-\\beta_2^t)}$$ $$w := w-\\alpha \\frac{v^{corrected}_{dw}}{\\sqrt{s^{corrected}_{dw}}+\\epsilon}$$ $$b := b-\\alpha \\frac{v^{corrected}_{db}}{\\sqrt{s^{corrected}_{db}}+\\epsilon}$$其中的学习率$\\alpha$需要进行调参，超参数$\\beta_1$被称为第一阶矩，一般取0.9，$\\beta_2$被称为第二阶矩，一般取0.999，$\\epsilon$一般取$10^{-8}$。 学习率衰减随着时间推移，慢慢减少学习率$\\alpha$的大小。在初期$\\alpha$较大时，迈出的步长较大，能以较快的速度进行梯度下降，而后期逐步减小$\\alpha$的值，减小步长，有助于算法的收敛，更容易接近最优解。常用到的几种学习率衰减方法有：$$\\alpha = \\frac{1}{1+\\text{decay_rate }* \\text{epoch_num}} * \\alpha_0$$ $$\\alpha = 0.95^{\\text{epoch_num}} * \\alpha_0$$ $$\\alpha = \\frac{k}{\\sqrt{\\text{epoch_num}} }* \\alpha_0$$其中的decay_rate为衰减率，epoch_num为将所有的训练样本完整过一遍的次数。 批标准化批标准化（Batch Normalization，BN）和之前的数据集标准化类似，是将分散的数据进行统一的一种做法。具有统一规格的数据，能让机器更容易学习到数据中的规律。 对于含有$m$个节点的某一层神经网络，对$z$进行操作的步骤为：$$\\mu = \\frac{1}{m} \\sum_{i=1}^m z^{(i)}$$ $$\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (z^{(i)}-\\mu)^2$$ $$z_{norm}^{(i)} = \\frac{z^{(i)}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}$$ $$\\tilde{z}^{(i)} = \\gamma z_{norm}^{(i)}+\\beta$$其中的$\\gamma$、$\\beta$并不是超参数，而是两个需要学习的参数，神经网络自己去学着使用和修改这两个扩展参数。这样神经网络就能自己慢慢琢磨出前面的标准化操作到底有没有起到优化的作用。如果没有起到作用，就使用 $\\gamma$和$\\beta$来抵消一些之前进行过的标准化的操作。例如当$\\gamma = \\sqrt{\\sigma^2+\\epsilon}, \\beta = \\mu$，就抵消掉了之前的正则化操作。 将图中的神经网络中的$z^{[1]}$、$z^{[2]}$进行批标准化后，$z^{[1]}$、$z^{[2]}$将变成$\\tilde{z}^{[1]}$、$\\tilde{z}^{[2]}$。 当前的获得的经验无法适应新样本、新环境时，便会发生“Covariate Shift”现象。对于一个神经网络，前面权重值的不断变化就会带来后面权重值的不断变化，批标准化减缓了隐藏层权重分布变化的程度。采用批标准化之后，尽管每一层的z还是在不断变化，但是它们的均值和方差将基本保持不变，这就使得后面的数据及数据分布更加稳定，减少了前面层与后面层的耦合，使得每一层不过多依赖前面的网络层，最终加快整个神经网络的训练。 批标准化还有附带的有正则化的效果：当使用小批量梯度下降时，对每个小批量进行批标准化时，会给这个小批量中最后求得的$z$带来一些干扰，产生类似与DropOut的正则化效果，但效果不是很显著。当这个小批量的数量越大时，正则化的效果越弱。 需要注意的是，批标准化并不是一种正则化的手段，正则化效果只是其顺带的小副作用。另外，在训练时用到了批标准化，则在测试时也必须用到批标准化。 训练时，输入的是小批量的训练样本，而测试时，测试样本是一个一个输入的。这里就又要用到指数加权平均，在训练过程中，求得每个小批量的均值和方差的数加权平均值，之后将最终的结果保存并应用到测试过程中。 Softmax回归Softmax回归模型是Logistic回归模型在多分类问题上的推广，在多分类问题中，输出y的值不再是一个数，而是一个多维列向量，有多少种分类是就有多少维数。激活函数使用的是softmax函数：$$\\sigma(z)_{j}=\\frac{exp(z_{j})}{\\sum_{i=1}^m exp(z_{i})}$$损失函数也变为：$$\\mathcal{L}(a^l, y) = - \\sum^m_{i=1}y_i \\log a^l$$ 参考资料 吴恩达-改善深层神经网络-网易云课堂 Andrew Ng-Improving Deep Neural Networks-Coursera 梯度下降法的三种形式-博客园 什么是批标准化-知乎专栏 Softmax回归-Ufldl 课程代码与资料-GitHub 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2017.10.07 完成初稿 2018.02.13 调整部分内容","link":"/2017/10/06/ML/deep_learning_4/"},{"title":"【Coursera】深度学习(5)：TensorFlow","text":"每次都从零开始全部靠自己去建立一个深层神经网络模型并不现实，借助现在众多流行的深度学习框架，能够高效地实现这些模型。TensorFlow便是其中之一。 TensorFlowTensorFlow是Google基于DistBelief进行研发的第二代人工智能学习系统，是一个使用数据流图进行数值计算的开源软件库。其命名来源于本身的运行原理，Tensor(张量)意味着N维数组，Flow（流）意味着基于数据流图的计算，TensorFlow便意为张量从流图的一端流动到另一端计算过程。TensorFlow 中包含了一款强大的线性代数编译器XLA，这可以帮助TensorFlow代码在嵌入式处理器、CPU（Central Processing Unit）、GPU（Graphics Processing Unit）、TPU（Tensor Processing Unit）和其他硬件平台上尽可能快速地运行。 版本选择TensorFlow分为CPU版和GPU版。GPU版需要有NVIDIA显卡的支持，TensorFlow程序通常在GPU上的运行速度明显高于CPU，查看设备是否配备了NVIDIA显卡方法另寻。 如果设备配备了NVIDIA显卡，还需根据显卡的具体型号，到NVIDIA官方文档上找到你的GPU，查看其该型号的N卡是否支持CUDA（Compute Unified Device Architecture），以及CUDA算力（Computer Capability）。TensorFlow要求GPU的CUDA计算能力值达到3.0以上，否则比直接CPU上运算的效果相差不大。一些年份较早或比较低端的显卡达不到这个要求，这种情况下即使安装了GPU版本也还是会调用CPU运算，不会调用GPU。 安装TensorFlow的Python版本一般直接使用pip命令进行安装，CPU版本的pip安装命令为： 1pip install --upgrade tensorflow GPU版本为： 1pip install --upgrade tensorflow-gpu 安装GPU版本后想要用N卡正常运行，需要先安装CUDA® Toolkit和cuDNN，根据各自的平台进行下载安装。而且需要注意，下载安装的版本务必按照安装好的TensorFlow支持的版本来。 测试安装完成后，打开在命令行或终端中激活Python，输入以下代码： 12345# Pythonimport tensorflow as tfhello = tf.constant('Hello, TensorFlow!')sess = tf.Session()print(sess.run(hello)) 安装无误的话，最后将打印出熟悉的“Hello, TensorFlow!”。此外，如果安装的是GPU版，输入第三行代码后将显示你的GPU配置信息。 实现MNIST手写数字识别MNIST数据集MNIST（Mixed National Institute of Standards and Technology database）是一个入门级的计算机视觉数据集，其中包含各种手写数字图片： 上面的四张图的标签（labels）分别为5,0,4,1。 MNIST数据集被分为三部分：55000个训练样本（mnist.train),10000个测试样本（mnist.test),5000个验证集（mnist.validation)。每个样本由两部分组成，一个大小为28*28的手写数字图片像素值矩阵$x$以及它的标签$y$，标签$y$是用one-hot向量表示的，一张数字图片中的真实数字值n,将表示成一个只在第n维度的数字为1的10位维向量。例如，标签3用one—hot向量表示为$[0,0,0,1,0,0,0,0,0,0]$。one-hot编码多用在多分类问题上。 则整个训练样本中$X$（mnist.train.images）的大小为55000*784,$Y$（mnist.train.labels)的大小为55000*10。其中的数据都已经进行过标准化。 Softmax回归Softmax回归模型是Logistic回归模型在多分类问题上的推广。为了收集证据以便将给定的图片划定到某个固定分类，首先对图片的像素值加权值w并求和，如果有很明显的证据表明一张图片中某些像素点会导致这张图片不属于这一类，则相应的权值为负数，反之权值为正。如下图所示，其中红色部分代表负数权值，蓝色部分代表正数权值。 训练过程中，为了排除输入中引入的干扰数据，还需要设定额外的偏执量b。其实就是进行简单的线性拟合：$${ \\text{evidence} = w^TX + b}$$其中参数矩阵$w$的大小为784*10,$b$的大小为10*1。和Logistic回归一样，其中的$w$、$b$还是直接初始化为0。 Softmax回归中，激活函数使用的是softmax函数：$$\\sigma(z)_{j}=\\frac{e^{z_{j}}}{\\sum_{i=1}^m e^{z_{i}}}$$损失函数也变为：$$\\mathcal{L}(\\hat y, y) = - \\sum^m_{i=1}y_i \\log \\hat y$$随后使用小批量梯度下降法来求得参数的最优解。 tensorflow实现使用tensorflow来实现这个模型： 12345678910111213141516171819202122232425262728293031#导入要用到的库import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot = True) #导入数据集#初始化参数W = tf.Variable(tf.zeros([mnist.train.images.shape[1],10])) #W初始化为0b = tf.Variable(tf.zeros([10])) #b初始化为0costs = []#建立模型x = tf.placeholder(tf.float32, [None, mnist.train.images.shape[1]])y = tf.placeholder(tf.float32, [None, 10]) #建立训练集占位符y_hat = tf.nn.softmax(tf.matmul(x,W) + b) #softmax激活cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_hat), reduction_indices=[1])) #成本函数train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cost) #梯度下降，最小化成本sess = tf.InteractiveSession() #创建sessiontf.global_variables_initializer().run() #初始化变量（声明了变量，就必须初始化才能用）#迭代运算for epoch in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) #每次使用100个小批量数据 sess.run([train_step, cost], feed_dict = {x: batch_xs, y: batch_ys}) #进行训练#计算准确率correct_prediction = tf.equal(tf.argmax(y_hat,1), tf.argmax(y,1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})) 最后用训练样本测试的准确率为92%左右。 实现手势数字识别手势数字数据集中，训练样本有1080张图片，测试样本有120张图片，其中有代表着0-5的六种图片。每个原始样本由两部分组成，一个大小为64*64的手势数字图片像素值矩阵$x$以及它的标签$y$，原始样本的标签$y$是直接用手势图片表示的数字真实值表示的，还需要将它们转换为one-hot向量进行表示。 1.导入数据12345678910111213141516#导入数据def load_dataset(): train_dataset = h5py.File('train_signs.h5', &quot;r&quot;)#训练集 train_set_x_orig = np.array(train_dataset[&quot;train_set_x&quot;][:]) train_set_y_orig = np.array(train_dataset[&quot;train_set_y&quot;][:]) test_dataset = h5py.File('test_signs.h5', &quot;r&quot;)#测试集 test_set_x_orig = np.array(test_dataset[&quot;test_set_x&quot;][:]) test_set_y_orig = np.array(test_dataset[&quot;test_set_y&quot;][:]) classes = np.array(test_dataset[&quot;list_classes&quot;][:]) train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0])) test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0])) return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes 2.one-hot编码编码方式如图： 代码实现： 1234#ong-hot编码def convert_to_one_hot(Y, C): Y = np.eye(C)[Y.reshape(-1)].T#np.eye():生成单位矩阵 return Y 3.前向传播12345678910111213141516#前向传播def forward_propagation(X, parameters): W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] W3 = parameters['W3'] b3 = parameters['b3'] Z1 = tf.add(tf.matmul(W1,X),b1) A1 = tf.nn.relu(Z1) Z2 = tf.add(tf.matmul(W2,A1),b2) A2 = tf.nn.relu(Z2) Z3 = tf.add(tf.matmul(W3,A2),b3) return Z3 4.成本计算12345678#成本计算def compute_cost(Z3, Y): logits = tf.transpose(Z3) labels = tf.transpose(Y) cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)) return cost 5.整个模型1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001, num_epochs = 1500, minibatch_size = 32, print_cost = True): ops.reset_default_graph() #使重新运行时不覆盖tf的变量 tf.set_random_seed(1) seed = 3 (n_x, m) = X_train.shape n_y = Y_train.shape[0] costs = [] X, Y = create_placeholders(n_x, n_y) #占位符 parameters = initialize_parameters() #初始化参数 Z3 = forward_propagation(X, parameters) #前向传播 cost = compute_cost(Z3, Y) #损失计算 optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost) #使用Adam梯度下降 init = tf.global_variables_initializer() #变量初始化 with tf.Session() as sess: sess.run(init) #运行变量初始化 for epoch in range(num_epochs): epoch_cost = 0. #每epoch一次后的成本值 num_minibatches = int(m / minibatch_size) #小批量数目 seed = seed + 1 minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed) #随机小批量 for minibatch in minibatches: (minibatch_X, minibatch_Y) = minibatch _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y}) epoch_cost += minibatch_cost / num_minibatches if print_cost == True and epoch % 100 == 0: print (&quot;%i epoch 后的成本值 : %f&quot; % (epoch, epoch_cost)) if print_cost == True and epoch % 5 == 0: costs.append(epoch_cost) plt.plot(np.squeeze(costs)) plt.ylabel('cost') plt.xlabel('iterations (per tens)') plt.title(&quot;Learning rate =&quot; + str(learning_rate)) plt.show() parameters = sess.run(parameters) print (&quot;参数训练完毕&quot;) correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;)) print (&quot;训练集准确度:&quot;, accuracy.eval({X: X_train, Y: Y_train})) print (&quot;测试集准确度:&quot;, accuracy.eval({X: X_test, Y: Y_test})) return parameters 6.得出结果12345678910#数据集处理X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T #转置X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).TX_train = X_train_flatten/255. #标准化X_test = X_test_flatten/255.Y_train = convert_to_one_hot(Y_train_orig, 6) #变为one-hot形式Y_test = convert_to_one_hot(Y_test_orig, 6)parameters = model(X_train, Y_train, X_test, Y_test) 结果为： 参考资料 吴恩达-改善深层神经网络-网易云课堂 Andrew Ng-Improving Deep Neural Networks-Coursera tensorflow官网 tensorflow中文社区 tensorflow环境搭建教程（Windows） tensorflow环境搭建教程（Ubuntu） 课程代码与资料-GitHub 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2017.10.15 完成初稿 2018.02.13 调整部分内容","link":"/2017/10/14/ML/deep_learning_5/"},{"title":"机器学习笔记（停更）","text":"笔记地址：http://hugsy.top/Machine_Learning/#/ 原先写的《深度学习》系列笔记太乱，所以把原先的笔记重新整理到了上述地址，以后机器学习理论相关的知识都将整合到那里，而博客中的相关内容不再更新。 关于本项目是本人学习过网络上一些机器学习相关的公开课后，所记录的一些笔记。笔记内容大致上以吴恩达老师2017年在Coursera平台发布的《深度学习》系列课程为主线，辅以其他相关课程进行扩充而来。 《深度学习》系列课程的笔记原先被记录在本人的博客当中，往后翻阅发现很多地方记录地不够好，且查阅起来不够不方便，于是决定在此将原来的笔记重新整理在此，且往后的学习到新形容也一律整合到这里，方便查阅。 本笔记停更很久了，暂时没有重新更新的计划（2021.4.11）。整理水平有限，如有不妥的地方欢迎指出。 参考目前本笔记内容涉及到的课程主要有： 吴恩达-深度学习系列：[Coursera] [网易云课堂] 吴恩达-机器学习：[Coursera] [网易云课程] 李宏毅-机器学习：[2017（bilibili）] [2019（bilibili）] [课程主页] 斯坦福-CS231n：[2017（网易云课堂）] [课程主页] 动手学深度学习：[主页] 许可本文档由docsify动态生成，引用的图片等内容均来源于网络及上述公开课程上提供的各种资料，如有侵权，请及时联系删除。 文章全部采用CC BY-NC-SA 4.0许可协议，转载请注明出处。","link":"/2019/04/13/ML/machine_learning/"},{"title":"【Coursera】深度学习(9)：循环神经网络","text":"随深度学习技术的发展，使用循环神经网络（Recurrent Neural Network，RNN）建立的各种序列模型，使语音识别、机器翻译及自然语言理解等应用成为可能。 表示与类型自然语言、音频等数据都是前后相互关联的数据，比如理解一句话要通过一整句而不是其中的几个词，分析一段音频要通过连续的片段而不是其中的几帧。前面所学的DNN以及CNN处理的都是前后毫无关联的一个个单独数据，对于这些序列数据则需要采用RNN进行处理。 用循环神经网络处理时间序列时，首先要对时间序列进行标记化。对一个序列数据$x$，用符号$x^{\\langle t\\rangle}$来表示这个序列数据中的第$t$个元素。这个元素的类型因数据类型而异，对一段音频，它可能其中的几帧，对一句话，它可能是一个或几个单词，如下图所示。 第$i$个序列数据的第$t$个元素用符号$x^{(i)\\langle t\\rangle}$表示，其标签用符号$y^{(i)\\langle t\\rangle}$表示。 序列中的每个元素有相应的标签，一般需要先建立一个包含序列中所有类型的元素的字典（Dictionary）。例如对上图中的句子，建立一个含有10000个单词的列向量词典，单词顺序以A~Z排列，然后根据单词在列向量中的位置，用one—hot向量来表示该单词的标签，部分表示如下图： 标记化完成后，将训练数据输入网络中。一种循环神经网络的结构如下图： 左边的网络可简单表示为右图的结构，其中元素$x^{\\langle t\\rangle}$输入对应时间步（TIme Step）的隐藏层的同时，该隐藏层也会接收上一时间步的隐藏层激活$a^{\\langle t-1\\rangle}$，其中$a^{\\langle 0\\rangle}$一般直接初始化为零向量。一个时间步输出一个对应的预测结果${\\hat y}^{\\langle t\\rangle}$，输入、激活、输出有对应的参数$W_{ax}$、$W_{aa}$、$W_{y}$。 以上结构的前向传播过程，有：$$a^{\\langle 0\\rangle} = \\vec 0$$ $$a^{\\langle t\\rangle} = g_1(W_{aa} a^{\\langle t-1\\rangle} + W_{ax} x^{\\langle t\\rangle} + b_a)$$ $${\\hat y}^{\\langle t\\rangle} = g_2(W_{y} a^{\\langle t\\rangle} + b_y)$$ 其中$b_a$、$b_y$是两个偏差参数，激活函数$g_1$通常选择tanh，有时也用ReLU，$g_2$的选择取决于需要的输出类型，可选sigmoid或Softmax。 具体计算中以上的式子还可以进一步简化，以方便运算。将$W_{ax}$和$W_{aa}$堆叠成一个矩阵$W_a$，$a^{\\langle t-1\\rangle}$和$x^{\\langle t\\rangle}$也堆叠成一个矩阵，有：$$ W_a = [W_{ax}, W_{aa}] $$ $$a^{\\langle t\\rangle} = g_1(W_{a}[a^{\\langle t-1\\rangle},x^{\\langle t\\rangle}] + b_a)$$ 反向传播的过程类似于深度神经网络，如下图所示： 这种结构的一个缺陷是，某一时刻的预测结果仅使用了该时刻之前输入的序列信息。根据所需的输入及输出数量，循环神经网络可分为“一对一”、“多对一”、“多对多”等结构： 这些网络结构可在不同的领域中得到应用。 RNN应用：语言模型语言模型（Language Model）是根据语言客观事实而进行的语言抽象数学建模。例如对一个语音识别系统，输入的一段语音可能表示下面两句话： 其中的“pair”和“pear”读音相近，但是在日常表达及语法上显然这段语音是第二句的可能性要大，要使这个语音识别系统能够准确判断出第二句话为正确结果，就需要语言模型的支持。这个语言模型能够分别计算出语音表示以上两句话的概率，以此为依据做出判断。 建立语言模型所采用的训练集是一个大型的语料库（Corpus）。建立过程中，如之前所述，需要先建立一个字典，之后将语料库中每个词表示为对应的one-hot向量。此外需要额外定义一个标记EOS（End Of Sentence）表示一个句子的结尾，也可以将其中的标点符号加入字典后也用one=hot向量表示。对于语料库中部分（英语）人名、地名等特殊的不包含在字典中的词汇，可在词典中加入再用一个UNK（Unique Token）标记来表示。 将标志化后的训练集输入网络中的训练过程，如下例所示： 第一个时间步中输入的$a^{\\langle 0\\rangle}$和$x^{\\langle 1\\rangle}$都是零向量，${\\hat y}^{\\langle 1\\rangle}$是通过softmax预测出的字典中每一个词作为第一个词出现的概率；第二个时间步中输入的$x^{\\langle 2\\rangle}$是下面的训练数据中第一个单词“cats”的标签$y^{\\langle 1\\rangle}$和上一层的激活$a^{\\langle 1\\rangle}$,输出的$y^{\\langle 2\\rangle}$表示的是单词“cats”后面出现字典中的其他词，特别是“average”的条件概率。后面的时间步与第二步类似，到最后就可以得到整个句子出现的概率。 这样，损失函数将表示为：$$ \\mathcal{L}({\\hat y}^{\\langle t\\rangle},y^{\\langle t\\rangle})=-\\sum_t y^{\\langle t\\rangle}_i log\\ {\\hat y}^{\\langle t\\rangle}$$ 成本函数表示为：$$ \\mathcal{J} = \\sum_t \\mathcal{L}^{\\langle t\\rangle}({\\hat y}^{\\langle t\\rangle},y^{\\langle t\\rangle})$$ 训练好一个这个语言模型后，可通过采样（Sample）新的序列，来了解这个模型中都学习到了一些什么。从模型中采样出新序列的过程如下： 第一个时间步中输入的$a^{\\langle 0\\rangle}$和$x^{\\langle 1\\rangle}$还是零向量，依据softmax预测出的字典中每一个词作为第一个词出现的概率，选取一个词${\\hat y}^{\\langle 1\\rangle}$作为第二个时间步的输入。后面与此类似，模型将自动生成一些句子，从这些句子中可发现模型通过语料库学习到的知识。 以上是基于词汇构建的语言模型，也就是所用的字典中包含的是一个个单词。实际应用中，还可以构建基于字符的语言模型，不过这种方法的结果中将得到过多过长的序列，计算成本也较大，在当今的NLP领域也用得较少。 GRU与LSTM如下图中的句子时，后面的动词用“was”还是“were”取决于前面的名词“cat”是单数还是复数。 一般的循环神经网络不擅于捕捉这种序列中存在的长期依赖关系，其中的原因是，一般的循环神经网络也会出现类似于深度神经网络中的梯度消失问题，而使后面输入的序列难以受到早先输入序列的影响。梯度爆炸的问题也会出现，不过可以采用梯度修剪（Gradient Clipping）应对，相比之下梯度消失问题更难以解决。 GRUGRU（Gated Recurrent Units, 门控循环单元）网络改进了循环神经网络的隐藏层，从而使梯度消失的问题得到改善。GRU的结构如下图： 其中的$c$代表记忆细胞（Memory Cell），用它来“记住”类似前面例子中“cat”的单复数形式，且这里的记忆细胞$c^{\\langle t\\rangle}$直接等于输出的激活$a^{\\langle t\\rangle}$；$\\tilde{c}$代表下一个$c$的候选值；$\\Gamma_u$代表更新门（Update Gate），用它来控制记忆细胞的更新与否。上述结构的具体表达式有：$$\\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[c^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c)$$ $$\\Gamma_u = \\sigma(W_u[c^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_u)$$ $$c^{\\langle t \\rangle} = \\Gamma_u \\times \\tilde{c}^{\\langle t \\rangle} + (1 - \\Gamma_u) \\times c^{\\langle t-1 \\rangle}$$ $$a^{\\langle t\\rangle} = c^{\\langle t\\rangle}$$$\\tilde{c}$的计算中以tanh作为激活函数，使用simgoid作为激活函数得到的$\\Gamma_u$值将在0到1的范围内。当$\\Gamma_u=1$时，输出的$c$值被更新为$\\tilde{c}$，否者保持为输入的$c$值。 上面所述的是简化后的GRU，完整的GRU结构如下： 其中相关门（Relevance Gate）$\\Gamma_r$表示上一个$c$值与下一个$c$的候选值的相关性。与简化的GRU相比，表达式发生如下变化：$$\\Gamma_r = \\sigma(W_r[c^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_r)$$ $$\\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[\\Gamma_r \\times c^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c)$$GRU其实只是一种LSTM的流行变体，其相关概念来自于2014年Cho等人发表的论文[On the properties of neural machine translation: Encoder-decoder approaches]以及Chung等人的[Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling]。 LSTM1997年Hochreiter和Schmidhuber共同在论文[Long short-term memory ]中提出的LSTM（Long Short Term Memory，长短期记忆）网络比GRU更加通用及强大，其结构如下： 相比之前的简化版GRU，LSTM中多了遗忘门（Forget Gate）$\\Gamma_f$和输出门（Output Gate）$\\Gamma_o$，具体表达式如下：$$\\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c)$$ $$\\Gamma_u = \\sigma(W_u[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_u)$$ $$\\Gamma_f = \\sigma(W_f[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_f)$$ $$\\Gamma_o= \\sigma(W_o[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_o)$$ $$c^{\\langle t \\rangle} = \\Gamma_f^{\\langle t \\rangle} \\times c^{\\langle t-1 \\rangle} + \\Gamma_u^{\\langle t \\rangle} \\times \\tilde{c}^{\\langle t \\rangle}$$ $$a^{\\langle t \\rangle} = \\Gamma_o^{\\langle t \\rangle}\\times \\tanh(c^{\\langle t \\rangle})$$ 更为常用的LSTM版本中，几个门值的计算不只取决于输入$x$和$a$值，有时也可以偷窥上一个细胞输入的$c$值，这叫**窥视孔连接（Peephole Connection)**。 多个LSTM单元连接在一起，形成一个LSTM网络： BRNN与DRNN前面介绍的循环神经网络在结构上都是单向的，也提到过它们具有某一时刻的预测结果仅使用了该时刻之前输入的序列信息的缺陷，而双向循环神经网络（Bidirectional RNN）弥补了这一缺陷。BRNN的结构图如下所示： 此外，循环神经网络的每个时间步上也可以包含多个隐藏层，形成**深度循环神经网络（Deep RNN)**，如下图： 参考资料 吴恩达-序列模型-网易云课堂 Andrew Ng-Sequence Model-Coursera 零基础入门深度学习-循环神经网络 课程代码与资料-GitHub 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2018.02.27 完成初稿","link":"/2018/02/20/ML/deep_learning_9/"},{"title":"【Coursera】机器学习(2)：Logistic回归","text":"这里主要介绍Logistic回归模型及正则化。 分类问题在分类问题中，预测的值是离散的。例如要实现一个垃圾邮件分类器，那么该分类器最后输出的结果应该只有两种–是垃圾邮件或者不是，而不像前面学习过的回归问题那样，输出的结果是与输入一一对应的连续值。 分类问题中，最简单的是二分类（binary classification）问题，其分类结果只有两种，一般选其中一类作为正类（positive class），并使其标记$y = 1$，另一类则作为**反类（negative class)，使其标记$y = 0$，因而$y \\in \\lbrace0, 1\\rbrace$。除此之外还有多分类（multi-classfication)**问题，可以将它转化成多个二分类问题后用二分类问题的解决方法进行解决。 对分类问题，可建立对应的Logistic回归模型来解决。由于历史原因，Logistic回归虽然称为“回归”，却是一种分类问题的学习算法，要注意将其区分。 Logistic回归假设函数分类与前面讲过的回归只是在输出的值上有所区别，考虑继续用线性回归算法来解决分类问题。 如上图中的肿瘤分类器，由训练样本拟合相应的线性回归模型后，可以在中间设定一个阈值，使预测结果高于该阈值的输出为正类，低于则输出为反类，以此起到分类的效果。然而该模型很容易受到一些特殊情况的影响，而产生图中的蓝线所示的偏差较大的结果，此外线性回归模型的输出值不一定都在$0$到$1$之间，与分类问题的要求不符。 延用线性回归中的假设函数$h_\\theta(x) = \\theta^Tx$，且采用sigmoid函数将其值约束在$0$到$1$的范围之内。Sigmoid函数又称Logistic函数，其表达式及图像如下：$$g(z) = \\frac{1}{1 + e^{-z}}$$ 由sigmoid函数的图像可值，其值一直保持在$0$到$1$之间，由此有了Logistic回归的假设函数：$$h_\\theta(x) = g(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}$$ $h_\\theta(x)$的值在此被赋予了新的含义，它表示的是给定参数为$\\theta$、输入为$x$的条件下标签$y=1$的概率，用数学表达式表示即为：$$h_\\theta(x) = P(y=1|x;\\theta)$$ 例如对训练好的肿瘤分类器输入肿瘤的大小$x$后，输出的$h_\\theta(x) = 0.7$，就表示分类器预测这种大小的肿瘤有$70%$的概率是恶性的（即$y = 1$）。 决策边界有了适用于分类的成本函数$h_\\theta(x)$，还需要设定一个阈值作为分类的标准。注意到，sigmoid函数有以下性质： $z \\ge 0$时，$0.5 \\le g(z) \\lt 1$ $z \\lt 0$时，$0 \\lt g(z) \\lt 0.5$ 进而有： $\\theta^Tx \\ge 0$时，$0.5 \\le h_\\theta(x) \\lt 1$ $\\theta^Tx \\lt 0$时，$0 \\lt h_\\theta(x) \\lt 0.5$ 因此可将$0.5$作为阈值，即使得：$$y = \\begin{cases} 1, &amp; h_\\theta(x) \\ge 0.5 \\\\ 0, &amp; h_\\theta(x) \\lt 0.5 \\end{cases}$$ 例如，由上图左边的训练样本，得到了右边成本函数中的各参数值，那么当$0.5 \\le h_\\theta(x)$时，有：$$ \\theta^Tx = -3 + x_1 + x_2 \\ge 0$$即：$$x_1 + x_2 \\ge 3$$ 该不等式取的是分界线以上的部分，也就是正类所在的区域。同理$h_\\theta(x) \\lt 0.5$时，将取图中的反类。而图中的那条分界线，称为两类的决策边界（decision boundary），其位置由参数$\\theta$决定。 此外，sigmoid函数还可以用于非线性函数而得到复杂的分类问题的决策边界： 成本函数Logistic回归中假设函数$h_\\theta(x)$为非线性函数，如继续使用均方误差作为成本函数，得到的将是非凸（non-convex）函数，难以找到最优解。因此在Logistic回归中，我们另设成本函数如下：$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m Cost(h_\\theta(x^{(i)}), y)$$其中：$$Cost(h_\\theta(x), y) = \\begin{cases} - \\log(h_\\theta(x)),&amp; y = 1 \\\\ - \\log(1-h_\\theta(x)), &amp; y = 0 \\end{cases}$$ 当$y = 1$时，该函数的图像为： 此时$h_\\theta(x)=1$时成本为$0$，且$h_\\theta(x) \\to 0$时，成本趋近$\\infty$。 反之，当$y = 0$时函数的图像为： 此时$h_\\theta(x)=0$时成本为$0$，且$h_\\theta(x) \\to 1$时，成本趋近$\\infty$。 以上成本函数，可以用一个式子更为简洁地表示：$$Cost(h_\\theta(x), y)=-y\\log h_\\theta(x^{(i)}) - (1-y)\\log(1 - h_\\theta(x^{(i)}))$$ 该式与原式完全相等，它又被称为交叉熵（cross entropy）损失函数。 由此，分类问题中常用的成本函数$J(\\theta)$的表达式为：$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log h_\\theta(x^{(i)}) + (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)}))]$$ 该函数也可以通过概率统计中常用的最大似然估计法推导出来，具体过程见深度学习(1)：Logistic回归。 梯度下降有了成本函数后，依然采用梯度下降法来将其最小化，以学习Logistic回归模型中的参数值。 下面对该成本函数的导数进行推导： 为方便计算，成本函数中的$\\log$默认以$\\exp$为底。展开其中的几项，有：$$ \\log h_\\theta(x^{(i)}) = \\log \\frac{1}{1 + e^{-\\theta^Tx^{(i)}}} = -\\log(1 + e^{-\\theta^Tx^{(i)}}) $$ $$ \\log(1 - h_\\theta(x^{(i)})) = \\log \\frac{e^{-\\theta^Tx^{(i)}}}{1 + e^{-\\theta^Tx^{(i)}}} = \\log e^{-\\theta^Tx^{(i)}} - \\log (1 + e^{-\\theta^Tx^{(i)}})$$ $$ = -\\theta^Tx^{(i)} - \\log (1 + e^{-\\theta^Tx^{(i)}})$$ 由此：$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [-y^{(i)} \\log(1 + e^{-\\theta^Tx^{(i)}}) - (1 - y^{(i)})(\\theta^Tx^{(i)} + \\log (1 + e^{-\\theta^Tx^{(i)}}))]$$ $$ = -\\frac{1}{m}\\sum_{i=1}^m [y^{(i)}\\theta^Tx^{(i)} - \\theta^Tx^{(i)} - \\log (1 + e^{-\\theta^Tx^{(i)}})] $$ $$ = -\\frac{1}{m}\\sum_{i=1}^m [y^{(i)}\\theta^Tx^{(i)} - (\\log e^{\\theta^Tx^{(i)}} + \\log (1 + e^{-\\theta^Tx^{(i)}}))] $$ $$ = \\frac{1}{m}\\sum_{i=1}^m [\\log (1 + e^{\\theta^Tx^{(i)}}) - y^{(i)}\\theta^Tx^{(i)}] $$ 对其求导，有：$$ \\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m [\\frac{\\partial}{\\partial \\theta_j} \\log (1 + e^{\\theta^Tx^{(i)}}) - \\frac{\\partial}{\\partial \\theta_j} (y^{(i)}\\theta^Tx^{(i)})] $$ $$ = \\frac{1}{m}\\sum_{i=1}^m \\frac{x^{(i)}je^{\\theta^Tx^{(i)}}}{1 + e^{\\theta^Tx^{(i)}}} - y^{(i)}x^{(i)}_j$$ $$ = \\frac{1}{m}\\sum{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j$$ 从而梯度下降更新参数值的过程为：$$\\begin{aligned} &amp; \\text{Repeat} \\ \\lbrace \\\\ &amp; \\ \\ \\ \\ \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\\\ &amp; \\rbrace \\end{aligned}$$ 除了梯度下降以外，还存在一些常用的较复杂的优化算法，如共轭梯度法（onjugate gradient）、BFGS、L-BFGS等，它们都可以用来代替梯度下降法进行参数学习。 预防过拟合模型评估在机器学习中，训练出来的模型经常存在下面几种情况： 对上面的回归问题，最为理想的情况是中间所示的模型，另外两种情况则分别称为： 欠拟合（underfitting）：模型与样本间存在较大的偏差（bias），如上面的左图 过拟合（overfitting）：模型与样本间存在较大的方差（variance），如上面的右图 用Logistic回归解决分类问题时，这些情况也是存在的： 其中，过拟合将导致模型的泛化（generalization）能力不够强，而它很容易在训练样本包含的特征种类过多而训练样本的数量却相对较少时发生，因此常用来预防过拟合的方法有： 通过人工筛选或模型选择算法丢弃部分训练样本的特征 正则化（regularization）成本函数 下面重点讲解其中的第二种方法。 正则化 上面的右图所示的回归模型明显存在过拟合，要解决该问题，最简单的做法就是想办法使模型中的参数$\\theta_3$、$\\theta_4$尽可能地接近于$0$，而使其近似地等于左图中最为理想的模型。 进一步地，我们知道，参数$\\theta$的值都是通过最小化成本函数而求得的。那么要达到上述目的，可以考虑将这几个参数“捆绑”在成本函数上：$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 + 1000\\theta_3^2 + 1000\\theta_4^2$$ 这样，在最小化成本函数时，参数$\\theta_3$、$\\theta_4$将受到“惩罚”而一同被最小化，以此达到防止过拟合的目的。所谓的正则化过程也与此类似。 建立机器学习模型时，学习到的参数值较小，就意味着假设函数会是一个相对“简单”的函数，过拟合也更不容易发生。正则化的思想即在于此，其具体做法是在成本函数后加入正则化项，对线性回归模型为：$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{i=1}^n \\theta_j^2$$ 对Logistic回归模型则为：$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log h_\\theta(x^{(i)}) + (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{i=1}^n \\theta_j^2$$ 其中$\\lambda$为正则化参数，需选取适当的值，其值过大容易导致欠拟合；$\\sum_{i=1}^n \\theta_j^2$是最常用的正则化项，它被称为L2范数，另有L0、L1范数。正则化保留了所有的特征，而通过使所有参数值最小化来防止过拟合。 对线性回归或Logistic回归模型，用梯度下降法最小化正则化后的成本函数的过程均为：$$\\begin{aligned} &amp; \\text{Repeat}\\ \\lbrace \\\\ &amp; \\ \\ \\ \\ \\theta_0 := \\theta_0 - \\alpha\\ \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\\\ &amp; \\ \\ \\ \\ \\theta_j := \\theta_j - \\alpha\\ \\left[ \\left( \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\right] &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j \\in \\lbrace 1,2…n\\rbrace \\\\ &amp; \\rbrace \\end{aligned}$$ 由于$\\theta_0$的值恒为$1$，不需要将它正则化，所以迭代过程种分成了两步。 另外，采用正规方程直接求解线性回归模型中的参数$\\theta$时，进行正规化的表达式为：$$\\theta = \\left( X^TX + \\lambda \\cdot L \\right)^{-1} X^Ty$$其中$L$是个大小为$(n+1)\\times(n+1)$的矩阵：$$L = \\begin{bmatrix} 0 &amp; &amp; &amp; &amp; \\\\ &amp; 1 &amp; &amp; &amp; \\\\ &amp; &amp; 1 &amp; &amp; \\\\ &amp; &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; &amp; 1 \\\\ \\end{bmatrix}$$ 前面提到过正规方程种$X^TX$为奇异矩阵或非方阵时，它将不存在逆矩阵。对正规方程进行正规化后，就不会出现这种情况，$X^TX + \\lambda \\cdot L$将一定是可逆的。 编程作业见如下github链接，程序仅供参考: ex2-python版 参考资料 Andrew Ng-Machine Learning-Coursera 吴恩达-机器学习-网易云课堂 交叉熵代价函数(损失函数)及其求导推导-CSDN 牛顿法 拟牛顿法DFP BFGS L-BFGS的理解-CSDN 机器学习中常常提到的正则化到底是什么意思？-知乎 机器学习中的范数规则化-CSDN 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Machine Learning课程及上述书籍、博客资料，版权归各作者所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2019.03.24 完成初稿","link":"/2019/03/21/ML/machine_learning_2/"},{"title":"SQL基础教程","text":"《SQL基础教程（第2版）》读书笔记 基础知识管理数据库的计算机系统被称为数据库管理系统（DataBase Management System，DBMS），按数据的保存格式，其主要可分为五类： 层次数据库（Hierarchical，HDB）：把数据通过层次（树形）结构表现出来 关系数据库（Relational，RDB)：用行和列组成的二维表来管理数据 面向对象数据库（Objec Oriented, OODB）：把数据及其操作集合起来以对象为单位进行操作 XML数据库（XMLDB）：处理XML数据 键值存储系统（Key-Value Store, KVS）：保存查询所用的主键和值的组合 目前应用最广也是数据库为RDB，这里要学习的就是专用用来对RDB数据进行操作的结构化查询语言（Structured Query Language，SQL）。RDBMS常采用客户端/服务器（C/S）结构，当前具有代表性的有： Oracle Database SQL Server (Microsoft) DB2 (IBM) PostgreSQL MySQL 在RDB中，包含如下概念： 表（table）：用来管理数据的二维表 字段：表的列，表示表中的数据项 记录：表的行，表示一条数据 在RDBMS中，必须以行为单位进行数据读写，而用来实现这些操作的一条（ISO定义的）标准SQL语句通常由关键字、表名、列名等组合而成，根据对RDBMS赋予的指令种类，SQL语句大致可分为三类： DDL（Data Definition Language，数据定义）：对数据库或表对象进行增删改操作 DML（Date Manipulation Lanugage，数据操纵）：对表中的数据进行增删改操作 DCL（Data Control Lanugage，数据控制）：确定、取消数据变更、设定用户权限等操作 实际使用的SQL语句大都为DML，这里重点学习的也是DML。SQL语句不区分大小写，通常习惯将关键字及表名首字母大写，除数字可以直接书，字符串、时间等常数都需要包括在单引号（’）中，各条语句以分号（;）结束。 基本操作语句表对象操作对数据库或表对象进行增删改的常用SQL语句有： 123456CREATE DATABASE &lt;database_name&gt;; -- 创建数据库CREATE TABLE &lt;table_name&gt;( &lt;line_name_1&gt; &lt;data_type&gt; &lt;constraint&gt;, &lt;line_name_n&gt; &lt;data_type&gt; &lt;constraint&gt;); -- 创建并定义表DROP TABLE &lt;table_name&gt;; -- 删除表ALTER TABLE &lt;table_name&gt; &lt;ops&gt;; -- 更新表的定义 其中及可由半角英文字母、数字、下划线组成，创建表的同时需要定义表中各字段的数据类型及约束，标准SQL中几种常见的数据类型有： INTEGER：整型 CHAR：定长字符串，以定长存储，长度可自行指定 VARCHAR：可变字符串，以边长存储，长度可自行指定 DATE：日期 NUMERIC：数值，可定义全体及小数位数 约束用以对各字段中存储的数据进行一些限制或追加条件，几种常用的约束有： PRIMARY KEY：主键 NOT NULL：非空 DEFAULT：默认值 更新表的定义便是表中的字段进行增改，中常用的有： 123ADD COLUMN &lt;line_define&gt;; -- 新增列DROP COLUMN &lt;line_name&gt;; -- 删除列RENAME TO &lt;new_name&gt;; -- 重命名 在PostgreSQL中，使用这些语句操作表对象的例子如下： 12345678910111213141516CREATE DATABASE shop; -- 创建数据库shopCREATE TABLE Product(product_id CHAR(4) NOT NULL, product_name VARCHAR(100) NOT NULL, product_type VARCHAR(32) NOT NULL, sale_price INTEGER , purchase_price INTEGER , regist_date DATE , PRIMARY KEY (product_id)); -- 创建并定义表ProductALTER TABLE Product ADD COLUMN product_name_pingyin VARCHAR(100); -- 增加字段ALTER TABLE Product RENAME TO product_name_chinese; -- 重命名字段ALTER TABLE Product DROP COLUMN product_name_chinese; -- 删除字段 DROP TABLE Product; -- 删除表Product 表的增删改对表中数据进行增删改的常用SQL语句有： 123INSERT INTO &lt;table_name&gt; (&lt;line_name_list&gt;) VALUES (&lt;value_list&gt;); -- 插入数据DELETE FROM &lt;table_name&gt; WHERE &lt;cond_expr&gt;; -- 依条件删除数据UPDATE &lt;table_name&gt; SET (&lt;line_name_list&gt;) = (&lt;expr&gt;) WHERE &lt;cond_expr&gt;; -- 依条件更新字段 插入数据时，清单（list）中的名字或值须以逗号（,）分隔，要插入的数据值清单必须和字段名清单一一对应，且可省略，此时将默认以从左到右的顺序赋到各个字段上，例如向Product中插入数据： 12INSERT INTO Product (product_id, product_name, product_type, sale_price, purchase_price, regist_date) VALUES ('0001', 'T恤' ,'衣服', 1000, 500, '2009-09-20');INSERT INTO Product VALUES ('0002', '打孔器', '办公用品', 500, 320, '2009-09-11'); -- 两者的效果是一样的 更新或删除数据时，可通过WHERE子句来指定条件，是由比较及逻辑运算符（AND、OR、NOT）组成的条件表达式，可以为常量算术表达式，没有WHERE子句时将对整张表进行相应的操作。要注意的是，在SQL中进行比较运算时用“&lt;&gt;”来表示不等于，判断表中为N记录项是否为空是不能用“=”，而需要用IS NULL、IS NOT NULL。 以上语句的用例如下所示： 123DELETE FROM Product WHERE sale_price &gt;= 4000 AND purchase_price &gt;= 4000; -- 删除符合条件的数据UPDATE Product SET regist_date = '2009-10-10'; -- 整张表的regist_date字段都更新为该值UPDATE Product SET sale_price = sale_price * 10 WHERE product_type = '厨房用具'; -- 根据条件更新数据 查询语句基本查询SELECT是SQL最基本也是最重要的语句，它常用以查询表中的数据，几个基本的SELECT查询语句如下： 1234SELECT &lt;line_name&gt; FROM &lt;table_name&gt;; -- 简单查询SELECT &lt;line_name&gt;/&lt;constant&gt; AS &lt;alias&gt; FROM &lt;table_name&gt;; -- 取别名SELECT DISTINCT &lt;line_name&gt; FROM &lt;table_name&gt;; -- 去重SELECT &lt;line_name&gt; FROM &lt;table_name&gt; WHERE &lt;cond_expr&gt;; -- 依条件查询 查询过程中，在SELECT子句中列出要查询的字段名，得到的查询结果将字段的罗列顺序一致，SQL语句中常用星号（*）来代表查询表中的所有字段。查询过程中还可以用AS关键字为一些字段取别名，查询结果中的字段名就会被别名所替换，以中文作为别名时需要用双引号（”）将文本括起来。另外，SELECT子句中还可以接常数，此时在得到的查询结果中，所有记录的该字段都将为该常数值。 在SELECT子句中使用DISTINCT关键字，可以根据一些字段来删除查询结果中的重复数据，也通过WHERE子句来指定条件，筛选出想要的查询结果。 这些基本查询语句的用例如下： 12345SELECT product_id, product_name, purchase_price FROM Product; -- 查询其中三列SELECT product_id AS &quot;商品编号&quot;, product_name AS &quot;商品名称&quot; FROM Product; -- 在查询结果中为一些字段设置别名SELECT '商品' AS string, 38 AS number, product_id, product_name FROM Product; -- 查询结果中所有记录的string、number字段都是相同的常数SELECT DISTINCT product_type, regist_date FROM Product; -- 对两个字段组合后的数据去重SELECT product_name, product_type FROM Product WHERE product_type = '衣服’; -- 得到符合条件的查询结果 聚合查询通过SQL对数据进行某种操作或计算时需要使用函数，其中对数据进行一些汇总操作的函数就被称为聚合函数，常用的聚合函数有 COUNT()：计算表中的记录数 SUM()：计算某字段的合计值 AVG()：计算某字段的平均值 MAX/MIN()：求出某字段的最大/最小值 以聚合函数为SELECT子句，结合GROUP BY关键字，可以实现对表中地数据进行分组汇总，其基本地语法如下： 1SELECT &lt;line_name&gt; FROM &lt;table_name&gt; GROUP BY &lt;line_name&gt;; -- 分组聚合 例如，可使用如下语句来统计Product中各类产品的数量： 1SELECT product_type, COUNT(*) FROM Product GROUP BY product_type; -- 分组聚合 其中GROUP BY子句中的字段名被称为聚合键，它决定了表的分组方式，SELECT子句中除了聚合函数外不可出现除该聚合键以外的字段名。 句中可引入WHERE语句，以实现依条件进行汇总： 1SELECT &lt;line_name&gt; FROM &lt;table_name&gt; WHERE &lt;cond_expr&gt; GROUP BY &lt;line_name&gt;; -- 根据条件进行分组聚合 GROUP BY子句一定要写在FROM及WHERE语句之后，且上述语句中各子句的执行顺序如下： FROM -&gt; WHERE -&gt; GROUP BY -&gt; SELECT 因此，GROUP BY中不可使用SELECT子句中由AS定义的别名作为聚合键。 用WHERE语句中只能对原始记录指定条件，要由分组聚合结果指定条件，即从分组结果中筛选数据，就需要用到HAVING子句，其语法如下： 1SELECT &lt;line_name&gt; FROM &lt;table_name&gt; GROUP BY &lt;line_name&gt; HAVING &lt;cond_expr&gt;; -- 从分组结果中依条件选择 HAVING子句可由常数、聚合函数或聚合键构造。例如，将Product按商品种类分组后，可使用如下语句取出其中数据行为2的组： 1SELECT product_type. COUNT(*) FROM Product GROUP BY product_type HAVING COUNT(*) = 2; 加入HAVING子句后，整个查询语句的执行顺序如下： SELECT -&gt; FROM -&gt; WHERE -&gt; GROUP BY -&gt; HAVING 在查询表中数据时，默认情况下的查询结果都是无序的。通过ORDER BY关键字，可查询到的表记录按某个字段进行排序，具体的语法如下： 1SELECT &lt;line_name&gt; FROM &lt;table_name&gt; ORDER BY &lt;line_name&gt; DESC/ASC; -- 按某字段进行升/降排序 其中通过最后的关键字为DESC或ASC来指定升降序。ORDER BY子句中的字段名被称为排序键，类似GROUP BY，排序键也可以有多个，且越先写的排序键优先级越高。 ORDER BY子句通常要放在SELECT语句的末尾，其中可以使用别名，因为ORDER BY子句总在最后执行，具体顺序如下： FROM -&gt; WHERE -&gt; GROUP BY -&gt; HAVING -&gt; SELECT -&gt; ORDER BY 要注意的是，聚合函数只能在SELECT、HAVING、ORDER BY子句中使用，WHERE子句中不可出现聚合函数。 事务与视图所谓的事务（transaction），指的是需要在同一个处理单元中执行的一系列更新处理的集合。表的增删改操作常需要用多条SQL语句才能完成，因此可以多条这样的SQL语句封装成一个事务来统一执行。在RDBMS中，事务是对表中数据进行更新的单位。 封装多条用于操作表的数据的SQL语句的方法，及构建事务的语法如下： 12345事务开始语句; DML语句1; DML语句2; ...事务结束语句; 例如下面在PostgreSQL中创建一个事务： 1234BEGIN TRANSACTION; UPDATE Product SET sale_price = sale_price - 1000 WHERE product_name = '运动T恤'; UPDATE Product SET sale_price = sale_price + 1000 WHERE product_name = 'T恤衫';COMMIT; 标准SQL中并没有定义事务的开始语句，因此此语句随DBMS的不同而不同，如PostgreSQL中规定的开始语句为“BEGIN TRANSACTION”，MySQL中的则为“START TRANSCTION”。事务的结束语句可为“COMMIT”或“ROLLBACK”，前者是按顺序执行事务中的DML语句并在最终提交，后者则是取消事务中所有对数据库的更新处理而恢复其原始状态。 DBMS中的事务都需要遵循四大特性： 原子性（Atomicity）：事务结束时，其中包含的更新处理要么全部执行，要么全部不执行 一致性（Consistency）：事务中包含的处理要满足数据库提前设置的约束 隔离性（Isolation）：不同事务间互不干扰 持久性（Durability）：事务结束后，DBMS能保证该时间点的数据被保存，即使发生故障也有恢复手段 取每个特性的首字符，这四大特性又称ACID特性。 事务是将多条对数据库进行增删改的语句封装到一起，如果将一些常用的查询语句封装起来，就有了视图（VIEW）。查询语句执行后的结果是一张表，要保存这张表的话需要一定存储空间，将查询语句封装为视图，需要这张表的时候只需要调用该视图即可，这可以极大地节省存储空间。 创建视图地语法如下： 123CREATE VIEW &lt;view_name&gt; (&lt;view_line_name_1&gt;, &lt;view_line_name_2&gt;, ...)AS SELECT语句; 例如将前面例子中用过的一条查询语句封装为ProductSum视图： 123CREATE VIEW ProductSum (product_type, cnt_product)AS SELECT product_type, COUNT(*) FROM Product GROUP BY product_type; 视图中将SELECT语句的查询结果中包含的字段为其字段，并为它们设置别名，且视图的使用方法也与普通的表无异： 1SELECT product_type, cnt_product FROM ProductSum; 要注意的是，因为RDB中的记录本身都是无序的，所以不能在视图中使用ORDER BY语句进行排序。虽然对视图的操作与表无异，但无法随意使用更新语句对视图进行数据更新，因为视图的更新过程将对原表中数据产生直接影响。 另外，可以使用如下语句删除某个视图： 1DROP VIEW &lt;view_name&gt;; 子查询所谓子查询，是将SELECT语句进行多层嵌套，而直接以SELECT语句作为某些关键字的子句。如下面的例子： 123SELECT Product_type, cnt_product FROM ( SELECT Product_type, COUNT(*) AS cnt_product FROM Product GROUP BY product_type;) AS ProductSum; 其中内层的SELECT语句将首先被执行并得到查询结果，外层的查询将以这些结果为源数据进行二次查询。简单来说，子查询就是一次性视图，在SELECT语句执行结束后便消失，而不是像视图那样永久保存在磁盘上。在上例中，还使用了AS关键字为这个子查询命名。 子查询可以进行无限嵌套，例如下面的SELECT语句中便嵌套了两层的SELECT语句： 12345SELECT product_type, cnt_product FROM ( SELECT * FROM ( SELECT product_type, COUNT(*) AS cnt_product FROM Product GROUP BY product_type ) AS ProductSum WHERE cnt_product = 4) AS ProductSum2; 返回结果为一行一列即单一值的子查询语句被称为标量子查询。不管是SELECT、WHERE子句还是GROUP BY、HAVING、ORDER BY子句，只要是能够用常数或列名的地方，都可以使用标量子查询语句。例如下面的例中WHERE子句的条件表达式： 1SELECT product_id, product_name, sale_price FROM Product WHERE sale_price &gt; (SELECT AVG(sale_price) FROM Product); 有时想要在GOROUP BY细分好的各组内进行一些比较操作，例如根据product_type将Product中的记录分组后，想要分别选出各组中价格高于该平均价格的商品，可能会想到如下语句： 1SELECT prodcut_id, product_name, sale_price FROM Product WHERE sale_price &gt; (SELECT AVG(sale_price) FROM Product GROUP BY product_type); 然而这是一个错误的SQL语句，其中的SELECT子查询语句不为标量，无法参与比较。要解决这类问题，需要用到关联子查询，如下所示： 123SELECT product_type, product_name, sale_price FROM Product AS P1 WHERE sale_price &gt; ( SELECT AVG(sale_price) FROM Product AS P2 WHERE P1.product_type = P2.product_type GROUP BY product_type); 子查询语句中，会根据product_type返回相应的平均价格，从而筛选中表中满足要求的记录。 集合与联结SQL语句除了能对RDB中的表进行增删改以及查询操作外，还能在多个表之间进行集合运算。其中用到的关键字如下： 123SELECT ... FROM &lt;table_name_1&gt; UNION SELECT ... FROM &lt;table_name_2&gt;; -- 并集SELECT ... FROM &lt;table_name_1&gt; INTERSECT SELECT ... FROM &lt;table_name_2&gt;; -- 交集SELECT ... FROM &lt;table_name_1&gt; EXCEPT SELECT ... FROM &lt;table_name_2&gt;; -- 差集 其中UINON、INTERSERT、EXCEPT关键字分别代表了对两表进行并集、交集、差集运算。对于两个具有相同字段的表，并集是将这些记录进行合并，交集则选出两表中的公共记录，差集则将把公共记录从第一个表中去除，这些过程都不会保留重复的记录，可在这些关键字后面添加ALL来把重复数据予以保留。 集合运算是以行方向为单位进行操作，而联结是以列为单位对表进行的操作。在联结运算中，应用最为广泛的为内联结，其用到的关键字为INNER JOIN。它是以两表共有的字段为桥梁，从而将两表的字段合并到一起而得到一张表。 内联结语句的语法为： 1SELECT &lt;line_name&gt; FROM &lt;table_name_1&gt; INNER JOIN &lt;table_ame_2&gt; ON &lt;expr&gt;; -- 由联结键内联 其中关键字ON后面的表达式便是联结两表的桥梁——联结键。例如有下面一张表ShopProduct： 1234567891011121314151617181920CREATE TABLE ShopProduct(shop_id CHAR(4) NOT NULL, shop_name VARCHAR(200) NOT NULL, product_id CHAR(4) NOT NULL, quantity INTEGER NOT NULL, PRIMARY KEY (shop_id, product_id)); INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000A', '东京', '0001', 30);INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000A', '东京', '0002', 50);INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000A', '东京', '0003', 15);INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000B', '名古屋', '0002', 30);INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000B', '名古屋', '0003', 120);INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000B', '名古屋', '0004', 20);INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000B', '名古屋', '0006', 10);INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000B', '名古屋', '0007', 40);INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000C', '大阪', '0003', 20);INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000C', '大阪', '0004', 50);INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000C', '大阪', '0006', 90);INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000C', '大阪', '0007', 70);INSERT INTO ShopProduct (shop_id, shop_name, product_id, quantity) VALUES ('000D', '福冈', '0001', 100); 该表与Product有相同的字段product_id，于是可以以此为联结键，将两表的字段进行合并： 123SELECT SP.shop_id, SP.shop_name, SP_product_id, P.product_name, P.sale_price FROM ShopProduct AS SP INNER JOIN Product AS P ON SP.product_id = P.product_id; 其中使用了别名以分别从两表中选出想要查询的字段，当ShopProduct中某个记录的product_id与Product中的某个记录一致，Product中该记录的几个字段就会被联结到ShopProduct上。使用这样的联结运算将满足规则的表联结起来时，前面用到的WHERE、GROUP BY HAVING、ORDER BY等语句都能正常使用。 既然有内连接，那必有与之相对的外联结。外联结以OUTER JOIN为关键字，过程中可使用关键字LEFT或RIGHT，将OUTER JOIN左边或右边的表作为主表，联结过程中不管与之联结的表中是否存在对应字段，主表中的记录都会被全部保留。如下面的例子所示： 1234567SELECT SP.shop_id, SP.shop_name, SP_product_id, P.product_name, P.sale_price FROM ShopProduct AS SP RIGHT OUTER JOIN Product AS P ON SP.product_id = P.product_id;SELECT SP.shop_id, SP.shop_name, SP_product_id, P.product_name, P.sale_price FROM Product AS P LEFT OUTER JOIN ShopProduct AS SP ON SP.product_id = P.product_id; -- 两条语句的结果一致 除了内外联之外，还存在一种称为交叉联结的表联结方式，在实际情况中的应用相对较少，具体的语法如下： 1SELECT &lt;line_name&gt; FROM &lt;table_name_1&gt; CROSS JOIN &lt;table_ame_2&gt;; -- 交叉联结 该联结过程是满足相同规则的表进行笛卡尔积运算，因此不支持ON关键字，要得到与内外联一致的结果，就必须使用WHERE关键字从大量的结果中进行筛选。 函数、谓词、CASE在聚合查询中是曾了解过几个简单的聚合函数，除此之外，SQL中还存在用以进行数值计算的算术函数、字符串操作的字符串函数、日期操作的日期函数及用于转换数据类型和值的转换函数。在这些函数中常用的如下： ABS(*)：求绝对值 MOD(m, n)：求余（m/n） ROUND(m, n)：舍入m（n为保留位数） ||/CONCAT()：拼接字符串 LENGTH()：求字符串长度 LOWER/UPPER()：对字符串进行大/小写转换 REPLACE(str, m, n)：将字符串str中的m替换为n SUBSTRING(str FROM m FOR n）：从str的第m位开始截取n个 CURRENT_DATE：当前日期 CURRENT_TIME：当前时间 CURRENT_TIMESTAMP：当前日期、时间 EXTRACT(m FROM n)：从时间n中截取元素m CAST(m AS n)：m的类型转换为n COALESCE(date1, date2, …)：参数可变，返回参数中左侧第一个不是NULL的值，常可用在含有NULL的记录项时防止最后的计算结果变为NULL SQL中还存在一个谓词的概念，所谓的谓词简单来说就是返回结果真值（True、False或Unknown）的函数，它们可以用于SQL语句中的WHERE、HAVING等子句中，参与各种逻辑计算。 LIKE谓词可用来进行模糊查询，从表中筛选出符合某种组合模式的数据。其中可用的通配符有： %：0个以上任意字符 _：1个任意字符 例如，可用如下语句从表中选出符合模式“任意个任意字符+ab+1个任意字符”的字符串： 1SELECT * FROM SampleLike WHERE strcol LIKE '%ab_'; BETWEEN谓词可用来代替通常的大于、小于号加逻辑运算符的组合，方便地实现范围查询，例如： 123SELECT product_name, sale_price FROM Product WHERE sale_price BETWEEN 100 AND 1000;SELECT product_name, sale_price FROM Product WHERE sale_price &gt;= 100 AND sale_price &lt;= 1000; -- 两者的结果一致 IS NULL和IS NOT NULL谓词则用来判断某个记录项是否为NULL。IN及NOT IN谓词可以用来代替OR，查询某个项目是否存在于某个集合中，如下所示： 123SELECT product_name, purchase_price FROM Product WHERE purchase_price IN (320, 500, 5000); SELECT product_name, purchase_price FROM Product WHERE purchase_price = 320 OR purchase_price = 500 OR purchase_price = 5000; -- 两者的结果一致 另外还有一个功能强大的谓词EXIST，可用其代替IN/NOT IN，但该谓词的语法较为复杂及难以理解。由表ShopProduct和Product，使用该谓词来选出“东京店在售之外的商品的售价”的语句如下： 12345SELECT product_name, sale_price FROM Product AS P WHERE EXISTS ( SELECT * FROM ShopProduct AS SP WHERE SP. shop_id = '000C' AND SP.product_id = P.product_id ); 使用SQL进行查询时，还可以使用CASE进行条件分支，其语法如下： 1234CASE WHEN &lt;value_expr&gt; THEN &lt;expr&gt; WHEN &lt;value_expr&gt; THEN &lt;expr&gt; ELSE &lt;expr&gt;END 例如： 1234567SELECT product_name, CASE WHEN product_type = '衣服' THEN 'A：' || product_type WHEN product_type = '办公用品' THEN 'B：' || product_type WHEN product_type = '厨房用具' THEN 'C：' || product_type ELSE NULL END AS abc_product_type i FROM Product; 参考资料 SQL基础教程-豆瓣 SQL基础教程（笔记）-幕布 更新历史： 2019.11.12 完成初稿","link":"/2019/11/04/CS/sql_tutorial/"},{"title":"【Android】香农APP","text":"一款可译用来统计信息熵，进行二元香农、诺顿、哈夫曼编码，以及判断分组码是否唯一可译的Android APP。 统计信息熵信息指的是各个事物运动的状态及状态变化的方式，它是使认识主体对某一事物的未知性或不确定性减少的有用知识。其基本概念就在于它的不确定性，任何已确定的事物都不含有信息。 在香农（C.E.Shannon）提出的狭义信息论中，给出了一种信息量的度量方式，即**信息熵(Information Entropy)**。在热力学中，“熵”是用来表示分子状态混乱程度的物理量。香农把这个词借用过来，用于描述信源的不确定度。 某个信源发出的符号通常可以根据各种符号出现的概率来度量。根据信息的定义，概率大，出现的机会多，不确定性小，也就是包含的信息量小；概率小，出现的机会少，不确定性大，包含的信息量反而更多。由此入手，对符号的出现概率取倒数，这样得到的值就符合这个特性了。为了将这个值约束在一定范围内，不至于过大或过小，进一步对这个值取对数，这样就得到了某个概率为$p$的符号$x$的自信息量,具体公式为：$$ I(x_i) = - \\log p(x_i) $$对于包含多个符号的信息，其信息熵就取它们的平均信息量，即：$$ H(X) = -\\sum_{i} p(x_i) \\log p(x_i) $$ APP中，使用Java语言编写的统计信息熵程序如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778// 符号数量统计与熵计算package top.binweber.shannon;import java.math.BigDecimal;import java.util.Map;import java.util.TreeMap;public class EntropyCalculate { private static int count; private static Map&lt;Character,Integer&gt; amountMap; private static Map&lt;Character,Double&gt; probabilityMap; public EntropyCalculate() { // 初始化 count = 0; // 总符号数 amountMap = new TreeMap&lt;&gt;(); // 各符号数量Map probabilityMap = new TreeMap&lt;&gt;(); // 各符号概率Map } public Map&lt;Character,Integer&gt; amountCal(String textStr) { // 数量统计 char[] chars = textStr.toCharArray(); // String转换为Char for(int i = 0; i &lt; chars.length; i++) { // 遍历，统计 if(!(chars[i] &gt;= 'a' &amp;&amp; chars[i] &lt;= 'z' || chars[i] &gt;= 'A' &amp;&amp; chars[i] &lt;= 'Z')) // 判断符号范围 continue; count ++; // 总数统计 Integer amount = amountMap.get(chars[i]); // 获取符号的当前数量 if (amount != null) // 当前数量不为null amount ++; else // 当前数量为null amount = 1; amountMap.put(chars[i], amount); // 存入Map中 } return amountMap; } public Map&lt;Character,Double&gt; probabilityCal() { // 概率计算 double pro = 0; // 概率 for(Map.Entry&lt;Character, Integer&gt; entry : amountMap.entrySet()) { // 遍历，统计 pro = (double) entry.getValue() / count; // 计算概率 BigDecimal bg = new BigDecimal(pro); pro = bg.setScale(3, BigDecimal.ROUND_HALF_UP).doubleValue(); // 保留3位小数 probabilityMap.put(entry.getKey(), pro); // 存入Map中 } return probabilityMap; } public double entropyCal() { // 符号熵计算 double entropy = 0; // 符号熵 for(Double p : probabilityMap.values()) { // 遍历 entropy += -(p * Math.log(p) / Math.log(2)); // 符号熵计算 } BigDecimal bg = new BigDecimal(entropy); entropy = bg.setScale(3, BigDecimal.ROUND_HALF_UP).doubleValue(); // 保留3位小数 return entropy; } public double entropyCal(Map&lt;Character,Double&gt; probMap) { double entropy = 0; for(Double p : probMap.values()) { entropy += -(p * Math.log(p) / Math.log(2)); } BigDecimal bg = new BigDecimal(entropy); entropy = bg.setScale(3, BigDecimal.ROUND_HALF_UP).doubleValue(); // 保留3位小数 return entropy; }} 香农编码APP中，使用Java语言编写的香农编码过程如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package top.binweber.shannon;import java.math.BigDecimal;import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;import java.util.List;import java.util.Map;import java.util.TreeMap;public class ShannonEncode { private static Map&lt;Character,Integer&gt; cLengthMap; // 码长Map private static Map&lt;Character, String&gt; codonMap; // 码字Map private static List&lt;Map.Entry&lt;Character, Double&gt;&gt; probArrayList; //概率值List private static double probs; // 累加概率 private static double aCLength; // 平均码长 public ShannonEncode(Map&lt;Character,Double&gt; probMap) { // 初始化 probs = 0.0; aCLength = 0.0; cLengthMap = new TreeMap&lt;&gt;(); codonMap = new TreeMap&lt;&gt;(); probArrayList = new ArrayList&lt;&gt;(probMap.entrySet()); // 按概率从大到小排序 Collections.sort(probArrayList,new Comparator&lt;Map.Entry&lt;Character,Double&gt;&gt;() { public int compare(Map.Entry&lt;Character,Double&gt; o1, Map.Entry&lt;Character,Double&gt; o2) { return o2.getValue().compareTo(o1.getValue()); } }); } public Map&lt;Character, String&gt; toShannon() { // 实现编码 double prob = 0; int cLength = 0; for(Map.Entry&lt;Character,Double&gt; entry:probArrayList) { // 由概率Map循环 prob = entry.getValue(); // 取出一个概率值 cLength = (int) Math.ceil(Math.log(1/prob)/Math.log((double)2)); // 计算码长，Math.ceil()-向上取整 cLengthMap.put(entry.getKey(), cLength); // 将算得的码长打包到Map codonMap.put(entry.getKey(), colonCal(probs, cLength)); // 调用下面写好的colonCal方法，算得码字，打包到Map aCLength += prob * cLength; // 计算平均码长 probs += prob; // 概率值累加 } return codonMap; } private String colonCal(double probs, int nLength) { // 将累加概率转换为二进制（码字） String coden = &quot;&quot;; for(int i = 0; i &lt; nLength; i++) { probs *= 2; // 累加概率乘以2 if(probs &gt;= 1) { probs -= 1; coden += 1; // 大于1则取1 } else { coden += 0; //小于1则取0 } } return coden; } public Map&lt;Character, Integer&gt; getCLengthMap() { return cLengthMap; } public List&lt;Map.Entry&lt;Character, Double&gt;&gt; getProbArrayList() { return probArrayList; } public double getACLength() { BigDecimal bg = new BigDecimal(aCLength); aCLength = bg.setScale(3, BigDecimal.ROUND_HALF_UP).doubleValue(); return aCLength; }} 费诺编码APP中，使用Java语言编写的费诺编码过程如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113package top.binweber.shannon;import android.util.Log;import java.math.BigDecimal;import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;import java.util.List;import java.util.Map;import java.util.TreeMap;public class FanoEncode { private static Map&lt;Character,Integer&gt; cLengthMap; // 码长Map private static Map&lt;Character, String&gt; codonMap; // 码字Map private static List&lt;Map.Entry&lt;Character, Double&gt;&gt; probArrayList; // 概率List private static double aCLength; // 平均码长 public FanoEncode(Map&lt;Character,Double&gt; probMap) { // 初始化 aCLength = 0.0; cLengthMap = new TreeMap&lt;&gt;(); codonMap = new TreeMap&lt;&gt;(); probArrayList = new ArrayList&lt;&gt;(probMap.entrySet()); // 将概率Map按概率值递减排列放到概率List Collections.sort(probArrayList,new Comparator&lt;Map.Entry&lt;Character,Double&gt;&gt;() { public int compare(Map.Entry&lt;Character,Double&gt; o1, Map.Entry&lt;Character,Double&gt; o2) { return o2.getValue().compareTo(o1.getValue()); } }); } public Map&lt;Character, String&gt; toFano() { double[] pros = new double[probArrayList.size()]; // 概率值数组 int i = 0; for(Map.Entry&lt;Character,Double&gt; entry:probArrayList) { pros[i] = entry.getValue(); // 从概率List中取出概率值放入数组 i++; } String[] codon = getGroup(pros, 0, pros.length - 1); // 调用方法，得到编码 int j = 0; for(Map.Entry&lt;Character,Double&gt; entry:probArrayList) { codonMap.put(entry.getKey(), codon[j]); // 将各码字放进codonMap cLengthMap.put(entry.getKey(), codon[j].length()); // 将各长放进cLengthMap aCLength += pros[j] * codon[j].length(); // 计算平均码长 j++; } return codonMap; } private String[] getGroup(double[] pros, int i, int j) { // 输入概率值数组，得到其中索引为i到j编码结果 int middle = 0; // 分组点索引 String[] codens = new String[pros.length]; // 存储编码结果 for (int k = 0; k &lt; pros.length; k++) { codens[k] = &quot;&quot;; // 编码结果初始化 } if(i &lt; j) { // 索引i小于索引 double sum = 2; // 用以比较的中间量（初始值随便取一个大于1的即可） for(int k = i; k &lt;= j; k++) { // 循环找到中位值的索引 if(Math.abs(sumGroup(pros,i,k) - sumGroup(pros, k+1, j)) &lt; sum) { // 如过两部分累加和之差小于中间量 sum = Math.abs(sumGroup(pros, i, k) - sumGroup(pros, k+1, j)); // 将两部分累加和之差作为中间量 middle = k; // 更新分组点的索引 } } String[] codens_1 = getGroup(pros, i, middle); // 递归获得前半部分编码 String[] codens_2 = getGroup(pros, middle+1, j); // 递归获得后半部分编码 for(int k = i; k &lt;= middle; k++) { // 对前半部分编码 codens[k] = &quot;0&quot; + codens_1[k]; } for(int k = middle + 1; k &lt;= j ; k++) { // 对后半部分编码 codens[k] = &quot;1&quot; + codens_2[k]; } } return codens; } private double sumGroup(double[] pros,int i, int j) { // 累加概率数组索引i到j的值 double sum = 0.0; for(int k = i; k &lt;= j; k++) { sum += pros[k]; } return sum; } public Map&lt;Character, Integer&gt; getCLengthMap() { return cLengthMap; } public List&lt;Map.Entry&lt;Character, Double&gt;&gt; getProbArrayList() { return probArrayList; } public double getACLength() { BigDecimal bg = new BigDecimal(aCLength); aCLength = bg.setScale(3, BigDecimal.ROUND_HALF_UP).doubleValue(); return aCLength; }} 哈夫曼编码APP中，使用Java语言编写的哈夫曼编码过程如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150package top.binweber.shannon;import java.math.BigDecimal;import java.util.ArrayDeque;import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;import java.util.List;import java.util.Map;import java.util.Queue;import java.util.TreeMap;class TreeNode { // 树节点类 public double prob; // 结点概率值 public String codeword = &quot;&quot;; // 码字 public TreeNode left; // 左子节点 public TreeNode right; // 右子节点 public TreeNode(double data){ this.prob = data; }}public class HuffmanEncode { private static Map&lt;Character, Integer&gt; cLengthMap; // 码长Map private static Map&lt;Character, String&gt; codonMap; // 码字Map private static List&lt;Map.Entry&lt;Character, Double&gt;&gt; probArrayList; // 概率List private static double aCLength; // 平均码长 private static List&lt;String&gt; codonList; // 码字List public HuffmanEncode(Map&lt;Character, Double&gt; probMap) { // 初始化 aCLength = 0.0; cLengthMap = new TreeMap&lt;&gt;(); codonMap = new TreeMap&lt;&gt;(); codonList = new ArrayList&lt;&gt;(); probArrayList = new ArrayList&lt;&gt;(probMap.entrySet()); // 将概率Map按概率值递减排列放到概率List Collections.sort(probArrayList, new Comparator&lt;Map.Entry&lt;Character, Double&gt;&gt;() { public int compare(Map.Entry&lt;Character, Double&gt; o1, Map.Entry&lt;Character, Double&gt; o2) { return o2.getValue().compareTo(o1.getValue()); } }); } public Map&lt;Character, String&gt; toHuffman() { // 进行编码 List&lt;TreeNode&gt; list = new ArrayList&lt;&gt;(); // 用于存放节点的List int i = 0; for(Map.Entry&lt;Character,Double&gt; entry:probArrayList) { double prob = entry.getValue(); // 取出概率值 TreeNode root = new TreeNode(prob); // 创建新节点 list.add(i, root); // 将新节点加入list i ++; } createTree(list); // 由list中的节点创建哈夫曼树 TreeNode root = (TreeNode)list.get(0); // 得到根节点 list.clear(); // 清空list printTree(root); // 获得编码结果 int j = 0; for(Map.Entry&lt;Character,Double&gt; entry:probArrayList) { codonMap.put(entry.getKey(), codonList.get(j)); cLengthMap.put(entry.getKey(), codonList.get(j).length()); aCLength += entry.getValue() * codonList.get(j).length(); j ++; } return codonMap; } private void createTree(List&lt;TreeNode&gt; list){ // 创建一棵哈夫曼树 double prob1 = (list.get(list.size() - 1)).prob; // 取出倒数第一个节点的概率 double prob2 = (list.get(list.size() - 2)).prob; // 取出倒数第二个节点的概率 double prob_sum = prob1 + prob2; // 两个最小的概率值相加 TreeNode root = new TreeNode(prob_sum); // 用相加的概率创建一个新节点 root.left = (list.get(list.size() - 1)); // 将倒数第一个作为左子节点 root.right = (list.get(list.size() - 2)); // 将倒数第二个作为右子节点 list.remove(list.size() - 1); // 已经list中加上树上的节点删除 list.remove(list.size() - 1); sortList(list, root); // 将新的节点加到list里，并排序 if(list.size() &gt; 1){ createTree(list); // 只要list里面还有节点，就递归创建树 } } private void sortList(List&lt;TreeNode&gt; list,TreeNode root){ // 将某个节点插入list，排序 if(list.size() == 0){ list.add(root); // list为空时直接插入 }else{ int i; for(i = 0; i &lt; list.size(); i++){ if(list.get(i).prob &lt;= root.prob){ // 循环比较大小 list.add(i, root); // 插入 break; } } if(i == list.size()) { // 到最后，直接插入 list.add(i, root); } } } private void printTree(TreeNode root){ // 使用广度优先查找算法，层次遍历哈夫曼树各节点 Queue&lt;TreeNode&gt; queue = new ArrayDeque&lt;&gt;(); // 节点队列 queue.offer(root); // 将根节点入队 while(!queue.isEmpty()) { // 队列非空，不断执行 root = queue.poll(); // 将一个节点出队 if (root.left != null) { // 该节点的左子节点非空 root.left.codeword = root.codeword + &quot;0&quot;; // 左子节点码字加0 queue.offer((root.left)); // 左子节点入队 } if (root.right != null) { // 该节点的右子节点非空 root.right.codeword = root.codeword + &quot;1&quot;; // 右子节点码字加1 queue.offer((root.right)); // 右子节点入队 } if (root.left == null &amp;&amp; root.right == null) { // 节点为叶子结点 codonList.add(root.codeword); // 将节点的码字存储起来 } } } public Map&lt;Character, Integer&gt; getCLengthMap() { return cLengthMap; } public List&lt;Map.Entry&lt;Character, Double&gt;&gt; getProbArrayList() { return probArrayList; } public double getACLength() { BigDecimal bg = new BigDecimal(aCLength); aCLength = bg.setScale(3, BigDecimal.ROUND_HALF_UP).doubleValue(); return aCLength; }} 判断唯一可译码APP中，使用Java语言编写的判断可译码的程序如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package top.binweber.shannon;import java.util.ArrayList;public class UniqueDecode { private boolean result; // 判断结果 private ArrayList&lt;String&gt; codonList; // 码字集 private ArrayList&lt;String&gt; suffixList; // 后缀集 public UniqueDecode(ArrayList&lt;String&gt; codonList) { // 初始化 result = true; // 默认唯一可译 this.codonList = codonList; suffixList = new ArrayList&lt;&gt;(); } public boolean compare() { // 比较判断 String suffix = null; cp: for(int i = 0; i &lt; codonList.size(); i++) { for(int j = i + 1; j &lt; codonList.size(); j++) { String codon1 = codonList.get(i); // 取出两个码字 String codon2 = codonList.get(j); suffix = compareCodon(codon1,codon2); // 获得尾随后缀 if(!result) { // 已判断出非唯一可译 break cp; // 跳出循环 } if(suffix != null &amp;&amp; !suffixList.contains(suffix)) { // 尾随后缀不为null，且尾随后缀集b不包含该后缀 suffixList.add(suffix); // 收集获得的尾随后缀 } } } compareList(codonList, suffixList, suffix); // 比较码字集与尾随后缀集 return result; } public ArrayList&lt;String&gt; getSuffixList() { return suffixList; } private String compareCodon(String codon1, String codon2) { // 比较两个码字 String suffix = null; // 尾随后缀初始化为null if(codon1.equals(codon2)) { // 如果有两个码字相同，奇异码 result = false; // 非唯一可议 } if(result) { if(codon1.startsWith(codon2)) // 如果码字1以码字2作为开头 suffix = codon1.substring(codon2.length(), codon1.length()); // 从码字2中取出尾随后缀 if(codon2.startsWith(codon1)) // 如果码字2以码字1作为开头 suffix = codon2.substring(codon1.length(), codon2.length()); // 从码字1中取出尾随后缀 } return suffix; // 返回尾随后缀 } private void compareList(ArrayList&lt;String&gt; a, ArrayList&lt;String&gt; b, String suffix) { // 比较码字集和尾随后缀集 boolean flag = false; // 继续递归判断标志 String codon1,codon2; cp: for(int i = 0; i &lt; a.size(); i++) { // 循环 for(int j = 0; j &lt; b.size(); j++) { codon1 = a.get(i); // 从a中取出一个码字1 codon2 = b.get(j); // 从b中取出一个尾随后缀 suffix = compareCodon(codon1,codon2); // 比较码字1、2，获得尾随后缀 if(!result) { // 已经判断出是非唯一可译 break cp; // 跳出循环 } if(suffix != null &amp;&amp; !b.contains(suffix)) { // 尾随后缀不为null，且尾随后缀集b中不包含该后缀 b.add(suffix); // 尾随后缀加入b中 flag = true; // 需要继续判断 break cp; // 跳出循环 } // 当前尾随后缀为null或得到的是重复的尾随后缀则继续循环 } } if(flag) { compareList(a, b, suffix); // 继续递归进行判断 } }} APP“香农”APP运行的部分截图如下： 参考资料 APP下载-网盘 APP源程序-Github Java实现三种信源编码-iteye 唯一可译码判决准则-csdn 更新历史： 2018.6.24 完成初稿","link":"/2018/06/24/EE/android_shannon/"},{"title":"单片机原理(3)：中断、定时&#x2F;计数、串行通信","text":"中断（Interrupt）是指在计算机运行过程中，出现某些意外情况需主机干预时，机器能自动停止正在运行的程序并转入处理新情况的程序，处理完毕后又返回原被暂停的程序继续运行。 定时器/计数器（Timer/Counter）在实时控制系统中，实现对于外界事件的定时延时及计数功能。 串行通信（Serial Communicate）是计算机与外界交换信息的一种基本通信方式。 中断系统中断过程中，请求产生中断的事件称为中断源，中断源向CPU提出的请求为中断请求（Interrupt Requst，IRQ），CPU通过上下文切换保存好当前的工作状态后，转而去处理中断请求，也就是产生中断响应。直到处理完中断请求事件，才返回原来的工作状态，继续工作。整个过程如下图所示： 51单片机中断系统的结构图如下： 中断源51单片机中有5个中断源，如下表所示： 中断号 优先级 中断源 中断入口地址 0 1(最高级) 外部中断0 0003H 1 2 定时器0 000BH 2 3 外部中断1 0013H 3 4 定时器1 001BH 4 5(最低级) 串口中断 0023H 每个中断源都分配了对应的中断号及中断服务入口地址，在这个中断入口地址里，存放着跳转到相应中断服务程序的跳转指令。当多个中断源同时向CPU提出中断请求时，CPU将根据中断源的优先级来依次响应中断。 相关寄存器SFR中与中断有关的寄存器有： IE**IE(Interrupt Enable)**，中断允许寄存器，可位寻址，其各位定义如下： 位地址 | AFH | AEH | ADH | ACH | ABH | AAH | A9H | A8H——|——|——|——|——|——|——|——|——|——定义 | EA | - | (ET2) | ES | ET1 | EX1 | ET0 | EX0 **EA(Enable All)**：CPU中断总控制位，EA=1，CPU对所有中断开放，EA=0，CPU禁止一切中断响应。 **ES(Enable Serial)**：串口中断允许控制位，ES=1，允许串行口接受、发送中断。 **ET0/ET1(Enable Timer)**：定时/计数器0/1中断允许控制位，ET0/ET1=1，允许T0/T1中断。52系列单片机里还有ET2。 **EX0/EX1(Enable Exterior)**：外部中断INT0/INT1中断允许控制位，EX0/EX1=1，允许外部中断INT0/INT1中断。 IP**IP(Interrupt Priority)**，中断优先级寄存器，可位寻址，其各位定义如下。未设置时或复位后，IP各位均为””则按照系统默认的优先级 位地址 | BFH | BEH | BDH | BCH | BBH | BAH | B9H | B8H——|——|——|——|——|——|——|——|——|——定义 | - | - | (PT2) | PS | PT1 | PX1 | PT0 | PX0 **PS(Priority Serial)**：串行口优先级设定位，PS=1，串行口为高优先级。 **PT0/PT1(Priority Timer)**：定时/计数器0/1优先级设定位，PT0/PT1=1，定时/计数器0/1为高优先级。 **PX0/PX1(Priority Exterior)**：外部中断INT0/INT1优先级设定位，PX0/PX1=1，外部中断INT0/INT1为高优先级。 TCON**TCON(Timer Control)**，定时/计数器控制寄存器，可位寻址，其各位定义如下： 位地址 | 8FH | 8EH | 8DH | 8CH | 8BH | 8AH | 89H | 88H——|——|——|——|——|——|——|——|——|——定义 | TF1 | TR1 | TF0 | TR0 | IE1 | IT1 | IE0 | IT0 **TF0/TF1(Timer Flag)**：定时/计数器0/1溢出标志位，当定时/计数器计满溢出时，由硬件自动置“1”，并申请中断；在进入中断服务程序后，又由硬件自动置“0”。 **TR0/TR1(Timer Run)**：定时/计数器0/1启停控制位，TR0/TR1=1，启动定时/计数器。 **IE0/IE1(Interrupt Exterior)**：外部中断INT0/INT1中断请求标志位，外部中断源有请求时，对应的标志位IE0/IE1由硬件置“1”，当CPU响应该中断后，又由硬件自动置“0”。 **IT0/IT1(Interrupt Touch)**：外部中断INT0/INT1的触发方式选择位，IT0/IT1=0，对应外部中断设置为低电平触发方式，IT0/IT1=1，对应外部中断设置为边沿触发方式。 中断处理过程51单片机在CPU的每个机器周期的S5 P2期间，将自动查询TCON中各个中断申请标志，若查询到某个中断标志位被置位，将启动中断机制。 中断源发出中断请求后。要使CPU能够响应中断，IE中的中断总允许位EA及对应的中断允许位（ES/ET/EX）都需要置为“1”。 处理外部中断时，若外部中断设为低电平触发方式，则CPU在每个查询中断申请标志时也将对INTi引脚进行采样，测得INTi=0，则认为有中断申请，随即将IEi标志位置位，否则测得INTi=1，则认为无中断请求，而清除IEi标志位。所以施加在INTi引脚上的低电平持续时间应该大于一个机器周期，且小于中断服务程序得执行时间。 若外部中断设为边沿触发方式，则CPU在每个查询中断申请标志时将对INTi引脚进行采样，若在连续两个机器周期采样到先高后低的电平变化，则认为有中断申请，随即将IEi标志位置位，否则测得INTi=1，则认为无中断请求，而清除IEi标志位。所以，为了保证CPU在两个机器周期内能够检测到由高到低跳变得电平，输入的高低电平持续时间至少要保持12个振荡周期（即一个机器周期）时间。 在中断处理过程中，如果正在执行同级或高优先级的中断服务程序，或是正在执行的指令还没完成，则中断响应会受到阻断。51单片机的中断响应时间最短为3个机器周期，其他情况的中断响应时间一般是3~8个周期。 在CPU响应中断后，应该撤除该中断请求，否则会再次产生中断，进入死循环。 中断程序的编写由中断的处理过程可知，在编写中断管理与控制程序时应该考虑一下几个方面： CPU开中断和关中断 某个中断源中断请求的允许或屏蔽 各中断源优先级别的设定 外部中断请求的触发方式 中断程序基本编写格式如下： 汇编： 1234567891011121314151617181920212223242526272829303132333435 ;中断入口设置 ORG 0000H ;起始地址 LJMP MAIN ;跳转到主程序 ORG 0003H ;中断入口地址1 LJMP INT1 ;跳转到外部中断INT0服务程序 ORG 000BH ;中断入口地址2 LJMP INT2 ;跳转到定时器0中断服务程序，没有则不写或写成“RETI”，下同 ORG 0013H ;中断入口地址3 LJMP INT3 ;跳转到外部中断INT1服务程序 ORG 001BH ;中断入口地址4 LJMP INT4 ;跳转到定时器1中断服务程序 ORG 0023H ;中断入口地址5 LJMP INT5 ;跳转到串口中断服务程序 ;主程序 ORG 0030H ;起始地址MAIN: SETB IT0 ;IT0=1，边沿触发 SETB EA ;EA=1，开启总中断 SETB EX0 ;EX0=1，允许外部中断INT0 ……LOOP: NOP LJMP LOOP ;死循环 ;中断服务程序 ORG 0100H ;起始地址INT1: PUSH ACC ;保存原来状态，下同 PUSH PSW ; ……INT2: …… …… POP PSW ;恢复原始状态，下同 POP ACC ; RETI ;中断返回 END ;结束 C语言： 12345678910111213141516#include &quot;reg51.h&quot;// 主程序void main() { IT0 = 1；//注释同汇编 EX0 = 1； EA = 1； while (1) { …… }}// 中断处理程序void int0() interrupt 0 { // 外部中断INT0服务程序 ……} 定时/计数器51单片机内部有两个16位可编程的定时/计数器0和1，分别用T0及T1表示。它们的工作方式、定时时间、量程、启动方式、等均可通过程序来设置和改变。 计数功能用于统计从T0（P3.4）、T1（P3.5）引脚输入的脉冲负跳变数量，每输入一个脉冲负跳变，计数器就加1。负跳变指的是一个机器周期采样为高电平，后一个机器周期采样为低电平。 定时功能是单片机通过对内部机器脉冲信号计数实现的，计数值乘以机器周期即是相应的时间。如单片机采用12MHz晶振，机器内部脉冲频率为1MHz，机器周期为1us，计数1000次，即1ms时间。 当计数值溢出后，定时计数器给出中断请求，进而使CPU去处理中断事件。 51单片机定时/计数器结构如下： 相关寄存器除前面介绍过的中断相关寄存器外，SFR中与定时/计数器有关的寄存器有： TMODTMOD(Timer Mode)，定时/计数器模式控制寄存器，不可位寻址，其各位定义如下。其中TMOD的高半字节D4D7用来控制定时/计数器1，低半字节D0D3用来控制定时/计数器0。 位号 | D7H | D6H | D5H | D4H | D3H | D2H | D1H | D0H——|——|——|——|——|——|——|——|——|——定义 | GATE | C\\T | M1 | M0 | GATE | C\\T | M1 | M0 GATE：门控制位，用来控制定时/计数器的启动方式。GATE=1，由外部中断引脚INT0/INT1来启动定时器T0/T1， 当INT0/INT1引脚为高电平且TR0/TR1置位，启动定时器T0/T1；GATE=0，仅由TR0/TR1置位而启动定时器T0/T1。 C\\T：功能选择位，C\\T=0，为定时模式，计数脉冲由内部提供，计数周期等于机器周期；C\\T=1，为计数模式，计数脉冲由外部引脚T0或T1引入。 M0/M1：工作方式控制位，用于设置定时/计数器的工作方式，如下表： M0 M1 工作方式 功能 0 0 方式0 13位计数器 1 0 方式1 16位计数器 0 1 方式2 8位重装计数器 1 1 方式3 定时器0分为两个独立8位计数器 T0、T1**T0、T1(Timer)**，两个定时/计时器的初始赋值寄存器，不可位寻址，用于存放定时/计数的初始值。它们是两个16位寄存器，均可分为两个独立的8位寄存器，高8位记为TH，即TH0、TH1，低8位记为TL，即TL0、TL1。使用定时/计数器时，当外部或系统时钟振荡器输入一个脉冲时，对应寄存器的值便自动加1。 编程时需要注意，16位计数初始值要分两次写入对应初始值寄存器。 定时/计数过程51单片机定时/计数器的工作模式、工作方式、计数初始值及启停操作均需要在使用前进行初始化。 首先是通过TMOD中的GATE位来设置定时/计数器T0/T1的启动操作方式，工作在定时还是计数模式则是通过C/T位来设置。其次就是选择它们的工作方式。 工作方式0、1、3为非自动重装方式，在初始化程序和对应中断服务程序中均需要对初始数据寄存器THi、TLi装载。方式0是一个13位定时/计数器，只用了16位寄存器的高8位THi和TLi的低5位0~4位，TLi高3位未用，装入数据时需要注意。方式1为16位寄存器。 方式3只适用于定时/计数器T0，一般在定时/计时器T1作串行口波特率发生器时，才会选择这个工作模式。此时T0拆分为两个独立8位计数寄存器TH0和TL0，其中TL0下为8位定时/计数器，操作方式同方式0、1；TH0只做简单的内部定时功能，它借用定时/计时器1的控制位TR1和溢出标志位TF1，占用T1的中断资源，启动和停止也仅受TR1控制，此时T1仅可用在不需要中断的场合。 方式2为8位重装寄方式，仅由TLi作为工作寄存器，THi的值一直保持不变。TLi溢出时，THi的值作为装载值由CPU自动装入TLi，自动完成计数值初始化。所以此种方式下需要初始化时在THi和TLi中放入相同的计数值。 T0/T1寄存器中的初值X与定时/计数器的工作方式、工作模式有关。 在工作方式0下，为13位寄存器，也就是说最大的计数值M=$2^{13}$=8192，超过这个数字就会溢出，触发中断。工作方式为1下，为16位寄存器，M=$2^{16}$=65536。工作方式为2下，为8位寄存器，M=$2^{8}$=256。工作方式为3下，定时/计数器0分为高8位和低8位两个独立8位计数器，THi、TLi的M均为$2^{8}$=256。 不管是进行定时还是计数模式，其具体实现都是归结于计数。 计数模式下，是对外部脉冲数计数，初值X = M - 计数值。定时模式下，是对机器周期进行计数，**初值X = M - 计数值 = M - $\\frac{定时时间T_c}{机器周期T_p}$ = M - $\\frac{定时时间T_c × 晶振频率}{12}$**。 最后，中断中的相关寄存器也需要进行设置，设置TRi=1，来启动定时/计数器。 定时/计数程序编写通过上面对定时/计数过程的分析，可知编写想关功能程序时，要先进行初始化，再根据公式计算出定时初值并装载。 定时/计数程序基本编写格式如下： 汇编： 12345678910111213141516171819202122232425262728293031323334 ;中断入口设置 ORG 0000H ;起始地址 LJMP MAIN ;跳转到主程序 ORG 000BH ;中断入口地址2 LJMP INT2 ;跳转到定时器0中断服务程序 ORG 001BH ;中断入口地址4 LJMP INT4 ;跳转到定时器1中断服务程序 ;主程序 ORG 0030H ;起始地址MAIN: MOV TMOD, #xxH ;设置工作模式，方式 MOV TH0, #xxH ;计数初值，高八位 MOV TL0, #xxH ;低八位 SETB EA ;EA=1 ，开启总中断 SETB ET0 ;ET0=1，允许定时/计数器中断 SETB TR0 ;TR0=1，启动定时/计数器 ……LOOP: NOP LJMP LOOP ;死循环 ;中断服务程序 ORG 0100H ;起始地址INT2: PUSH ACC ;保存原来状态，下同 PUSH PSW ; MOV TH0, #xxH ;重新赋计数初值（非重装方式下） MOV TL0, #xxH ……INT4: …… …… POP PSW ;恢复原始状态，下同 POP ACC ; RETI ;中断返回 END ;结束 C语言： 123456789101112131415161718192021#include &quot;reg51.h&quot;// 主程序void main() { TMOD = 0xxx; //注释同汇编 TH0 = 0xxx; TL0 = 0xxx; EA = 1; ET0 = 1; TR0 = 1; while (1) { …… }}// 中断处理程序void int2() interrupt 1{ // 定时器0中断服务程序 TH0 = 0xxx; TL0 = 0xxx; ……} 串口通信通信相关概念计算机通信方式可以分为并行（Parallel）与串行（Serial）通信两大类，它将计算机技术和通信技术的相结合，完成计算机与外部设备或计算机与计算机之间的信息交换： 并行通信：数据的各个二进制位在不同的数据线上同时传输。这样传输速度快，效率高，但所需的数据线多，成本高，抗干扰能力较差，适用于近距离传输。 串行通信：将数据拆分成多个二进制位，逐一的在同一条数据线上输出。虽然这样传输速度较慢，效率较低，但所需的数据线少、硬件电路简单、抗干扰能力强，且适用于远距离数据传输。 串行通信又分为同步（Synchronous）通信和异步（Asynchronous）通信两种方式： 同步通信：串行连续地传输数据，待发送的若干个字符数据构成一个数据块，在该数据块前部添加1~2个同步字符，在数据块的末尾添加校验信息，以此种方式构成数据帧，以数据帧为单位进行串行通信。 异步通信：每个字符数据被封装成帧，之后以帧的形式发送。每一帧由四部分构成，分别是起始位、数据位、校验位和停止位。起始位是数据开始传送的标志，用逻辑0表示；数据位紧跟起始位，通常是5~8位二进制位；校验位用于校验数据位是否发送正确，可以选择奇校验、偶校验或者不使用校验位。帧和帧之间可以连续，或者加入任意的空闲位，空闲位用逻辑1表示。 串行数据的传输制式可分为单工（Simplex）、半双工（Half Duplex）、全双工（Full Duplex）。 单工：数据传输仅能沿一个方向，不能实现反向传输。 半双工：数据传输可以沿两个方向，但需要分时进行。 全双工：数据可以同时进行双向传输。 串行通信的错误校验方式有奇偶校验（Parity Check）、代码和校验、循环冗余校验（Cyclical Redundancy Check，CRC）三种： 奇偶校验：在发送数据时，数据位尾随的1位为奇偶校验位（1或0）。奇校验时，数据中“1”的个数与校验位“1”的个数之和应为奇数；偶校验时，数据中“1”的个数与校验位“1”的个数之和应为偶数。接收字符时，对“1”的个数进行校验，若发现不一致，则说明传输数据过程中出现了差错。 代码和校验：发送方将所发数据块求和（或各字节异或），产生一个字节的校验字符（校验和）附加到数据块末尾。接收方接收数据同时对数据块（除校验字节外）求和（或各字节异或），将所得的结果与发送方的“校验和”进行比较，相符则无差错，否则即认为传送过程中出现了差错。 循环冗余校验：通过某种数学运算实现有效信息与校验位之间的循环校验，常用于对磁盘信息的传输、存储区的完整性校验等。这种校验方法纠错能力强，广泛应用于同步通信中。 波特率（Baud Rate）是串口通信时每秒钟传输二进制代码的位数，单位是位／秒（bps）。 51单片机的串行接口是一个全双工通信接口，即能够同时进行数据发送和接收。它可作通用异步收发传输器（Universal Asynchronous Receiver/Transmitter，UART）用，也可以作同步移位寄存器。其结构如下： 相关寄存器除前面介绍过的中断及定时/计数器相关寄存器外，SFR中与串口通信相关的寄存器有： SCON**SCON(Serial Control)**，串行口控制寄存器，可位寻址，其各位定义如下： 位地址 | 9FH | 9EH | 9DH | 9CH | 9BH | 9AH | 99H | 98H——|——|——|——|——|——|——|——|——|——定义 | SM0 | SM1 | SM2 | REN | TB8 | RB8 | TI | RI **SM0和SM1(Serial Mode)**：串行口工作方式控制位，用于设置串行口工作方式，如下表： SM0 SM1 工作方式 功能 波特率 0 0 方式0 8位同步移位寄存器 晶振频率/12 0 1 方式1 10位UART 可变 1 0 方式2 11位UART 晶振频率/64 或 /32 1 1 方式3 11位UART 可变 SM2：多机通讯控制位，主要在以上工作方式为2或3下使用，工作方式0或1下都应该设为“0”状态。 **REN(Receive Enable)**：串行允许接收位，REN=1，允许接受数据。 **TB8(Transfer Bit 8)**：工作方式为2或3下存放发送数据的第9位，由软件置”0”或”1”。 **RB8(Receive Bit 8)**：工作方式为2或3下存放接受数据的第9位。方式0下不使用；方式1下，当SM2=0时，用于存放接收到的停止位。 TI(Transfer Interrupt)：发送中断标志位，用于指示一帧数据是否发送完成。使用前必须软件复位为“0”，发送完一帧数据后硬件将自动置”1”。 RI(Receive Interrupt)：接受中断标志位，用于指示一帧数据是否接受完成。接受完一帧数据后硬件将自动置”1”，之后必须软件复位为“0”。 SBUF**SBUF(Serial Data Buffer)**，串行数据缓冲器，不可位寻址。串行口两个SBUF，分别为发送寄存器和接收寄存器，它们在物理结构上是完全独立的，在SFR中的字节地址都是99H。这个重叠的地址靠读/写指令区分：串行发送时，CPU向SBUF写入数据，此时99H表示发送SBUF；串行接收时，CPU从SBUF读出数据，此时99H表示接收SBUF。 PCON**PCON(Power Control)**，电源控制寄存器，不可位寻址，其中只有一位与串口设置有关，其余都用于电源控制。其各位定义如下： 位号 | D7H | D6H | D5H | D4H | D3H | D2H | D1H | D0H——|——|——|——|——|——|——|——|——|——定义 | SMOD | - | - | - | GF1 | GF0 | PD | IDL **SMOD(Serial Mode)**：波特率选择位。工作方式1、2、3下串行通信波特率与$2^{SMOD}$成正比。也就是SMOD=1，通信波特率可提高一倍。 GF0/GF1(General Flag)：通用标志位，用户可自由使用 PD(Power Down)：掉电控制位。PD=0，单片机正常工作；PD=1，进入掉电模式，外部晶振停振，CPU、定时器、串行口全部停止工作，只有外部中断工作。在该模式下，只有硬件复位和上电能够唤醒单片机。 IDL(Idle)：空闲控制位。IDL=0，单片机正常工作；IDL=1，单片机进入空闲模式，除CPU不工作外，其余仍继续工作，在空闲模式下可由任一个中断或硬件复位唤醒。 串行通信过程一般情况下，当CPU允许接收（REN=1）且接收中断标志RI复位时，就启动一次接收过程。外界数据通过引脚RXD（P3.0）串行输入，数据最低位首先输入一个大小为9位移位寄存器，当一帧数据接收完毕后再并行送入接收SBUF中，同时RI也置“1”。当用软件读取完数据并复位RI后，才进行下一次操作。 当发送中断标志TI复位后，CPU便开始执行一条写SBUF指令，启动一次发送过程，发送控制器同时启动，并开始发送数据。发送的数据通过引脚TXD（P3.1）输出，首先输出最低位。当一帧数据发送完毕即发送SBUF为空时，CPU自动将TI值“1”。使用软件将TI复位，才执行下一个发送过程。 51单片机串行口有4种工作方式，通过SCON寄存器中的SM0、SM1口选择。 方式0下，串行口为同步移位寄存器的输入输出方式，主要用于和外部同步移位寄存器外接，以达到扩展一个并行输入或输出口的目的。这种方式下，数据从RXD端串行输入或输出，同步移位信号从TXD输出，波特率固定为晶振频率的1/12。TXD引脚每输出一位同步移位脉冲，数据就由RXD引脚输入或输出一个二进制位，发送和接收均为8位数据，低位在先，高位在后，没有起始位和停止位。此时，SM2、RB8、TB8均设为“0”，不起作用。方式0工作时序图如下： 方式1下，串行口为10位异步通信方式，即1个起始位、8个有效数据位和1个停止位，波特率由定时/计数器T1溢出率及PCON中的SMOD位共同决定。进行发送操作时，发送电路会自动在8位发送数据前后分别加上一位起始位和一位停止位。接收操作时，接收器以所选择波特率的16倍速率采样RXD引脚电平，检测负跳变时，则说明起始位有效，将其移入输入移位寄存器，并开始接收这一帧信息的其余位。接收过程中，数据从输入移位寄存器右边移入，起始位移至输入移位寄存器最左边时，控制电路进行最后一次移位，随后将接收到的9位数据的前8位数据装入接收SBUF，第9位（停止位）进入RB8，并置RI=1，向CPU申请中断。如果上述条件不满足，数据则会被舍弃。此时，SM2也要设为“0”。方式1工作时序图如下： 方式2及方式3下，串行口都为11为异步收发方式，即1个起始位、8个有效数据位、1个附加数据位和1个停止位，其两者的差异在于通信波特率的不同：方式2的波特率取决于晶振频率和PCON中的SMOD位，固定为晶振频率的1/64或1/32；方式3的波特率由定时器T1的溢出率和SMOD位共同决定，故其波特率是可调的。比方式1多出来的一个附加位发送时为SCON中的TB8，接收时为RB8，由用户安排，可作奇偶校验位，也可作其他控制位。这两种方式很适合主从式的通信结构，在多机通讯时，主机的SM2位设为“0”，而从机的SM2位设为“1”，以便之间相互识别。 方式0下：$$波特率 = \\frac{晶振频率 f_{osc}}{12}$$ 方式2下：$$波特率 = \\frac{2^{SMOD}}{64} * f_{osc}$$ 方式1下及方式3下，波特率都由定时/计数器T1溢出率及PCON中的SMOD位共同决定。T1的溢出率又取决于计数速率和计数值。定时模式下，计数速率为晶振频率的1/12；计数模式下，计数速率取决于外部输入时钟频率，但不可超过晶振频率的1/24。作波特率发生器时，T1通常设置为定时模式，工作在工作方式2下，作8位重装寄存器，此时波特率计算公式为：$$ {T1溢出周期 = 定时时间 = \\frac{12}{f_{osc}} * (256 - 计数初值X) }$$$$ T1溢出率 = \\frac{1}{T1溢出周期} $$$$ {波特率 = \\frac{2^{SMOD}}{32} * T1溢出率 } = \\frac{2^{SMOD} * f_{osc}}{384 * (256 - X)}$$ 则T1的计数初值，也就是装载值为：$$ {计数初值X = 256 - \\frac{2^{SMOD} * f_{osc}}{384 * 波特率}}$$ 串行通信程序编写由串行通信的基本过程可知，在使用串行口之前需要对串行口进行初始化，确定使用的工作方式并进行相关配置。 串行通信程序基本编写格式如下： 汇编： 123456789101112131415161718192021222324252627282930313233343536 ;中断入口设置 ORG 0000H ;起始地址 LJMP MAIN ;跳转到主程序 ORG 0023H ;中断入口地址5 LJMP INT5 ;跳转到串口中断服务程序 ;主程序 ORG 0030H ;起始地址MAIN: MOV TMOD, #20H ;设置定时/计数器T1工作模式为定时，工作方式2 MOV TH1, #F4H ;计数初值，高八位，波特率2400 MOV TL1, #F4H ;低八位 MOV SCON, #90H ;设置串行口工作方式2，单机，允许接收 MOV PCON, #00H ;波特率不加倍 SETB EA ;EA=1 ，开启总中断 SETB ES ;ES=1,开启串口中断 SETB TR1 ;TR1=1，启动定时/计数器 ……SEND: MOV A, #xxH ;待发送数据放入A，即刻生成了偶校验位放在P中 MOV C, P ;将偶校验位放入位寄存器C CPL C ;奇校验则把P取反 MOV TB8, C ;将校验位放入TB8中 MOV SUBF, A ;发送数据 JBC TI, xxx ;发送完一帧数据后清除TI并进行下一步LOOP: NOP LJMP LOOP ;死循环 ;中断服务程序 ORG 0100H ;起始地址INT5: MOV A, SBUF ;取出接收到的数据 JB RB8, ERR ;进行校验，不同则跳转到错误处理 CLR RI ;复位，准备下次接收 …… RETI ;中断返回 ERR: …… …… END ;结束 C语言： 123456789101112131415161718192021222324252627282930313233343536373839#include &quot;reg51.h&quot;// 主程序void main() { TMOD = 0x20H; //注释同汇编 TH0 = 0xF4; TL0 = 0xF4; SCON = 0x90; PCON = 0x00; EA = 1; ES = 1; TR1 = 1; while (1) { send(data); }}//发送数据void send(unsigned char data) { ACC = data; TB8 = P; SBUF = data; while(!TI == 0){ …… TI = 0； }}// 中断处理程序，接收数据void int5() interrupt 5{ // 定时器0中断服务程序 ACC = SBUF; RI = 0; if (RB8 == p){ …… }else{ …… }} 更新历史： 2017.11.27 完成初稿","link":"/2017/11/24/EE/msc51_3/"},{"title":"AlexNet的实现与应用","text":"详解AlexNet，用TensorFlow实现该架构并用它来完成Kaggle上的Dogs Vs Cats竞赛。 总体架构AlexNet的总体架构如下所示： 整个网络总共有八层，其中前五层为卷积层，后面三层为全连接层； 用了两个GPU（GTX 580，3G内存）进行训练，因此整个架构被平均分成了两部分，除了第二层与第三层是跨GPU进行连接，其他卷积层都只和各自的GPU内的前一层进行连接； 在第一、二个卷积层后进行现在并不常用的局部响应归一化（local response normalization，LRN），LRN的公式如下： 公式中的$k$、$n$、$\\alpha$、$\\beta$均为超参数，在验证集上实验得到$k = 2$、$n = 5$、$\\alpha = 0.00001$、$\\beta = 0.75$； 在LRN、第五层后面进行窗口大小和步幅不相等的最大池化，称为最大重叠池化； 批量大小为$128$，并使用ReLU作为激活函数， 用数据扩充和DropOut法来防止过拟合。 各层中的具体过程为： 输入的大小原为$224 \\times 224 \\times 3$，为方便后续处理将其调整为$227 \\times 227 \\times 3$； 第一层用$96$个大小为$11 \\times 11$的卷积核进行步幅为$4$的卷积，之后用大小为$3\\times3$的窗口进行步幅为$2$的最大重叠池化后，再进行尺度为$5\\times5$的LRN； 第二层用$256$个大小为$5 \\times 5$的卷积核进行步幅为$1$的卷积，之后和前一层一样进行重叠池化和LRN； 第三层用$384$个大小为$3 \\times 3$的卷积核连接到第二层的所有输出； 第四、第五层分别用$384$、$256$个大小为$3 \\times 3$的卷积核进行步幅为$1$的卷积,且在第五层后用大小为$3\\times3$的窗口进行步幅为$2$的最大重叠池化。 第六层的输入大小为$6 \\times 6 \\times 256$，与$4096$个大小为$6 \\times 6 \\times 256$的卷积核进行卷积，就得到包含$4096$个节点的全连接层。训练时，需要在该全连接层进行一次drop_prob为$0.5$的Dropout。 第七层的$4096$个节点与上一个全连接层进行全连接，训练时，也需要在该全连接层进行一次drop_prob为$0.5$的Dropout。 第八层进行第三次全连接，并输出最后的结果。 TensorFlow实现辅助方法卷积： 12345678910111213141516171819def conv(x, filter_height, filter_width, filters_num, stride_x, stride_y, name, padding='SAME', groups=1): # groups: 分成多个部分 input_channels = int(x.get_shape()[-1]) # 输入通道数 convolve = lambda i, k: tf.nn.conv2d(i, k, strides=[1, stride_x, stride_y, 1], padding=padding) # 卷积 with tf.variable_scope(name) as scope: weights = tf.get_variable('weights', shape=[filter_height, filter_width, input_channels/groups, filters_num]) bias = tf.get_variable('bias', shape=[filters_num]) if groups == 1: conv = convolve(x, weights) else: input_groups = tf.split(value=x, num_or_size_splits=groups, axis=3) # 切分 weight_groups = tf.split(value=weights, num_or_size_splits=groups, axis=3) output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)] # 分别卷积 conv = tf.concat(values=output_groups, axis=3) # 拼接 z = tf.reshape(tf.nn.bias_add(conv, bias), conv.get_shape().as_list()) relu = tf.nn.relu(z, name=scope.name) return relu 池化、LRN、Dropout： 1234567891011121314151617181920212223def max_pool(x, filter_height, filter_width, stride_x, stride_y, name, padding='SAME'): return tf.nn.max_pool(x, [1, filter_height, filter_width, 1], strides=[1, stride_x, stride_y, 1], padding=padding, name=name)# 局部响应归一化def lrn(x, radius, alpha, beta, name, bias=1.0): return tf.nn.lrn(x, depth_radius=radius, alpha=alpha, beta=beta, bias=bias, name=name) # bias对应k, radius对应n/2# 全连接def fc(x, num_in, num_out, name, relu=True): with tf.variable_scope(name) as scope: weights = tf.get_variable('weights', shape=[num_in, num_out]) bias = tf.get_variable('bias', shape=[num_out]) z = tf.nn.xw_plus_b(x, weights, bias, name=scope.name) if relu == True: act = tf.nn.relu(z) else: act = z return act# Dropoutdef dropout(x, keep_prob): return tf.nn.dropout(x, keep_prob) 用动态图测试上面的各方法： 1234567import tensorflow.contrib.eager as tfetfe.enable_eager_execution()x = tf.truncated_normal(shape=[1, 227, 227, 3], seed = 1)cnv = conv(x, 11, 11, 96, 4, 4, padding='VALID', name='conv')pool = max_pool(cnv, 3, 3, 2, 2, padding='VALID', name='pool')norm = lrn(pool, 2, 2e-05, 0.75, name='norm')norm.get_shape() 整个模型建立整个AlenNet： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class AlexNetModel(object): def __init__(self, num_classes=1000, keep_prob=0.5, skip_layer=[], weights_path='DEFAULT'): self.num_classes = num_classes self.keep_prob = keep_prob self.skip_layer = skip_layer if weights_path == 'DEFAULT': self.weights_path = 'bvlc_alexnet.npy' else: self.weights_path = weights_path def inference(self, x, training=False): # 模型 # conv1: CONV --&gt; POOL --&gt; LRN conv1 = conv(x, 11, 11, 96, 4, 4, padding='VALID', name='conv1') pool1 = max_pool(conv1, 3, 3, 2, 2, padding='VALID', name='pool1') norm1 = lrn(pool1, 2, 2e-05, 0.75, name='norm1') # conv2: CONV --&gt; POOL --&gt; LRN with 2 Groups conv2 = conv(norm1, 5, 5, 256, 1, 1, groups=2, name='conv2') pool2 = max_pool(conv2, 3, 3, 2, 2, padding='VALID', name='pool2') norm2 = lrn(pool2, 2, 2e-05, 0.75, name='norm2') # conv3: CONV conv3 = conv(norm2, 3, 3, 384, 1, 1, name='conv3') # conv4: CONV with 2 Groups conv4 = conv(conv3, 3, 3, 384, 1, 1, groups=2, name='conv4') # conv5: CONV --&gt; PooL with 2 Groups conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5') pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5') # fc6: Flatten --&gt; FC --&gt; Dropout flattened = tf.reshape(pool5, [-1, 6*6*256]) fc6 = fc(flattened, 6*6*256, 4096, name='fc6') if training: fc6 = dropout(fc6, self.keep_prob) # fc7: FC --&gt; Dropout fc7 = fc(fc6, 4096, 4096, name='fc7') if training: fc7 = dropout(fc7, self.keep_prob) # fc8: FC self.score = fc(fc7, 4096, self.num_classes, relu=False, name='fc8') return self.score def loss(self, batch_x, batch_y): # 损失 y_predict = self.inference(batch_x, training=True) self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_predict, labels=batch_y)) return self.loss def optimize(self, learning_rate, train_layers=[]): # 优化 var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in train_layers] return tf.train.AdamOptimizer(learing_rate).minimize(self.loss, var_list=var_list) def load_original_weights(self, session): # 导入训练好的权重 weights_dict = np.load(self.weights_path, encoding='bytes').item() for op_name in weights_dict: if op_name not in self.skip_layer: with tf.variable_scope(op_name, reuse=True): for data in weights_dict[op_name]: if len(data.shape) == 1: var = tf.get_variable('bias') session.run(var.assign(data)) else: var = tf.get_variable('weights') session.run(var.assign(data)) 模型测试用原始的参数值来测试构建好的AlexNet模型，原始参数的文件可从这里下载。 123456789101112131415161718192021222324252627282930313233343536373839import matplotlib.pyplot as pltfrom caffe_classes import class_namesimport cv2import osimg_dir = os.path.join(os.getcwd(), 'images')img_file = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith('.jpeg')]imgs = []for f in img_file: imgs.append(cv2.imread(f))imagenet_mean = np.array([104., 117., 124.], dtype=np.float32) # ImageNet中的图片像素均值x = tf.placeholder(tf.float32, [1, 227, 227, 3])model = AlexNetModel()score = model.inference(x)softmax = tf.nn.softmax(score)writer = tf.summary.FileWriter('./graph/alexnet', tf.get_default_graph())with tf.Session() as sess: sess.run(tf.global_variables_initializer()) model.load_original_weights(sess) fig2 = plt.figure(figsize=(15, 6)) for i, image in enumerate(imgs): img = cv2.resize(image.astype(np.float32), (227, 227)) img -= imagenet_mean img = img.reshape((1, 227, 227, 3)) probs = sess.run(softmax, feed_dict={x: img}) class_name = class_names[np.argmax(probs)] writer.close() fig2.add_subplot(1,3,i+1) plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) plt.title(&quot;Class: &quot; + class_name + &quot;, probability: %.4f&quot; %probs[0,np.argmax(probs)]) plt.axis('off') 得到预测结果： Dogs VS Cats下载Kaggle上的Dogs vs. Cats Redux Competetion的数据集，考虑微调上面构建好的AlexNet来完成该竞赛。 解压其中的压缩包后，可以看到其中包含$25000$张带标签的训练数据： 和$15000$张不带标签的测试数据： 数据处理首先将数据集划分一下，这里把$85%$的训练数据也就是$21250$张图片作为训练集，剩下的$4750$张图片作为验证集。 竞赛中要求将猫图的标签设为$0$，狗图的标签设为$1$。要使用这些数据来训练我们的模型，为了方便读取，可考虑获取所有的图片的路径和对应的标签，把它们统一放在一个txt文件中。实现该过程的程序如下： 123456789101112131415161718192021222324252627import ostrain_sets_dir = os.path.join(os.getcwd(), 'train')train_images_file = os.listdir(train_sets_dir)train_sets_list = []for fn in train_images_file: file_label = fn.split('.')[0] if file_label == 'cat': label = '0' else: label = '1' path_and_label = os.path.join(train_sets_dir, fn) + ' ' + label + '\\n' train_sets_list.append(path_and_label)validate_sets_list = train_sets_list[int(len(train_sets_list)*0.85):] # 15%作为验证集train_sets_list = train_sets_list[:int(len(train_sets_list)*0.85)]train_text = open('train.txt', 'w') # 写入txt文件for img in train_sets_list: train_text.writelines(img) validate_text = open('validate.txt', 'w') # 写入txt文件for img in validate_sets_list: validate_text.writelines(img) 这样就能得到名为train.txt、validate.txt的两个文件，文件的内容如下： 前面是图片的数据路径，后面则是该图片对应的标签。 导入数据用下面的辅助方法来调用tf.data.Dataset导入数据，并对数据进行一些简单的处理： 1234567891011121314151617181920212223242526272829303132333435# 数据处理IMAGENET_MEAN = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32) # 用于放缩范围def parse_image(filename, label): img_string = tf.read_file(filename) # 读取 img_decoded = tf.image.decode_png(img_string, channels=3) # 编码 img_resized = tf.image.resize_images(img_decoded, [227, 227]) # 裁剪 img_converted = tf.cast(img_resized, tf.float32) # 数据格式 img_centered = tf.subtract(img_resized, IMAGENET_MEAN) # 放缩范围 return img_centered, labeldef data_generate(txt_file, batch_size, num_classes, shuffle=True): paths_and_labels = np.loadtxt(txt_file, dtype=str).tolist() # 读取文件，组成列表 if shuffle: np.random.shuffle(paths_and_labels) # 打乱 paths, labels = zip(*[(l[0], int(l[1])) for l in paths_and_labels]) # 将paths和labels分开 steps_per_epoch = np.ceil(len(labels)/batch_size).astype(np.int32) paths = tf.convert_to_tensor(paths, dtype=tf.string) # 转换为tensor labels = tf.one_hot(labels, num_classes) labels = tf.convert_to_tensor(labels, dtype=tf.float32) dataset = tf.data.Dataset.from_tensor_slices((paths, labels)) # 创建数据集 dataset = dataset.map(parse_image) # 调函数进行预处理 if shuffle: dataset = dataset.shuffle(buffer_size=batch_size) dataset = dataset.batch(batch_size) # 小批量 return dataset, steps_per_epoch 微调AlexNet，其中的卷积层仍采用原始参数，而使用现有的数据集来训练其中的全连接层。因此调用上面的方法导入数据，设置并初始化迭代器，并设置一些超参数如下： 12345678910111213141516171819train_file = 'train.txt'validate_file = 'validate.txt'learning_rate = 0.01 # 超参数num_epochs = 10batch_size = 256num_classes = 2train_layers = ['fc8', 'fc7', 'fc6']train_data, train_steps = data_generate(train_file, batch_size=batch_size, num_classes=num_classes)validate_data, validate_steps = data_generate(validate_file, batch_size=batch_size, num_classes=num_classes)iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes) # 迭代器train_init = iterator.make_initializer(train_data)validate_init = iterator.make_initializer(validate_data)imgs, labels = iterator.get_next() 建立并训练模型建立AlexNet模型： 123456model = AlexNetModel(num_classes=num_classes, skip_layer=train_layers)loss = model.loss(imgs, labels) # 损失optimizer = model.optimize(learning_rate=learning_rate)correct_pred = tf.equal(tf.argmax(model.score, 1), tf.argmax(labels, 1))accuracy = tf.reduce_sum(tf.cast(correct_pred, tf.float32)) # 识别正确的总个数 训练并保存模型： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from datetime import datetime saver = tf.train.Saver()with tf.Session() as sess: sess.run(tf.global_variables_initializer()) writer.add_graph(sess.graph) model.load_original_weights(sess) print(&quot;{} Start training...&quot;.format(datetime.now())) for epoch in range(num_epochs): # 开始训练 sess.run(train_init) # 训练数据初始化 total_loss = 0 n_batches = 0 total_acc = 0 try: while True: _, l, ac = sess.run([optimizer, loss, accuracy]) total_loss += l total_acc += ac n_batches += 1 except tf.errors.OutOfRangeError: pass print('Average loss epoch {0}: {1}'.format(epoch, total_loss/n_batches)) # 平均损失 print(&quot;{} Training Accuracy = {:.4f}&quot;.format(datetime.now(), total_acc/21250.0) # 训练集准确率 print(&quot;{} Start validation&quot;.format(datetime.now())) sess.run(validate_init) # 初始化验证集 total_correct_preds = 0 try: while True: accuracy_batch = sess.run(accuracy) total_correct_preds += accuracy_batch except tf.errors.OutOfRangeError: pass print(&quot;{} Validation Accuracy = {:.4f}&quot;.format(datetime.now(), total_correct_preds/4750.0)) # 验证集准确率 print(&quot;{} Saving checkpoint of model...&quot;.format(datetime.now())) model_name = os.path.join(os.getcwd() + '/model', 'model_epoch'+str(epoch+1)+'.ckpt') save_path = saver.save(sess, model_name) # 保存模型 print(&quot;{} Model checkpoint saved at {}&quot;.format(datetime.now(), model_name)) 使用前面设置的超参数，本人的Colab上训练的结果如下： 精确度还不够高，可以尝试继续调整超参数。 测试模型在测试集上测试训练好的模型： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import osimport pandas as pdimport tensorflow as tftest_sets_dir = os.path.join(os.getcwd(), 'test')test_images_file = os.listdir(test_sets_dir)test_images_file.sort(key=lambda x:int(x[:-4]))test_sets_list = []for fn in test_images_file: path = os.path.join(test_sets_dir, fn) + '\\n' test_sets_list.append(path)test_text = open('test.txt', 'w') # 写入txt文件for img in test_sets_list: test_text.writelines(img) IMAGENET_MEAN = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32) # 用于放缩范围def parse_test_image(filename): img_string = tf.read_file(filename) img_decoded = tf.image.decode_png(img_string, channels=3) img_resized = tf.image.resize_images(img_decoded, [227, 227]) img_converted = tf.cast(img_resized, tf.float32) img_centered = tf.subtract(img_resized, IMAGENET_MEAN) return img_centeredimages_path = np.loadtxt('./test.txt', dtype=str).tolist()images_path = tf.convert_to_tensor(images_path, dtype=tf.string) test_dataset = tf.data.Dataset.from_tensor_slices((images_path))test_dataset = test_dataset.map(parse_test_image) test_dataset = test_dataset.batch(1000)test_iterator = test_dataset.make_one_shot_iterator() test_image = test_iterator.get_next()model = AlexNetModel(num_classes=2)score = model.inference(test_image)predicts = []saver=tf.train.Saver()with tf.Session() as sess: saver.restore(sess, './model/model_epoch10.ckpt') try: while True: scores = sess.run(score) predicts.extend(tf.argmax(scores, 1).eval()) except tf.errors.OutOfRangeError: pass# 生成测试结果并写入cvs文件中results = pd.Series(predicts, name=&quot;label&quot;) submission = pd.concat([pd.Series(range(1,12501),name = &quot;id&quot;), results],axis = 1)submission.to_csv(&quot;sample_submission.csv&quot;,index=False) 上传到kaggle上得到的成绩： 参考资料 ImageNet classification with deep convolutional neural networks Finetuning AlexNet with TensorFlow AlexNet详细解读-CSDN finetune_alexnet_with_tensorflow-Github tensorflow-cnn-finetune-Github 更新历史： 2019.5.4 完成初稿","link":"/2019/05/04/ML/cnn_alexnet/"},{"title":"【Coursera】深度学习(1)：Logistic回归","text":"深度学习（deep learning）是机器学习的一大分支，它试图在机器上建立模仿人脑机制进行分析学习的神经网络，赋予机器解释图像、声音、文本等数据的能力。 这里，首先介绍深度学习中的最基础的学习算法–Logistic回归。 二分类Logistic回归常用于解决二分类（Binary Classification）问题。在二分分类问题中，对于某个输入，输出的结果是离散的值。 例如：想要构建一个猫图分类器，即输入一张图片，希望该分类器准确判断出该图片是否是一张猫图，并输出它的预测结果。 首先，考虑这个猫图分类器的输入。图片是一类非结构化的数据，一张图片在计算机中以RGB编码时，是以红、绿、蓝为三基色，每个像素点上三基色对应的量的多少（即“亮度”）编码为数据进行存储。那么在计算机上，一张图片就可以由大小与图片一致的三个矩阵来表示，三个矩阵分别表示各颜色通道，矩阵中的值则表示各像素值。由此，上图中的猫图大小为$64 \\times 64$，便可以表示为三个大小为$64\\times64$的矩阵。 然而，在模式识别（Pattern Recognition）以及机器学习中，对于处理的各种类型的数据，通常采用一些特征向量来表示。简单地将一张猫图表示为一个特征向量，可以直接把三个矩阵进行拆分重塑，最终形成维数$n_x = 64 \\times 64 \\times 3 = 12288 $的一个向量$x$: 其次，实现这个分类器，需要准备大量的猫图及少量的非猫图，并取其中大部分组成该分类器的训练样本，少部分组成测试样本。将这些样本都以上述的方式表示为特征向量的形式，一个样本由一对$(x,y)$进行表示，其中x为$n_x$维的特征向量，$y$是该特征向量的标签（Label），根据该特征向量表示的是猫或非猫，取值为$0$或$1$。如果有$m$个训练样本对，它们将被表示为：$$ {(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})} $$ 更进一步，可以用矩阵的形式将我们的数据表示得更为紧凑。训练集中所有特征向量$x^{(1)}$、$x^{(2)}$…以及它们的标签$y^{(1)}$、$y^{(2)}$…分别进行列组合，变成两个矩阵$X$、$Y$：$$ {X = [x^{(1)},x^{(2)},…,x^{(m)}]}$$ $$ {Y = [y^{(1)},y^{(2)},…,y^{(m)}]}$$这样，$X$会是个大小为$n_x \\times m$的矩阵，$Y$是个大小为$1 \\times m$的矩阵。 Logistic回归模型Logistic回归是一种用于解决监督学习（Supervised Learning）问题的学习算法。进行Logistic回归的目的，是使训练数据的标签值与预测出来的值之间的误差最小化。建立猫图分类器的Logistic回归模型过程如下： 猫图分类器中，要实现的是：对于给定的以$n_x$维特征向量$x$表示、标签为$y$的一张图片，估计出这张图片为猫图的概率$\\hat{y}$，即：$$\\hat{y} = p(y = 1|x), 0 \\le \\hat{y} \\le 1$$ 有大量猫图的数据时，考虑采用线性拟合的方法，来找到一个$\\hat{y}$关于$x$的函数，从而实现这个猫分类器。规定一个$n_x$维向量$w$和一个值$b$作为参数，可得到线性回归的表达式：$${\\hat{y} = w^Tx + b}$$ 由于$\\hat{y}$为概率值，取值范围为$[0,1]$，简单地进行线性拟合，得出的$\\hat{y}$可能非常大，还可能为负值。此时，就需要用一个Logistic回归单元来对其值域进行约束。这里以sigmoid函数为逻辑回归单元，其表达式为：$$\\sigma{(z)} = \\frac{1}{1+e^{-z}}$$ 函数图像为： 从中可以看出，sigmoid函数具有如下性质： 当$z$趋近于正无穷大时，$\\sigma{(z)} = 1$; 当$z$趋近于负无穷大时，$\\sigma{(z)} = 0$; 当$z = 0$时，$\\sigma{(z)} = 0.5$。 所以可以用sigmoid函数来约束$\\hat{y}$的值域，得到该分类器的Logistic回归模型：$${ \\hat{y} = σ(w^Tx + b) = \\frac{1}{1+e^{-(w^Tx + b)}}}$$ 建立好Logistic回归模型后，接下来要考虑的是如何利用我们的训练集数据，找到该模型中的两个参数$w$和$b$的最优解。 成本函数为了训练逻辑回归模型中的参数w和b，使得输出值$\\hat{y}$与真实值y尽可能一致，即尽可能准确地判断一张图是否为猫，需要定义一个成本函数（Cost Function）作为衡量的标准。 单个样本的预测值（${\\hat y}^{(i)}$）与其真实值（$y^{(i)}$）之间的误差大小用损失函数（Loss Function）来衡量。平方误差（Square Loss）就是一种常用的损失函数，其表达式为：$${\\mathcal{L}(\\hat y,y)=\\frac{1}{2}(\\hat y-y)^2}$$ 但Logistic回归中，一般不采用这个损失函数，因为在训练参数过程中，使用这个损失函数将得到一个非凸函数而存在很多个局部最优解，使得后面可能无法使用梯度下降（Gradient Descent）来得到我们想要的最优解。 对这个Logistic回归模型，希望能够满足如下条件概率：$$p(y|x) = \\begin{cases} \\hat{y}, &amp; \\text{$(y=1)$} \\\\ 1 - \\hat{y}, &amp; \\text{$(y=0)$} \\end{cases}$$ 将上下两个式子合二为一，可写成：$$p(y|x) = \\hat{y}^y (1 - \\hat{y})^{(1 - y)} $$ 对两边取对数，进一步化简为：$$log\\ p(y|x) = ylog\\ \\hat y+(1-y)log\\ (1-\\hat y) $$ 我们希望$p(y|x)$的值越大越好，而损失越小越好。所以为上式添上负号，就可以将它作为一个损失函数，这便是应用很广的交叉熵（Cross Entropy）损失函数，表达式为：$${\\mathcal{L}(\\hat y,y)=-[ylog\\ \\hat y+(1-y)log\\ (1-\\hat y)]}$$ 交叉熵损失函数有如下性质： 当$y^{(i)}=1$时，${\\mathcal{L}({\\hat y}^{(i)},y^{(i)})=-log\\ {\\hat y}^{(i)}}$，意味着损失越小，$\\hat{y}$越接近于1； 当$y^{(i)}=0$时，${\\mathcal{L}({\\hat y}^{(i)},y^{(i)})=-log\\ (1-{\\hat y}^{(i)})}$，意味着损失越小，$\\hat{y}$越接近0。 对m个训练样本整体的成本函数，可以使用数理统计中的参数估计方法之一–最大似然估计法（Maximum Likelihood Estimation）推导出来的。 假设所有训练样本独立同分布，则它们的联合概率为所有样本概率的乘积，得到似然函数为:$$P (x) = \\prod_{i=1}^m p(y^{(i)}|x^{(i)})$$ 两边取对数有：$$log\\ P(x) = \\sum_{i=1}^m log\\ p(y^{(i)}|x^{(i)}) = - \\sum_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})$$ 最大似然估计中的下一步，是求解出一组使得上式取得最大值的 参数。而在这里，由于我们训练模型时，目标是使成本函数最小化，所以不直接使用上式作为成本函数，而将其负号去掉，同时为方便处理数据，而乘上一个常数$1/m$对式子适当进行放缩，最后得到下面的成本函数：$${J(w,b) = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}({\\hat y}^{(i)}, y^{(i)}) = - \\frac{1}{m} \\sum_{i=1}^m [ylog\\ \\hat y+(1-y)log\\ (1-\\hat y)]}$$ 梯度下降要找到参数的最优解，一般采用梯度下降法。 标量场中某一函数上某一点的梯度，指向该函数在该点处增长最快的方向，函数在该点处沿着该方向变化最快且变化率最大。 在空间坐标中以$w$，$b$为轴，画出的损失函数$J$的图像将类似于下图，要找到函数取得最小值的最优参数，先为$w$和$b$赋一个初始值，正如下图的最上面的红点。 对Logistic回归模型，因为成本函数是凸函数，无论参数的初始值是多少，最后总能到达同一个点或大致相同的点，所以几乎任何初始化方法都有效，于是通常将参数直接初始化为0。 所谓的梯度下降，就是从起始点开始，试图每次都沿最陡峭的下降方向下坡，尽可能快地到达最低点即凸函数取得最小值的点，而下坡的方向便是各点上的梯度值。 从一个二维上的图像看，函数的导数方向即为梯度方向，下降速度最快。即每经过一次梯度下降，参数$w$、$b$即更新为：$${w = w-\\alpha\\frac{\\partial J(w,b)}{\\partial w}}$$$${b = b-\\alpha\\frac{\\partial J(w,b)}{\\partial b}}$$ 式中的$\\alpha$被称为学习率，通常为一个小于1的值，用于控制梯度下降过程中每一次移动的规格大小，好比于下降时每一次迈步的大小。$\\alpha$的不宜太小也不宜过大：太小会使迭代次数增加，容易陷入局部最优解；太大容易错过最优解。 由梯度下降法，由训练数据经过多次迭代，就可以求得使成本函数的取得最小值的参数$w$和$b$，从而建立好Logistic模型，实现我们想要的猫图分类器。 Python实现使用Python编写一个逻辑回归分类器来识别猫，以此来了解如何使用神经网络的思维方式来进行这项任务，识别过程如图： 实现过程其中用到的Python包有： numpy是使用Python进行科学计算的基础包。 matplotlib是Python中著名的绘图库。 h5py在Python提供读取HDF5二进制数据格式文件的接口，本次的训练及测试图片集是以HDF5储存的。 PIL(Python Image Library)为Python提供图像处理功能。 scipy基于NumPy来做高等数学、信号处理、优化、统计和许多其它科学任务的拓展库。几个Python包的安装及基本使用方法详见官网。 1.导入要用到的所有包 1234567#导入用到的包import numpy as npimport matplotlib.pyplot as pltimport h5pyimport scipyfrom PIL import Imagefrom scipy import ndimage 2.导入数据 12345678910111213141516#导入数据def load_dataset(): train_dataset = h5py.File(&quot;train_cat.h5&quot;,&quot;r&quot;) #读取训练数据，共209张图片 test_dataset = h5py.File(&quot;test_cat.h5&quot;, &quot;r&quot;) #读取测试数据，共50张图片 train_set_x_orig = np.array(train_dataset[&quot;train_set_x&quot;][:]) #原始训练集（209*64*64*3） train_set_y_orig = np.array(train_dataset[&quot;train_set_y&quot;][:]) #原始训练集的标签集（y=0非猫,y=1是猫）（209*1） test_set_x_orig = np.array(test_dataset[&quot;test_set_x&quot;][:]) #原始测试集（50*64*64*3 test_set_y_orig = np.array(test_dataset[&quot;test_set_y&quot;][:]) #原始测试集的标签集（y=0非猫,y=1是猫）（50*1） train_set_y_orig = train_set_y_orig.reshape((1,train_set_y_orig.shape[0])) #原始训练集的标签集设为（1*209） test_set_y_orig = test_set_y_orig.reshape((1,test_set_y_orig.shape[0])) #原始测试集的标签集设为（1*50） classes = np.array(test_dataset[&quot;list_classes&quot;][:]) return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes 需要说明的是，本次的训练及测试图片集是以HDF5格式储存的,train_cat.h5、test_cat.h5文件打开后结构如下： 另外，也可以调用以下方法来查看训练集或测试集中的图片： 123456789#显示图片def image_show(index,dataset): index = index if dataset == &quot;train&quot;: plt.imshow(train_set_x_orig[index]) print (&quot;y = &quot; + str(train_set_y[:, index]) + &quot;, 它是一张&quot; + classes[np.squeeze(train_set_y[:, index])].decode(&quot;utf-8&quot;) + &quot;' 图片。&quot;) elif dataset == &quot;test&quot;: plt.imshow(test_set_x_orig[index]) print (&quot;y = &quot; + str(test_set_y[:, index]) + &quot;, 它是一张&quot; + classes[np.squeeze(test_set_y[:, index])].decode(&quot;utf-8&quot;) + &quot;' 图片。&quot;) 3.sigmoid函数 1234#sigmoid函数def sigmoid(z): s = 1/(1+np.exp(-z)) return s 4.初始化参数w，b 12345#初始化参数w,bdef initialize_with_zeros(dim): w = np.zeros((dim,1)) #w为一个dim*1矩阵 b = 0 return w, b 5.计算Y_hat,成本函数J以及dw，db 1234567891011121314#计算Y_hat,成本函数J以及dw，dbdef propagate(w, b, X, Y): m = X.shape[1] #样本个数 Y_hat = sigmoid(np.dot(w.T,X)+b) cost = -(np.sum(np.dot(Y,np.log(Y_hat).T)+np.dot((1-Y),np.log(1-Y_hat).T)))/m #成本函数 dw = (np.dot(X,(Y_hat-Y).T))/m db = (np.sum(Y_hat-Y))/m cost = np.squeeze(cost) #压缩维度 grads = {&quot;dw&quot;: dw, &quot;db&quot;: db} #梯度 return grads, cost 6.梯度下降找出最优解 12345678910111213141516171819202122232425#梯度下降找出最优解def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):#num_iterations-梯度下降次数 learning_rate-学习率，即参数ɑ costs = [] #记录成本值 for i in range(num_iterations): #循环进行梯度下降 grads, cost = propagate(w,b,X,Y) dw = grads[&quot;dw&quot;] db = grads[&quot;db&quot;] w = w - learning_rate*dw b = b - learning_rate*db if i % 100 == 0: #每100次记录一次成本值 costs.append(cost) if print_cost and i % 100 == 0: #打印成本值 print (&quot;循环%i次后的成本值: %f&quot; %(i, cost)) params = {&quot;w&quot;: w, &quot;b&quot;: b} #最终参数值 grads = {&quot;dw&quot;: dw, &quot;db&quot;: db}#最终梯度值 return params, grads, costs 7.得出预测结果 123456789101112131415#预测出结果def predict(w, b, X): m = X.shape[1] #样本个数 Y_prediction = np.zeros((1,m)) #初始化预测输出 w = w.reshape(X.shape[0], 1) #转置参数向量w Y_hat = sigmoid(np.dot(w.T,X)+b) #最终得到的参数代入方程 for i in range(Y_hat.shape[1]): if Y_hat[:,i]&gt;0.5: Y_prediction[:,i] = 1 else: Y_prediction[:,i] = 0 return Y_prediction 8.建立整个预测模型 123456789101112131415161718192021222324252627#建立整个预测模型def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False): #num_iterations-梯度下降次数 learning_rate-学习率，即参数ɑ w, b = initialize_with_zeros(X_train.shape[0]) #初始化参数w，b parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) #梯度下降找到最优参数 w = parameters[&quot;w&quot;] b = parameters[&quot;b&quot;] Y_prediction_train = predict(w, b, X_train) #训练集的预测结果 Y_prediction_test = predict(w, b, X_test) #测试集的预测结果 train_accuracy = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100 #训练集识别准确度 test_accuracy = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100 #测试集识别准确度 print(&quot;训练集识别准确度: {} %&quot;.format(train_accuracy)) print(&quot;测试集识别准确度: {} %&quot;.format(test_accuracy)) d = {&quot;costs&quot;: costs, &quot;Y_prediction_test&quot;: Y_prediction_test, &quot;Y_prediction_train&quot; : Y_prediction_train, &quot;w&quot; : w, &quot;b&quot; : b, &quot;learning_rate&quot; : learning_rate, &quot;num_iterations&quot;: num_iterations} return d 9.初始化样本，输入模型，得出结果 1234567891011121314#初始化数据train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()m_train = train_set_x_orig.shape[0] #训练集中样本个数m_test = test_set_x_orig.shape[0] #测试集总样本个数num_px = test_set_x_orig.shape[1] #图片的像素大小train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T #原始训练集的设为（12288*209）test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T #原始测试集设为（12288*50）train_set_x = train_set_x_flatten/255. #将训练集矩阵标准化test_set_x = test_set_x_flatten/255. #将测试集矩阵标准化d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True) 结果分析运行程序最终得到的结果为： 训练集识别准确率接近100％，测试集的准确率有70％。由于训练使用的小数据集，而且逻辑回归是线性分类器，所以这个结果对于这个简单的模型实际上还是不错。 使用mathplotlib画出学习曲线： 1234567# 画出学习曲线costs = np.squeeze(d['costs'])plt.plot(costs)plt.ylabel('cost')plt.xlabel('iterations (per hundreds)')plt.title(&quot;Learning rate =&quot; + str(d[&quot;learning_rate&quot;]))plt.show() 学习率不同时的学习曲线： 1234567891011121314151617learning_rates = [0.01, 0.001, 0.0001]models = {}for i in learning_rates: print (&quot;学习率: &quot; + str(i)) models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False) print ('\\n' + &quot;-------------------------------------------------------&quot; + '\\n')for i in learning_rates: plt.plot(np.squeeze(models[str(i)][&quot;costs&quot;]), label= str(models[str(i)][&quot;learning_rate&quot;]))plt.ylabel('cost')plt.xlabel('iterations')legend = plt.legend(loc='upper center', shadow=True)frame = legend.get_frame()frame.set_facecolor('0.90')plt.show() 说明不同的学习率会带来不同的成本，从而产生不同的预测结果。 参考资料 吴恩达-神经网络与深度学习-网易云课堂 Andrew Ng-Neural Networks and Deep Learning-Coursera 课程代码与资料-GitHub 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2017.09.13 完成初稿 2017.09.17 修正部分错误 2018.02.13 修改部分内容 2019.01.10 修改部分内容","link":"/2017/09/12/ML/deep_learning_1/"},{"title":"【Coursera】深度学习(2)：神经网络","text":"神经网络（Neural Network）的构筑理念是受到生物神经网络功能的运作启发而产生的。人工神经网络通常是通过一个基于数学统计学类型的学习方法得以优化，所以人工神经网络也是数学统计学方法的一种实际应用。 和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被基于传统规则的编程解决的。 什么是神经网络机器学习领域所说的神经网络，指的是一种模仿生物神经网络的结构和功能而建立的数学或计算模型，用于对函数进行估计或近似。下面通过一个房价预测模型的例子，来对所谓的神经网络有一个感性的认识。 给定的一些市面上房子的面积及其对应价格的数据，希望据此建立一个房价预测模型，即输入一个房子的面积，希望通过这个模型输出该房价的预测值。因为一般情况下房价和房子的面积都成正相关，所以这是可以作为一个线性回归问题，并且可以将已知数据之间的关系表现在平面坐标系中： 对这些数据进行线性拟合，因为房价永远不会是负数，将得到图中绘制的ReLU函数（Rectified Linear Unit，修正线性单元），这个函数将在后面多次用到。 将这个房价预测模型构建成一个神经网络，便以房子的面积作为输入，房价作为输出，ReLU函数则充当其中的神经元的作用，来产生输出，得到下面一个简单的神经网络模型： 更进一步地，考虑到房价除了受房子的面积影响之外，还会受卧室的数量、房子的位置以及地区的财富水平等其他因素的影响，搜集到这些因素的相关数据，要将这些因素也考虑在内的话，就需要构建一个更为复杂的神经网络。 如上图所示，综合了几种因素作为神经网络的输入，就构成了稍为复杂的神经网络。该网络将ReLU等函数作为隐藏层（Hidden Units）来处理输入数据，并输出我们想要的结果。 在这个预测房价的例子中，只要拥有足够量的训练数据，就能够生成一个较好的神经网络模型，从而对于任意的输入，都能得到较为精确的预测结果。 包括此在内的大多数机器学习问题，都是通过输入带有正确输出结果的数据来训练模型，在机器学习领域这种方法被称为监督学习（Surprised Learning）。对于不同领域的监督学习问题，需要用不同神经网络进行解决，而神经网络大致可分为如上所示的标准神经网络、擅长处理图像数据的卷积神经网络（CNN）以及处理序列数据的循环神经网络（RNN），前者处理的常是数据库中的结构化数据（Structured Data），而后两者则是图像、音频、文字序列等非结构化数据（Unstructured Data）。 简单而言，深度学习便是更为复杂的神经网络。 在逻辑回归中，通过建立一个简单的神经网络模型，输入训练样本(x,y)，希望得出一个预测值$\\hat{y}$，使得$\\hat{y}$尽可能等于y。训练的流程如下： 在这个模型中，先建立了一个损失函数，进而不断采用梯度下降法找到参数w和b的最优解。采用这种算法编写的猫识别器最终的准确率只有70%，想要进一步提高识别的精准度，就需要建立起一个多层的神经网络来训练样本。 符号约定如图所示的神经网络中，前面为输入层，中间为隐藏层 ，最后为输出层。中间层被称为隐藏层的原因是因为在训练过程中，将看到输入的样本有哪些，输出的结果是什么，中间层中的神经节点产生的真实值无法被观察到。所以中间层被称为隐藏层，只是因为你不会在训练集中看到它。 前面的逻辑回归中，用$X$表示输入，这里用符号$a^{[0]}$代替，上标“[ ]”括号中的数字表示神经网络中的第几层，且符号$a$代表着激活（Activation），指的是不同层次的神经网络传递给后续层次的值。 将输入集传递给隐藏层后，隐藏层随之产生激活表示为$a^{[1]}$，而隐藏层的第一节点生成的激活表示为$a^{[1]}_1$，第二个节点产生的激活为$a^{[1]}_2$，以此类推，则：$${ a^{[1]} = \\begin{bmatrix} a^{[1]}_1 \\\\ a^{[1]}_2 \\\\ a^{[1]}_3 \\\\ a^{[1]}_4 \\end{bmatrix}\\quad}$$最后，输出层输出的值表示为$a^{[2]}$，则$\\hat{y} = a^{[2]}$。 图中的这个神经网络也被称为两层神经网络，原因是计算神经网络的层数时，通常不考虑输入层。所以这个神经网络中，隐藏层是第一次层，输出层是第二层，而输入层为第零层。 图中的隐藏层中，将存在参数w和b，它们将分别表示为$w^{[1]}$和$b^{[1]}$，$w^{[1]}$将会是个4×3矩阵，$b^{[1]}$将会是个4×1矩阵。输出层中，也会存在参数$w^{[2]}$和$b^{[2]}$，$w^{[2]}$是个1×4矩阵，$b^{[2]}$是个1×1矩阵。 神经网络的表示 如图所示，将样本输入隐藏层中的第一个节点后，可得；$$ z^{[1]}_1 = w^{[1]T}_1X + b^{[1]}_1, a^{[1]}_1 = σ(z^{[1]}_1)$$以此类推：$$ z^{[1]}_2 = w^{[1]T}_2X + b^{[1]}_2, a^{[1]}_2 = σ(z^{[1]}_2)$$ $$ z^{[1]}_3 = w^{[1]T}_3X + b^{[1]}_3, a^{[1]}_3 = σ(z^{[1]}_3)$$ $$ z^{[1]}_4 = w^{[1]T}_4X + b^{[1]}_4, a^{[1]}_4 = σ(z^{[1]}_4)$$将它们都表示成矩阵形式：$${ z^{[1]} = \\begin{bmatrix} w^{[1]}_1 &amp; w^{[1]}_1 &amp; w^{[1]}_1\\\\ w^{[1]}_2 &amp; w^{[1]}_2 &amp; w^{[1]}_2 \\\\ w^{[1]}_3 &amp; w^{[1]}_3 &amp; w^{[1]}_3 \\\\ w^{[1]}_4 &amp; w^{[1]}_4 &amp; w^{[1]}_4 \\end{bmatrix}\\quad}\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\quad + \\begin{bmatrix} b^{[1]}_1 \\\\ b^{[1]}_2 \\\\ b^{[1]}_3 \\\\ b^{[1]}_4 \\end{bmatrix} = \\begin{bmatrix} z^{[1]}_1 \\\\ z^{[1]}_2 \\\\ z^{[1]}_3 \\\\ z^{[1]}_4 \\end{bmatrix}\\quad\\quad$$即：$$ z^{[1]} = w^{[1]}X + b^{[1]} $$ $$a^{[1]} = σ(z^{[1]})$$ 进过隐藏层后进入输出层，又有:$$ z^{[2]} = w^{[2]}a^{[1]} + b^{[2]}$$ $$a^{[2]} = σ(z^{[2]})$$ 可以发现，在一个的共有l层，且第l层有$n^{[l]}$个节点的神经网络中，参数矩阵 $w^{[l]}$的大小为$n^{[l]}$*$n^{[l-1]}$，$b^{[l]}$的大小为$n^{[l]}$*1。 逻辑回归中，直接将两个参数都初始化为零。而在神经网络中，通常将参数w进行随机初始化，参数b则初始化为0。 除w、b外的各种参数，如学习率$\\alpha$、神经网络的层数$l$，第$l$层包含的节点数$n^{[l]}$及隐藏层中用的哪种激活函数，都称为超参数（Hyper Parameters），因为它们的值决定了参数w、b最后的值。 激活函数建立一个神经网络时，需要关心的一个问题是，在每个不同的独立层中应当采用哪种激活函数。逻辑回归中，一直采用sigmoid函数作为激活函数，此外还有一些更好的选择。 tanh函数（Hyperbolic Tangent Function，双曲正切函数）的表达式为：$$tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$函数图像为： tanh函数其实是sigmoid函数的移位版本。对于隐藏单元，选用tanh函数作为激活函数的话，效果总比sigmoid函数好，因为tanh函数的值在-1到1之间，最后输出的结果的平均值更趋近于0，而不是采用sigmoid函数时的0.5，这实际上可以使得下一层的学习变得更加轻松。对于二分类问题，为确保输出在0到1之间，将仍然采用sigmiod函数作为输出的激活函数。 然而sigmoid函数和tanh函数都具有的缺点之一是，在z接近无穷大或无穷小时，这两个函数的导数也就是梯度变得非常小，此时梯度下降的速度也会变得非常慢。 线性修正单元，也就是上面举例解释什么是神经网络时用到的ReLU函数也是机器学习中常用到的激活函数之一，它的表达式为：$$g(z) = max(0,z) =\\begin{cases} 0, &amp; \\text{($z$ $\\le$ 0)} \\\\ z, &amp; \\text{($z$ $\\gt$ 0)} \\end{cases} $$函数图像为： 当z大于0时是，ReLU函数的导数一直为1，所以采用ReLU函数作为激活函数时，随机梯度下降的收敛速度会比sigmoid及tanh快得多，但负数轴的数据都丢失了。 ReLU函数的修正版本，称为Leaky-ReLU，其表达式为：$$g(z) = max(0,z) =\\begin{cases} \\alpha z, &amp; \\text{($z$ $\\le$ 0)} \\\\ z, &amp; \\text{($z$ $\\gt$ 0)} \\end{cases} $$函数图像为： 其中$ \\alpha $是一个很小的常数，用来保留一部非负数轴的值。 可以发现，以上所述的几种激活函数都是非线性的，原因在于使用线性的激活函数时，输出结果将是输入的线性组合，这样的话使用神经网络与直接使用线性模型的效果相当，此时神经网络就类似于一个简单的逻辑回归模型，失去了其本身的优势和价值。 前向传播和反向传播如图，通过输入样本$x$及参数$w^{[1]}$、$b^{[1]}$到隐藏层，求得$z^{[1]}$，进而求得$a^{[1]}$；再将参数$w^{[2]}$、$b^{[2]}$和$a^{[1]}$一起输入输出层求得$z^{[2]}$，进而求得$a^{[2]}$，最后得到损失函数$\\mathcal{L}(a^{[2]},y)$，这样一个从前往后递进传播的过程，就称为前向传播（Forward Propagation）。前向传播过程中：$$ z^{[1]} = w^{[1]T}X + b^{[1]} $$ $$a^{[1]} = g(z^{[1]})$$ $$ z^{[2]} = w^{[2]T}a^{[1]} + b^{[2]}$$ $$a^{[2]} = σ(z^{[2]}) = sigmoid(z^{[2]})$$ $${\\mathcal{L}(a^{[2]}, y)=-(ylog\\ a^{[2]} + (1-y)log(1-a^{[2]}))}$$ 在训练过程中，经过前向传播后得到的最终结果跟训练样本的真实值总是存在一定误差，这个误差便是损失函数。想要减小这个误差，当前应用最广的一个算法便是梯度下降，于是用损失函数，从后往前，依次求各个参数的偏导，这就是所谓的反向传播（Back Propagation），一般简称这种算法为BP算法。 sigmoid函数的导数为：$${a^{[2]’} = sigmoid(z^{[2]})’ = \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} = a^{[2]}(1 - a^{[2]})}$$ 由复合函数求导中的链式法则，反向传播过程中：$$ da^{[2]} = \\frac{\\partial \\mathcal{L}(a^{[2]}, y)}{\\partial a^{[2]}} = -\\frac{y}{a^{[2]}} + \\frac{1 - y}{1 - a^{[2]}}$$ $$ dz^{[2]} = \\frac{\\partial \\mathcal{L}(a^{[2]}, y)}{\\partial a^{[2]}} \\cdot \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} = a^{[2]} - y$$ $$ dw^{[2]} = \\frac{\\partial \\mathcal{L}(a^{[2]}, y)}{\\partial a^{[2]}} \\cdot \\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\cdot \\frac{\\partial z^{[2]}}{\\partial w^{[2]}} = dz^{[2]}\\cdot a^{[1]T}$$ $$ db^{[2]} = \\frac{\\partial \\mathcal{L}(a^{[2]}, y)}{\\partial a^{[2]}} \\cdot \\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\cdot \\frac{\\partial z^{[2]}}{\\partial b^{[2]}} = dz^{[2]}$$ $$ da^{[1]} = \\frac{\\partial \\mathcal{L}(a^{[2]}, y)}{\\partial a^{[2]}} \\cdot \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\cdot \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} = dz^{[2]} \\cdot w^{[2]} $$ $$ dz^{[1]} = \\frac{\\partial \\mathcal{L}(a^{[2]}, y)}{\\partial a^{[2]}} \\cdot \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\cdot \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\cdot \\frac{\\partial a^{[1]}}{\\partial z^{[1]}}= dz^{[2]} \\cdot w^{[2]} × g^{[1]’}(z^{[1]}) $$ $$ dw^{[1]} = \\frac{\\partial \\mathcal{L}(a^{[2]}, y)}{\\partial a^{[2]}} \\cdot \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\cdot \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\cdot \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} \\cdot \\frac{\\partial z^{[1]}}{\\partial w^{[1]}}= dz^{[1]} \\cdot X^T $$ $$ db^{[1]} = \\frac{\\partial \\mathcal{L}(a^{[2]}, y)}{\\partial a^{[2]}} \\cdot \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\cdot \\frac{\\partial z^{[2]}}{\\partial a^{[1]}} \\cdot \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} \\cdot \\frac{\\partial z^{[1]}}{\\partial b^{[1]}}= dz^{[1]}$$这便是反向传播的整个推导过程。 在具体的算法实现过程中，还是需要采用逻辑回归中用到梯度下降的方法，将各个参数进行向量化、取平均值，不断进行更新。 深层神经网络深层神经网络含有多个隐藏层，构建方法如前面所述，训练时根据实际情况选择激活函数，进行前向传播获得成本函数进而采用BP算法，进行反向传播，梯度下降缩小损失值。 拥有多个隐藏层的深层神经网络能更好得解决一些问题。如图，例如利用神经网络建立一个人脸识别系统，输入一张人脸照片，深度神经网络的第一层可以是一个特征探测器，它负责寻找照片里的边缘方向，卷积神经网络（Convolutional Neural Networks，CNN）专门用来做这种识别。 深层神经网络的第二层可以去探测照片中组成面部的各个特征部分，之后一层可以根据前面获得的特征识别不同的脸型的等等。这样就可以将这个深层神经网络的前几层当做几个简单的探测函数，之后将这几层结合在一起，组成更为复杂的学习函数。从小的细节入手，一步步建立更大更复杂的模型，就需要建立深层神经网络来实现。 Python实现之前使用逻辑回归实现过一个猫分类器，最终用测试集测试最高能达到的准确率有70%，现在用一个4层神经网络来实现。 1.L层神经网络参数初始化 1234567891011#初始化参数def initialize_parameters_deep(layer_dims): np.random.seed(1) parameters = {} L = len(layer_dims) for l in range(1, L): parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #初始化为随机值 parameters['b' + str(l)] = np.zeros((layer_dims[l], 1)) #初始化为0 return parameters 2.前向传播和后向传播 1234567891011121314151617181920212223242526272829303132333435363738#部分代码此处省略，详细代码见参考资料-Github#L层神经网络模型的前向传播def L_model_forward(X, parameters): caches = [] A = X L = len(parameters) // 2 for l in range(1, L): A_prev = A A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = &quot;relu&quot;) caches.append(cache) AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = &quot;sigmoid&quot;) caches.append(cache) assert(AL.shape == (1,X.shape[1])) return AL, caches#L层神经网络模型的反向传播def L_model_backward(AL, Y, caches): grads = {} L = len(caches) m = AL.shape[1] Y = Y.reshape(AL.shape) dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) current_cache = caches[L-1] grads[&quot;dA&quot; + str(L)], grads[&quot;dW&quot; + str(L)], grads[&quot;db&quot; + str(L)] = linear_activation_backward(dAL, current_cache, activation = &quot;sigmoid&quot;) for l in reversed(range(L-1)): current_cache = caches[l] dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[&quot;dA&quot; + str(l + 2)], current_cache, activation = &quot;relu&quot;) grads[&quot;dA&quot; + str(l + 1)] = dA_prev_temp grads[&quot;dW&quot; + str(l + 1)] = dW_temp grads[&quot;db&quot; + str(l + 1)] = db_temp return grads 3.L层神经网络模型 12345678910111213141516171819202122232425#L层神经网络模型def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False): np.random.seed(1) costs = [] parameters = initialize_parameters_deep(layers_dims) for i in range(0, num_iterations): AL, caches = L_model_forward(X, parameters) cost = compute_cost(AL, Y) grads = L_model_backward(AL, Y, caches) parameters = update_parameters(parameters, grads, learning_rate) if print_cost and i % 100 == 0: print (&quot;循环%i次后的成本值: %f&quot; %(i, cost)) if print_cost and i % 100 == 0: costs.append(cost) plt.plot(np.squeeze(costs)) plt.ylabel('cost') plt.xlabel('iterations (per tens)') plt.title(&quot;Learning rate =&quot; + str(learning_rate)) plt.show() return parameters 4.输入数据，得出结果 1234567891011121314151617train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()m_train = train_x_orig.shape[0] #训练集中样本个数m_test = test_x_orig.shape[0] #测试集总样本个数num_px = test_x_orig.shape[1] #图片的像素大小train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0],-1).T #原始训练集的设为（12288*209）test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0],-1).T #原始测试集设为（12288*50）train_x = train_x_flatten/255. #将训练集矩阵标准化test_x = test_x_flatten/255. #将测试集矩阵标准化layers_dims = [12288, 20, 7, 5, 1]parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)pred_train = predict(train_x, train_y, parameters)pred_test = predict(test_x, test_y, parameters) 得到的结果为： 样本集预测准确度: 0.985645933014测试集预测准确度: 0.8 参考资料 吴恩达-神经网络与深度学习-网易云课堂 Andrew Ng-Neural Networks and Deep Learning-Coursera 课程代码与资料-GitHub 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2017.09.28 完成初稿 2018.02.13 调整部分内容 2019.01.01 二刷课程后修改内容","link":"/2017/09/25/ML/deep_learning_2/"},{"title":"【Coursera】深度学习(7)：卷积神经网络","text":"计算机视觉（Computer Vision）是一门研究如何教机器“看”的科学，计算机视觉研究相关的理论和技术，试图创建能够从图像或者多维数据中获取“信息”的人工智能系统。 随着深度学习技术的发展，计算机视觉领域的研究也得到了快速的发展。在对各种图像进行处理的过程中，往往在少量的图像中便蕴含着大量的数据，难以用一般的DNN进行处理。而卷积神经网络（Convolutional Neural Network, CNN）作为一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，在图像处理工作上有着出色的表现。 卷积神经网络 前面在神经网络中提到过，构建一个深度神经网络来进行人脸识别时，深度神经网络的前面一层可以用来进行边缘探测，其次一层用来探测照片中组成面部的各个特征部分，到后面的一层就可以根据前面获得的特征识别不同的脸型等等。其中的这些工作，都是依托CNN实现的。 基本概念以下通过边缘检测的例子来阐述深度学习中卷积的基本概念。 如上图中，最左边是一张用大小为6×6的矩阵表示的存在明显分界线的灰度图，矩阵中的值大于“1”则代表白色，等于“0”则代表灰色，小于“0”则代表黑色。 通过与中间的大小为3×3的矩阵进行卷积（Convolution）运算，得到的大小为4×4的矩阵，就是图片中的存在的边缘。其中，中间这个矩阵$\\begin{bmatrix} 1 &amp; 0 &amp; -1 \\\\ 1 &amp; 0 &amp; -1 \\\\ 1 &amp; 0 &amp; -1 \\end{bmatrix}$被称为滤波器（Filter）。 在数学领域，“*”号是标准的卷积运算符号，但在计算机里通常用这个符号代表一般的乘法，要注意加以区分。 深度学习里面所称的卷积运算，和泛函分析中的卷积运算有所不同，它的求解过程只是简单将图像矩阵中，从左到右，由上到下，取与滤波器同等大小的一部分，每一部分中的值与滤波器中的值对应相乘后求和，最后的结果组成一个矩阵，其中没有经过翻转、反褶等过程。其运算过程如下面的卷积运算所示： 将这个灰度图左右的颜色进行翻转再与之前的滤波器进行卷积： 得到的结果中，原来的“30”也会变为“-30”，表示这时的图片是左灰右白，与原来相反。 上面的这个滤波器$\\begin{bmatrix} 1 &amp; 0 &amp; -1 \\\\ 1 &amp; 0 &amp; -1 \\\\ 1 &amp; 0 &amp; -1 \\end{bmatrix}$可以用来探测垂直方向的边缘，那么只要把这个滤波器翻转下，变成$\\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ -1 &amp; -1 &amp; -1 \\end{bmatrix}$，则这个新的滤波器就可以用来探测水平方向的边缘。如下图： 所以，不同的滤波器有着不同的作用。滤波器矩阵的大小和其中的值也都不是固定不变的，可以根据需求来选择。其中，$\\begin{bmatrix} 1 &amp; 0 &amp; -1 \\\\ 2 &amp; 0 &amp; -2\\\\ 1 &amp; 0 &amp; -1 \\end{bmatrix}$叫Sobel滤波器，它增加了中间行的权重，这样可能更稳健些。计算机视觉的研究中还会用到$\\begin{bmatrix} 3 &amp; 0 &amp; -3 \\\\ 10 &amp; 0 &amp; -10\\\\ 3 &amp; 0 &amp; -3 \\end{bmatrix}$，它叫Scharr滤波器。滤波器中的值还可以作为参数，通过训练来得到。 填充和步长前面可以看到，大小为6×6的矩阵与大小为3×3的滤波器进行卷积运算，得到的结果是大小为4×4的矩阵。假设矩阵的大小为$n \\times n$，而滤波器的大小为$f \\times f$，$f$一般是个奇数，则卷积后结果的大小就为$(n - f + 1)\\times(n - f + 1)$。 可以发现，原来的矩阵与滤波器进行卷积后的结果中损失了部分值，而且用滤波器处理一张图片时，往往在边角处只检测了部分像素点，丢失了图片边界处的众多信息。为解决这个问题，可以在进行卷积操作前，对原矩阵进行边界填充（Padding），也就是在矩阵的边界上填充一些值，以增加矩阵的大小，通常都用“0”作为填充值。 如上图中，设填充数量为$p$，当$p=1$时，就是在原来大小为6×6的矩阵的边界上填充一个像素，就得到一个大小为8×8的矩阵，再与滤波器卷积，最后得到的结果就会是一个大小为6×6的矩阵。填充后，卷积后结果的大小就为$(n + 2p - f + 1)\\times(n + 2p - f + 1)$。 这样，在进行卷积运算时，就存在两种选择： Valid 卷积：不进行任何处理，直接卷积卷积后结果的大小就为$(n - f + 1)\\times(n - f + 1)$。 Same 卷积：进行填充，并使得卷积后结果的大小与原来的一致，这时$p = \\frac{f - 1}{2} $。 卷积过程中，有时需要通过填充来避免信息损失，有时也要在卷积时通过设置的步长（Stride）来压缩一部分信息。 如上图中所示，设卷积的步长为$s$，当$s=2$时，就是在卷积运算时，每次滤波器在原矩阵中向右或向下移动一次的距离从原来的1变成2，这样最后得到的结果大小会是3×3，比一般的卷积结果还小。设置卷积步长为s后，卷积后的结果大小为$\\lfloor \\frac{n + 2p - f}{s} + 1 \\rfloor × \\lfloor \\frac{n + 2p - f}{s} + 1 \\rfloor$。”$\\lfloor \\ \\rfloor$”是向下取整符号，用于结果不是整数时进行向下取整。 高维卷积进行高维度的卷积运算，需要注意一些问题。 如上图，要处理一张有三个信道（Channel）RGB图片，也就是一个大小为6×6×3的矩阵，其中每一个信道为一个特征图（Feature Map），那么滤波器也要有三个信道，即滤波器矩阵大小为3×3×3的。计算的过程是将矩阵与滤波器对应的每一信道进行卷积运算，最后相加，得到的结果就还是个只有一个信道的大小为4×4矩阵。而且三层滤波器中，每一层中的值可以根据需求来设定。例如只想用滤波器来检测图中红色层的边缘，那么只要将后面两层滤波器的值全部置为0即可。 此外，还可以同时用多个滤波器来处理一个矩阵，以检测多个特征。如上图中第一个可以是用来检测检测图像矩阵的垂直边缘的滤波器，第二个可以是用来检测图像矩阵的水平边缘，把得到的两个结果组合在一起，结果是一个大小为4×4×2的矩阵。 符号约定简单来说，对于一个卷积神经网络，滤波器组成参数$w^{[l]}$，将它与输入$a^{[l-1]}$卷积之后，再加上一个为实数的参数$b^{[l]}$，就组成一般的神经网络中$z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}$的形式。此外，还需要进行一些符号的约定。 一般情况下使用到的滤波器都是个方形矩阵，也就是大小为$n\\times n$。在此，用$f^{[l]}$表示神经网络的第$l$层使用的滤波器大小，$p^{[l]}$表示对第$l$层的填充数，$s^{[l]}$表示第$l$层的卷积步长。 用$n_{H}$、$n_{W}$、$n_{C}$表示一张图片的长、宽、信道数，则对神经网络的第$l$层，输入矩阵$a^{[l-1]}$的大小将是$n_{H}^{[l-1]} \\times n_{W}^{[l-1]} \\times n_{C}^{[l-1]}$，使用的每个滤波器$w^{[l]}$的大小都应该是$f^{[l]} \\times f^{[l]} \\times n_{C}^{[l-1]}$，输出矩阵$z^{[l]}$以及激活$a^{[l]}$的大小都将是$n_{H}^{[l]} \\times n_{W}^{[l]} \\times n_{C}^{[l]}$，由之前得出的公式：$$ n_{H}^{[l]} = \\lfloor \\frac{n_{H}^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1 \\rfloor $$$$ n_{W}^{[l]} = \\lfloor \\frac{n_{W}^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1 \\rfloor $$ 由对高维卷积中的讲解中可知，$n_{C}^{[l]}$就等于卷积过程中用到的滤波器的个数，所以整个$w^{[l]}$的大小就应该是$n_{H}^{[l-1]} \\times n_{W}^{[l-1]} \\times n_{C}^{[l-1]} \\times n_{C}^{[l]}$，而$b^{[l]}$的大小就将是$1 \\times 1 \\times 1 \\times n_{C}^{[l]}$。 卷积神经网络的结构通常一个卷积神经网络是由输入层（Input）、卷积层（Convolution）、池化层（Pooling）、全连接层（Fully Connected）组成。 在输入层输入原始数据，卷积层中进行的是前面所述的卷积过程，用它来进行提取特征。全连接层就是将识别到的所有特征全部连接起来，并输出到分类器（如Softmax）。 池化层通常池化层都是用于对卷积层输出特征进行压缩，以加快运算速度，也可以对于一些特征的探测过程变得更稳健。 采用较多的一种池化过程叫最大池化（Max Pooling），其具体操作过程如下： 池化过程类似于卷积过程，且池化过程中没有需要进行学习的参数。把一个大小为4×4的矩阵分成四个等块，经过Max Pooling，取每一个区域中的最大值，输出一个大小为2×2的矩阵。这个池化过程中相当于使用了一个大小$f=2$的滤波器，且池化步长$s=2$。卷积过程中的几个计算大小的公式在此也都适用。 池化过程在一般卷积过程后，而卷积过程是个提取特征的过程。把上面的例子中这个4×4的输入看作某些特征的集合，则数字大就意味可能提取到了某些特定特征，Max池化的功能提取出这些特定特征，将它们保留下来。 也还有一种平均池化（Average Pooling）,就是从从以上取某个区域的最大值改为求这个区域的平均值： 采用卷积神经网络能够建立出更好的模型，获得更为出色的表现，在于其拥有的一些突出的优点。 对于大量的输入数据，卷积过程有效地减少了参数数量，而这主要归功于以下两点： **参数共享(Parameter Sharing)**：在卷积过程中，不管输入有多大，一个特征探测器也就前面所说的滤波器就能对整个输入的特征进行探测。 **局部感知(Local Perception)**：在每一层中，输入和输出之间的连接是稀疏的，每个输出值只取决于输入的一小部分值。 池化过程则在卷积后很好地聚合了特征，通过降维而减少了运算量。 示例结构一种典型卷积网络结构是LeNet-5，它是由用LeCun在上世纪九十年代提出的用来识别数字的卷积网络，下图的卷积网络结构与它类似： 其中，一个卷积层和一个池化层组成整个卷积神经网络中的一层，图中所示整个过程为Input-&gt;Conv-&gt;Pool-&gt;Conv-&gt;Pool-&gt;FC-&gt;FC-&gt;FC—&gt;Softmax。 下面各层中的数据： 可以看出，激活的大小随着向卷积网络深层的递进而减小，参数的数量在不断增加后在几个全连接过程后将逐渐减少。 经典的卷积网络LeNet-5LeNet-5是LeCun等人1998年在论文[Gradient-based learning applied to document recognition]中提出的卷积网络。其结构如下图： LeNet-5卷积网络中，总共大约有6万个参数。随着深度的增加，$n_H$、$n_W$的值在不断减小，$n_C$却在不断增加。其中的Conv-Pool-Conv-Pool-FC-FC-Output是现在用到的卷积网络中的一种很常见的结构。 AlexNetAlexNet是Krizhevsky等人2012年在论文[ImageNet classification with deep convolutional neural networks]中提出的卷积网络。其结构如下图： AlexNet卷积网络和LeNet-5有些类似，但是比后者大得多，大约有6千万个参数。 VGG-16VGG-16是Simonyan和Zisserman 2015年在论文[Very deep convolutional networks for large-scale image recognition]中提出的卷积网络。其结构如下图： VGG-16卷积网络的结构比较简单，其只要通过池化过程来压缩数据。VGG-16中的16指的是它有16个有权重的层。它是个比上面两个大得多的卷积网络，大约有13800万个参数。 ResNets当一个神经网络某个深度时，将会出现梯度消失（Vanishing Gradient）和梯度爆炸（Exploding Gradient）等问题。而ResNets能很好得解决这些问题。 ResNets全称为**残差网络（Residual Networks)**，它是微软研究院2015年在论文[Deep Residual Learning for Image Recognition]中提出的卷积网络。 如上图是一个神经网络中的几层，它一般的前向传播的过程，也称为“主要路径（main path）”为$a^{[l]}$-linear-ReLU-linear-ReLU-$a^{[l+2]}$，计算过程如下：$$ z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]} $$$$a^{[l+1]} = g(z^{[l+1]}) $$$$ z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]} $$$$a^{[l+2]} = g(z^{[l+2]}) $$ 在残差网络中，通过“捷径（short cut）”直接把$a^{[l]}$添加到第二个ReLu过程里，也就是最后的计算过程中：$$ a^{[l+2]} = g(z^{[l+2]} + W_s a^{[l]}) $$ 其中$a^{[l]}$需要乘以一个矩阵$W_s$使得它的大小和$z^{[l+2]}$匹配。 深度神经网络通过这种跳跃网络层的方式能获得更好的训练效果。上面这种结构被称为一个残差块（Residual Blocks）。 在普通神经网络（Plain NetWork）中使用多个这种残余块的结构，就组成了一个残差网络。两者的成本曲线分别如下： 普通的神经网络随着梯度下降的进行，理论上成本是不断下降的，而实际上当神经网络达到一定的深度时，成本值降低到一定程度后又会趋于上升，残差神经网络则能解决这个问题。 对于一个神经网络中存在的一些恒等函数（Identity Function），残差网络在不影响这个神经网络的整体性能下，使得对这些恒等函数的学习更加容易，而且很多时候还能提高整体的学习效率。 Network In NetWork2013年新加坡国立大学的林敏等人在论文[Network In NetWork]中提出了1×1卷积核及NIN网络。 使用1×1卷积核进行卷积的过程如上图，它就是在卷积过程中采用大小为1×1的滤波器。如果神经网络的当前一层和下一层都只有一个信道，也就是$n_C = 1$，那么采用1×1卷积核起不到什么作用的。但是当它们分别为有m和n个信道时，采用1×1卷积核就可以起到跨信道聚合的作用，从而降低（或升高）数据的维度，可以达到减少参数的目的。换句话说，1×1的卷积核操作实现的其实就是一个特征数据中的多个Feature Map的线性组合，所以这个方法就可以用来改变特征数据的信道数。 如上面的例子中，用32个大小为1×1×192的滤波器进行卷积，就能使原先数据包含的192个信道压缩为32个。在此注意到，池化能压缩数据的高度（$n_H$）及宽度（$n_W$），而1×1卷积核能压缩数据的信道数（$n_C$）。 Inception Network最早的Inception结构的V1版本是由Google的Szegedy 2014年在论文[Going deeper with convolutions]中提出的，它是ILSVRC 2014中取得最好成绩的GoogLeNet中采用的的核心结构。通过不断改进，现在已经衍生有了V4版本。 早期的V1版本的结构借鉴了NIN的设计思路，对网络中的传统卷积层进行了修改，其结构大致如下面的例子中所示： 通常在设计一个卷积网络的结构时，需要考虑卷积过程、池化过程的滤波器的大小，甚至是要不要使用1×1卷积核。在Inception结构中，考虑到多个不同大小的卷积核（滤波器）能够增强网络的适应力，于是分别使用三个大小分别为1×1、3×3、5×5的卷积核进行same卷积，同时再加入了一个same最大池化过程。最后将它们各自得到的结果放在一起，得到了图中一个大小为28×28×256的结果。然而，这种结构中包含的参数数量庞大，对计算资源有着极大的依赖，上面的例子中光是与大小为5×5的滤波器进行卷积的过程就会产生1亿多个参数！ 在其中的过程中，再加入1×1卷积能有效地对输出进行降维。如上图中所示，中间的一层就像是一个沙漏的瓶颈部分，所以这一层有时被称为瓶颈层（Bottleneck Layer）。通过1×1卷积，最后产生参数数量有1240万左右，相比起原来的1亿多要小了许多。 在论文中提出的整个Inception模型结构如下： 在一个卷积网络中加入多个这种模型，就构成了一个Inception网络，也就是： 其中还包含一些额外的最大池化层用来聚合特征，以及最后的全连接层。此外还可以从中间层的一些Inception结构中直接进行输出（图中没有画出），也就是中间的隐藏层也可以直接用来参与特征的计算及结果预测，这样能起到调整的作用，防止过拟合的发生。 Inception模型后续有人提出了V2、V3、V4的改进，以及引入残差网络的版本，这些变体都源自于这个V1版本。 最后，值得一提的是，Inception这个名字来自于电影《盗命空间》，用其中”We need to go deeper”这个梗，表明作者建立更深层次更加强悍的神经网络的决心！ 其他注意点在建立一个卷积神经网络时，可以从参考Github等网站上其他人建立过的相关模型，必要时可以直接拿来根据拥有的数据量大小进行前面介绍过的迁移学习，从而减轻一些工作负担。 当收集到的图像数据较少时，可以采用优化神经网络中讲过的数据扩增法，对现有的图像数据进行翻转、扭曲、放大、裁剪，甚至是改变颜色等方法来增加训练数据量。 参考资料 吴恩达-卷积神经网络-网易云课堂 Andrew Ng-Convolutional Neural Networks-Coursera 池化-ufldl LeNet-5官网 梯度消失、梯度爆炸-csdn 残差resnet网络原理详解-csdn 关于CNN中1×1卷积核和Network in Network的理解-csdn GoogLeNet 之 Inception(V1-V4)-csdn 课程代码与资料-GitHub 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2017.12.10 完成初稿 2018.02.13 调整部分内容","link":"/2017/11/28/ML/deep_learning_7/"},{"title":"【Coursera】深度学习(6)：结构化机器学习项目","text":"构建好一个机器学习系统并获得一些初步结果时，为得到最令人满意的结果，后续往往还需要进行大量的改进。如前面优化神经网络中所述，改进的方法多种多样，可能是收集更多的数据，或者是进行正则化，或者是采用不同的优化算法。 想要找准改进的方向，使一个机器学习系统更快更有效地工作，需要学习一些在构建机器学习系统时常用到的策略。 正交化构建机器学习系统的挑战之一就是其中有很多你可以尝试、更改的东西，例如，其中有很多的超参数需要进行训练。把握好尝试、更改的方向，认识到进行的调整将带来的影响，是十分关键的。 正交化（Orthogonalization）是确保修改一个系统中的某个算法指令或者组件时，不会产生或传播副作用到系统种的其他组件的一种系统设计属性。它使得验证一个算法独立于另一个算法时变得更加容易，同时也能减少设计和开发的时间。 例如在学习驾驶一辆汽车时，主要是在学习转向、加速、制动这三个基本的控制方法，这三种控制手段互不干扰，你只要通过不断得训练，熟练掌握它们就好。而假如要你去学会驾驶一辆只带一根操纵杆的汽车，操纵杆设计好了每操作一下就控制一定的转角、一定的速度，这时学习成本就大了很多。所谓的正交化，就是这个道理。 当设计一个监督学习系统，需做到符合下面四个假设且它们是正交的： 建立的模型在训练集上表现良好； 建立的模型在开发集（验证集）上表现良好； 建立的模型在测试集上表现良好； 建立的模型在实际的应用中表现良好。 当做到正交化时，如果发现 训练集上表现不够好–尝试采用更大的神经网络或者换一种更好的优化算法； 验证集上表现不够好–尝试进行正则化处理或者加入更多训练数据； 测试集上表现不够好–尝试采用更大的验证集进行验证； 现实应用中表现不够好–可能是因为测试集没有设置正确或者成本函数评估出错。 面对遇到的各种问题，正交化能够帮助我们更为精准地定位及有效地解决问题。 找准目标单一数字评估构建机器学习系统时，通过设置一个单一数字评估指标（single-number evaluation metric），可以更为快速地判断出在经过几次调整后得到的不同结果里，哪个的效果要好些。 对于一个分类器，评价分类器性能的指标一般是分类的准确率（Accuracy），也就是正确分类的样本数和总样本数之比，它也就可以作为一个单一数字估计指标。例如之前的猫分类器的几个算法都是通过准确率作其性能好坏的标准。 对于二分类问题常用的评价指标是精确率（Precision）和召回率（Recall），将所关注的类作为正类（Positive），其他的类为负类（Negative），分类器在数据集上预测正确或不正确，4种情况出现的种数分别记为： TP（True Positive）——将正类预测为正类数 FN（False Negative）——将正类预测为负类数 FP（False Positive）——将负类预测为正类数 TN（Ture Negative）——将负类预测为负类数 将精准率定义为：$$ P = \\frac{TP}{TP + FP} $$召回率定义为：$$ R = \\frac{TP}{TP + FN} $$ 而当遇到以下这种情况不好判别时，就需要采用F1度量（F1 Score）来判断两个分类器的好坏。 F1度量定义为：$$ F_1 = \\frac{2}{\\frac{1}{P} + \\frac{1}{R}} = \\frac{2TP}{2TP + FP + FN} $$ F1度量值其实就是精准率和召回率的调和平均数（Harmonic Mean），它是一种基于其平均值改善的方法，比简单地取平均值效果要好。如此，算出上图种A分类器的F1度量值为92.4%，B分类器的为91.0%，从未得知A分类器效果好些。这里F1度量值就作为了单一数字评估指标。 满足、优化指标 然而有时，评判的标准不限于一个单一数字评估指标。比如上图中的几个猫分类器，想同时关心它们各自的识别准确率和运行时间，但如果把这两个指标组合成一个单一数字评估指标的话，就不太好了。这时，就需要把一个指标作为优化指标（Optimizing Metric），而另外的一些的作为满足指标（Satisficing Metric）。 如上面所举的例子中，准确率就是一个优化指标，因为想要分类器尽可能做到正确分类，而运行时间就是一个满足指标，如果你想要分类器的运行时间不多于某个值，那你需要选择的分类器就应该是以这个值为界里面准确率最高的那个，以此作出权衡。 除了采用这些标准来评判一个模型外，也要学会在必要时及时地调整一些评判指标，甚至是更换训练数据。例如两个猫分类器A和B的识别误差分别为3%和5%，但是处于某种原因，A识别器会把色情图片误识为猫，引起用户的不适，而B不会出现这种情况，这时，识别误差大一些的B反而是更好的分类器。可以用以下公式来计算错误识别率：$$ Error: \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L} \\{ {\\hat{y}}^{i} \\neq y^{i}\\}\\ $$ 还可以设置一个$w^{(i)}$，当$x^{(i)}$是色情图片时，$w^{(i)}$为10，否则为1，以此来区分色情图片及其他误识别的图片：$$ Error: \\frac{1}{\\sum w^{(i)}} \\sum_{i=1}^m w^{(i)} \\mathcal{L} \\{ {\\hat{y}}^{i} \\neq y^{i}\\}\\ $$ 所以要根据实际情况，正确确定一个评判指标，确保这个评判指标最优。 数据处理构建机器学习系统时，对数据集的处理方法将影响你个整个构建过程中的进度。通过前面已经知道，一般把收集到的现有数据分为训练集、开发集和测试集，其中开发集也称为交叉验证集。构建机器学习系统时，采用一些不同的方法，在训练集上训练出不同的模型，随后使用开发集对模型的好坏进行评估，确信某个模型效果足够好时再用测试集进行测试。 首先需要注意的是，开发集和测试集的来源应该是相同的，且必须从所有数据中随机抽取，选择的开发测试集也要尽可能和以后机器学习系统要面对的数据一致，这样才能做到尽可能不偏离目标。 其次需要注意每个数据集大小的划分。在早期的机器学习时代，对于你拥有的所有数据集，通常就是把其中的70%作为训练集，剩下的30%作为测试集；或者要加入开发集的话，就把60%作为训练集，开发、测试集各取20%。当获得的数据比较少的时候，这种划分是合理的。但到了现在的机器学习时代，获得的数据数量一般都是成千上万的，这时就不能按传统的数据划分法来划分了。 假如获得了一百万个数据样本，将其中98%的数据都作为训练集，而只要将1%的也就是一万个数据作为开发集，1%的数据作为测试集就足够了。所以要根据实际情况，进行数据划分，而不是死板地遵循着传统。测试集的大小应该设置得足够提高系统整体性能得可信度，开发集的大小也要设置得足够用于评估几个不同的模型。 比较人类表现水平如今，设计和建立一个机器学习系统比以前变得更为简单高效，一些机器学习算法的在很多领域的表现已经可以和我们人类一决高下了，例如之前由Google DeepMind公司开发的声名全球AlphaGo。然而，很多任务对于我们人类来说，都能够几近完美地完成，达到人类的表现水平便是机器学习试图去模仿、实现的。 上图展示了随着时间的推进机器学习和人的表现水平的变化，一般的，当机器学习超过人的表现水平后，它就进步地很缓慢了，其中有一个重要的原因是人类的对于一些自然感知问题的表现水平几近于贝叶斯误差（Bayes Error）。贝叶斯误差被定义为最优的可能误差，换句话说，就是任何从x到精确度y映射的函数都不可能超过这个值。 当建立的机器学习模型的表现还没达到人类的表现水平时，可以通过各种手段来提升它。例如采用人工标记过的数据进行训练，通过人工误差分析了解为什么人能够正确识别，或者是进行误差、偏差分析。 在优化神经网络中曾涉及过偏差和方差的概念，通过与人类在某件事情上的表现水平向比较，可以清楚地表明一个机器学习模型对此的表现的好坏程度，由此作出判断出后续应该进行减小偏差还是减小方差。 训练集的误差和人类表现水平的误差的差值就称为可避免偏差（Avoidable Bias）。例如上图中的两个场景下，将人的错误率和机器学习模型的错误率进行比较，看以看出在A场景下，学习算法的错误率和人的错误率的可避免偏差较大，这种情况下后续的工作就是通过之前介绍过的方法来降低训练集的错误率，以减小偏差。而在B场景下，学习算法和人的表现相当，偏可避免偏差只有0.5%，，后续的工作就应该转向尽可能地减小开发集和训练集那部分2%的方差。 人类表现水平给了我们一种估计贝叶斯误差的方式，而不是之前的将训练的错误率直接对着0%的方向进行优化。所以在获取到某项任务的人为误差数据时，就可以将这些数据作为贝叶斯误差的代理，以此来分析学习算法的偏差及方差。如果训练集与人类表现水平之间的误差差值大于训练集与开发集之间的差值，往后就应该专注于降低偏差，反之，就应该专注于降低方差。 上图中总结出了各种应对偏差及方差的方法。总之，可避免偏差低便意味着你的训练过程做的很好，可接受范围内的方差意味着你的模型在开发、测试集上的表现与训练集上同样好。 此外，如果出现可避免误差为负值，也就是机器学习模型的表现超过人类表现水平即贝叶斯误差的代理的情况时，并不意味着你的模型的性能已经达到极点了，往后想要再提高就要另寻一些常规方法了。现在机器学习在很多领域已经做到这一点了，例如进行在线广告业务、产品推销、预测物流运输时间、信用评估等等。 错误分析当使用一个学习算法做人类可以做的任务时，如果这个学习算法还达不到人类去做时的性能，通过人工检查算法得出的结果中出现的一些错误，可以使你深入了解下一步要进行的工作，这个过程便称为错误分析。 比如对于一个猫分类器，在开发组里你已经取得了90%的识别准确率，还存在10%的出错率，而且还发现分类器会将一些看起来像猫的狗的图片误识别为猫，这时就不是立即盲目地转向去做一个能够精确识别出狗的算法，而是先进行错误分析。可以先把学习算法标签错误的图片找出来，然后进行人工检查，假如100张错误标签的图片中有5张是狗的图片，那么也就表明你的学习算法的10%的错误中大致只有5%来自于狗，也就是0.5%的错误来自于狗，这表明改善狗的识别问题并不能给你的学习算法带来多大提升。但如果100张错误标签的图片中有50张是狗的图片，也就是5%的错误是狗产生的，那么对于狗的识别就是你要解决的问题了。这样，先通过少量的时间去分析问题，再决定后面的要进行大方向。 在对输出结果中错误标签的样本进行人工分析时，常常可以建立一个表格，来记录每一个样本的一些情况，比如某些图像是模糊的，或者是把狗识别成了猫等等，以此来更加清楚地进行分析。 当发现输入的样本中，有一些样本是人为原因的错误标签，如果是在训练集中，只要这些出错的样本数量较小，分布地也比较随机，由于机器学习算法对于随机误差的稳健性（Robust）（也称“鲁棒性”），就不必花时间去一一修正它们；如果出现在开发集或者测试集，就可以在进行误差分析的时候，将这种情况对你的识别准确率的影响作出大致分析，这样对于是否值得去将输入中的错误标记一一进行修正，就有个明确的答案了。 总结一下：在构建一个机器学习系统时，尽可能按照以下几点来做： 设置好训练、开发、测试集及衡量指标，找准目标； 快速构建出一个初步的系统，用训练集来拟合参数，开发集调参，测试集评估； 采用方差/偏差分析或者错误分析等方法来决定下一步工作。 不匹配的数据集对于获取的数据集，前面一直在强调训练、开发、测试集的来源都应该要是相同的。在无法达成这一要求下，对于不同来源的数据集，就要充分考虑如何将它们进行划分。 想要开发一款手机应用，就像现在的Google相册中分类功能一样，能对用户上传的猫的图片进行识别。假如现在有1万张普通用户上传的猫的图片数据，这些图片的质量都不太好，有一些可能是模糊的，另外又网络上获取了20万张质量较好的猫的图片。 构建机器学习模型时，在开发集和测试集上，一定要反映出将来需要面临的数据。考虑到例子中这个机器学习模型主要将应用在识别用户拍摄的猫的图片上，在划分数据上，就可以将20万张网络获取和图片和5千张用户上传的图片共20.5万张图片作为训练集，剩下的5千张图片一半作开发集，一半作测试集。长远来看，这种分配方法比起随机打乱所有数据样本再进行分配性能要好。 在这种情况下，由于数据集的不匹配，后续如果进行方差/偏差分析，就很难找到问题的根源了，例如对于上面的例子，由于开发集包含的样本比训练集中的样本更加难以准确识别，开发集的错误率往往会大于训练集的错误率。为了解决这个问题，可以再定义一个训练-开发集，训练-开发集和训练集的来源相同，但是这部分并不参与训练。 如上图所示，加入训练-开发集后得到的数据中，在B场景下训练集错误率和训练-开发集之间相差较大，它们的来源相同，由此就可以知道这是个偏差问题，后面各个场景下都可以按照之前的偏差/方差分析进行分析了，训练-开发集便起到了这个作用。 对于不匹配的数据集，偏差/方差分析过程如下： 在一些情况下，可以直接通过一些手段直接解决数据集不匹配的问题。首先，需要进行人工错误分析，以了解训练、开发、测试集之间存在的差异，随后根据分析结果制作一些数据或者收集之间互相匹配的数据集。例如，必要是可以进行人工合成，将一些高质量的照片模糊化，或者对一些声音信息加入一些噪音，以此来使数据集匹配。 多任务和端对端学习迁移学习迁移学习（Tranfer Learning）是将一个神经网络从一个任务中学到的知识和经验，运用到另一个任务中。 如上图中，将为猫识别器构建的神经网络迁移应用到放射诊断中，因为猫识别器的神经网络已经学习到了关于图像的结构和性质等方面的知识，所以只要先删除神经网络的中最后一层，输出层的权重值也改为随机初始化的值，随后输入新的训练数据进行训练，就完成了以上的迁移学习。如果重新训练神经网络中的所有参数，则在这个图像识别的初始阶段称为预训练（Pre-Training）,它将预先初始化各个神经网络的权重值，之后的权重更新过程便称为微调（Fine-Tuning）。 符合下面的条件时，进行迁移学习才是有意义的： 两个任务使用的数据集相同； 拥有更多数据的任务到数据较少的任务； 任务底层神经网络的某些功能对另一个任务有帮助。 多任务学习多任务学习（Multi-Task Learning）是采用一个神经网络来同时执行多个任务，且这些任务的执行可以相互促进。 在自动驾驶技术中，车辆必须同时检测视野范围内的各种物体，这时就要用到多任务学习，训练一个神经网络来检测多种物体，这样可以做到一些早期特征在不同类型的对象间共享。假如要同时识别行人、汽车、路标、交通灯，则上图片的标签将表示为：$$ y^{(i)} = \\begin{bmatrix}0\\1\\1\\0\\\\end{bmatrix} $$其中每一行分别表示四个要识别的物体，0或1代表无或有。其中的某些项目就算没有标记出来，并不产生影响。 神经网络的结构和损失函数将如下图所示: 符合下面的条件时，采用多任务学习才是有意义的： 进行的任务都能从共享的较低级别功能中受益； 对每个任务拥有的数据量相当； 有能力训练足够大的神经网络来完成所有任务。 端到端学习端到端深度学习（End-to-end Deep Learning）是将处理或学习系统简化为一个神经网络。 如图，传统的语音识别系统，是由声学模型、词典、语言模型构成，而其中的语音模型和语言模型是分别训练的，而不同的语言也有不同的语言模型，比如英语和中文。而端到端的语音识别系统，从语音特征（输入端）到文字串（输出端）中间就只需要一个神经网络模型。 端到端深度学习并不能应用于每一个问题，因为它需要大量的标记数据。它主要应用于音频转录，图像捕捉，图像合成，机器翻译，自驾车转向等。 参考资料 吴恩达-结构化机器学习项目-网易云课堂 Andrew Ng-Structuring Machine Learning Projects-Coursera Andrew Ng-Machine Learning Yearning 端到端学习究竟是什么? 课程代码与资料-GitHub 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2017.11.18 完成初稿 2018.02.13 调整部分内容","link":"/2017/11/09/ML/deep_learning_6/"},{"title":"【Coursera】深度学习(8)：卷积神经网络的应用","text":"认识了解了卷积神经网络的工作原理及其一些经典的网络结构后，这里介绍如何应用卷积神经网络，实现目标检测、人脸识别及神经风格转换。 目标检测图像识别中，目标检测的任务，是对输入图像样本准确进行分类的基础上，检测其中包含的某些目标，并对它们准确定位并标识。 目标定位图像分类问题一般都采用Softmax回归来解决，最后输出的结果是一个多维列向量，且向量的维数与假定的分类类别数一致。在此基础上希望检测其中的包含的各种目标并对它们进行定位，这里对这个监督学习任务的标签表示形式作出定义。 如上图所示，分类器将输入的图片分成行人、汽车、摩托车、背景四类，最后输出的就会是一个四维列向量，四个值分别代表四种类别存在的概率。 加入目标检测任务后，用$p_c$表示存在目标的概率；以图片的左上角为顶点建立平面坐标系，用$b_x$、$b_y$组成图像中某个目标的中点位置的二维坐标，它们的值都进行了归一化，相当于把图片右下角坐标设为(1,1)；$b_h$、$b_w$表示图中用以标识目标的红色包围盒（Bounding Box）的长度和宽度；$c_n$表示存在第n个种类的概率。不存在目标时，$p_c = 0$，此时剩下的其他值都是无效的。整个标签的表示形式如下：$$y = \\begin{bmatrix} p_c \\\\ b_x \\\\ b_y \\\\ b_h \\\\ b_w \\\\ c_1 \\\\ c_2 \\\\ … \\end{bmatrix}$$ 在计算损失时将有：$$\\mathcal{L}(\\hat y,y) = \\begin{cases} (\\hat{y_1}-y_1)^2 + (\\hat{y_2}-y_2)^2 + … +(\\hat{y_n}-y_n)^2 , &amp; \\text{$(y_1(p_c)=1)$} \\\\ (\\hat{y_1}-y_1)^2, &amp; \\text{$(y_1(p_c)=0)$} \\end{cases}$$ 其中，对不同的值，可采用不同的损失函数。 另外，如需检测某幅图像中的某些特征点，比如一张人脸图像中五官的各个位置，可以像标识目标的中点位置那样，在标签中，将这些特征点以多个二维坐标的形式表示。 滑窗检测用以实现目标检测的算法之一叫做滑窗检测（Sliding Windows Detection）。 滑动窗口检测算法的实现，首先需要用卷积网络训练出一个能够准确识别目标的分类器，且这个分类器要尽可能采用仅包含该目标的训练样本进行训练。随后选定一个特定大小（小于输入图像大小）的窗口，在要检测目标的样本图像中以固定的步幅滑动这个窗口，从上到下，从左到右依次遍历整张图像，同时把窗口经过的区域依次输入之前训练好的分类器中，以此实现对目标及其位置的检测。 选用窗口的大小、进行遍历的步长决定了每次截取的图片大小及截取的次数，如上图所示，这也关系到了检测性能的好坏以及计算成本的高低。然而这种方法实现滑窗的计算成本往往很大，效率也较低。 上述的滑窗检测的过程类似于前面进行卷积运算过程，而滑窗检测算法其实就可以用一个完整的卷积网络较为高效地实现，此时需要将卷积网络中原来的全连接层转化为卷积层。 上图所示为一个卷积神经网络，经过卷积、池化后，全连接过程可以看作是将池化后得到的大小为5×5×16的结果与400个大小也为5×5×16的卷积核分别进行卷积，输出的结果大小为1×1×400，进一步全连接再采用Softmax后，最后输出的结果大小为1×1×4。由此，全连接过程本质上还是一个卷积过程。 在卷积网络中实现滑窗检验的过程如上图。向之前的示例中输入一个更大的图像，而保持各层卷积核的大小不变，最后的输出结果大小为2×2×4，也就相当于用一个大小为14×14的窗口，以2个单位的步长，在输入的图像中进行滑窗检测后得到的结果，图中对此用不同的颜色进行了标识。 其实，在滑动窗口的过程中可以发现，卷积过程的很多很多计算都是重复的。用卷积网络实现滑动窗口检验，减少了重复的计算，从而提高了效率。这样一个方法，是Sermanet等人2014年在论文[OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks]中提出来的。 YOLO算法采用滑窗检测进行目标检测，难以选取到一个可以完美匹配目标位置的，大小合适的窗口。 YOLO（You Only Look Once）算法是Redmon等人2015年在论文[You Only Look Once: Unified, Real-Time Object Detection]中提出的另一种用于目标检测的算法。 YOLO算法中，将输入的图像划分为S×S个网格（Grid Cell)，对这S×S个网格分别指定一个标签，标签的形式如前面所述： $p_c$标识该网格中的目标存在与否。为“1”则表示存在；“0”则表示不存在，且标签中其他值都无效。 $b_x$、$b_y$表示包围盒的中心坐标值，它们相对于该网格进行了归一化，也就是它们的取值范围在0到1之间； $b_h$、$b_w$表示包围盒的长度和宽度； $c_n$表示第n个假定类别存在的概率。 某个目标的中点落入某个网格，该网格就负责检测该对象。 如上面的示例中，如果将输入的图片划分为3×3个网格、需要检测的类别有3类，则每一网格部分图片的标签会是一个8维的列矩阵，最后输出结果的大小就是3×3×8。要得到这个结果，就要训练一个输入大小为100×100×3，输出大小为3×3×8的卷积神经网络。 预测出的目标位置的准确程度用IOU（Intersection Over Union）来衡量，它表示预测出的包围盒（Bounding Box）与实际边界（Ground Truth）的重叠度，也就是两个不同包围盒的交并比。如下图中所示，IOU就等于两个包围盒的交集面积（黄色部分）占两个包围盒的并集面积（绿色部分）的比率。一般可以约定一个阈值，以此判断预测的包围盒的准确与否。 使用YOLO算法进行目标检测，因为是多个网格对某些目标同时进行检测，很可能会出现同一目标被多个网格检测到，并生成多个不同的包围盒的情况，这时需要通过非极大值抑制（Non-max Suppression）来筛选出其中最佳的那个。 对于每一个网格，将通过一个置信度评分（Confidence Scores）来评判该网格检测某个目标的准确性，这个评分值为$p_c$值与各$c_n$值的乘积中的最大值，也就是说每个包围盒将分别对应一个评分值。进行非极大值抑制的步骤如下： 选取拥有最大评分值的包围盒； 分别计算该包围盒与所有其他包围盒的IOU值，将所有IOU超过预设阈值的包围盒丢弃； 重复以上过程直到不存在比当前评分值更小的包围盒。 上述算法只适用于单目标检测，也就是每个网格只能检测一个对象。要将该算法运用在多目标检测上，需要用到Anchor Boxes。在原单目标检测所用的标签中加入其他目标的标签，每个目标的标签表示形式都如上所述，一组标签即标明一个Anchor Box，则一个网格的标签中将包含多个Anchor Box，相当于存在多个用以标识不同目标的包围盒。 如上面的例子中，还是将输入的图片划分为3×3个网格且检测的类别为3类。希望同时检测人和汽车，则每个网格的标签中要含有两个Anchor Box，每一网格部分图片的标签会是一个16维的列矩阵，最后输出结果的大小就是3×3×16。 单目标检测中，图像中的目标被分配给了包含该目标的中点的那个网格；引入Anchor Box进行多目标检测，图像中的目标则被分配到了包含该目标的中点的那个网格以及具有最高IOU值的网格的Anchor Box。 R-CNNR-CNN（Region CNN)是Girshick等人2013年在论文[Rich feature hierarchies for accurate object detection and semantic segmentation]中提出的一种目标检测算法，其中提出的候选区域（Region Proposal）概念在计算机视觉领域有很大的影响力，它可以说是利用深度学习进行目标检测的开山之作。 R-CNN意为带区域的卷积网络，类似之前所述的滑窗检测算法，先用卷积网络训练一个能够准确识别目标的分类器，但这个算法试图选出一些区域为候选区域，只在这些区域也就是只在少数的窗口上运行分类器。候选区域的选取采用的是一种称为图像分割的算法。 后续有一系列的研究工作，试图改进这个算法，而出现了Fast R-CNN、Faster R-CNN算法，不过（Andrew Ng认为）这些算法在运行速度方面还是不如YOLO算法。 人脸识别人脸验证（Face Verification）和人脸识别（Face Recognition）是两个在人脸识别相关文献中被普遍提及的术语，前者一般指一个一对一问题，只需要验证输入的人脸图像等信息是否与某个已有的身份信息对应，而后者需要验证输入的人脸图像是否与多个已有的信息中的某一个匹配，是一个更为复杂的一对多问题。 在真实的应用场景中，人脸识别是一个One—Shot学习的过程，要求人脸识别系统只采集某人的一个面部样本，就能对这个人做出快速准确的识别，也就是说只用一个训练样本训练而获得准确的预测结果，这是对人脸识别的研究上所面临的挑战。 这里，One-Shot学习过程通过学习一个Similarity函数来实现。Similarity函数的表达式为：$$Similarity = d(img1, img2)$$ 它定义了输入的两幅图片之间的差异度。设置一个超参数$\\tau$，当$d(img1, img2) \\le \\tau$，则两幅图片为同一人，否则为不同。 Siamese网络 上图的示例中，将图片$x^{(1)}$、$x^{(2)}$分别输入两个相同的卷积网络中，经过全连接后不再进行Softmax，得到它们的特征向量$f(x^{(1)})$、$f(x^{(2)})$。此时Similarity函数就被定义为这两个特征向量之差的2范数：$$d(x^{(1)}, x^{(2)}) = \\mid \\mid f(x^{(1)}) - f(x^{(2)}) \\mid \\mid^2_2$$ 这种对两个不同输入运行相同的卷积网络，然后对它们的结果进行比较的神经网络，叫做Siamese网络。 利用一对相同的Siamese网络，可以将人脸验证看作二分类问题。如上图中，输入的两张图片$x^{(i)}$、$x^{(j)}$，经过卷积网络后分别得到m维的特征向量$f(x^{(i)})$、$f(x^{(j)})$，将它们输入一个逻辑回归单元，最后输出的预测结果中用1和0表示相同或不同的人。 其中对最后的输出结果$\\hat{y}$，如果使用的逻辑回归单元是sigmoid函数，表达式就会是：$$\\hat{y} = \\sigma(\\sum_{k=1}^m w_i \\mid f(x^{(i)})_k - f(x^{(j)})_k \\mid + b)$$ 以上所述内容，都来自Taigman等人2014年发表的论文[DeepFace closing the gap to human level performance]中提出的DeepFace。 Triplet损失利用神经网络实现人脸识别，想要训练出合适的参数以获得优质的人脸图像编码，需要在每次正向传播后计算Triplet损失。 Triplet损失函数的定义基于三张图片–两张同一人的不同人脸图像和一张其他人的人脸图像，它们的特征向量分别用符号A（Anchor）、P（Positive）、N（Negative）表示，如下图。 对于这三张图片，想要实现：$$\\mid \\mid f(A) - f(P) \\mid \\mid_2^2 + \\alpha &lt; \\mid \\mid f(A) - f(N) \\mid \\mid_2^2$$ 其中的$\\alpha$为间隔参数，用以防止产生无用的结果。则Triplet损失函数被定义为：$$\\mathcal{L}(A, P, N) = max(\\mid \\mid f(A) - f(P) \\mid \\mid_2^2 - \\mid \\mid f(A)- f(N) \\mid \\mid_2^2 + \\alpha, 0)$$ 式中的主要部分为A与P之差的范数减去A与N之差的范数后，再加上间隔参数$\\alpha$，因为它的值需要小于等于0，所以直接取它和0的max。 这样，训练这个神经网络就需要有大量经过特定组合的包含Anchor、Postive、Negative的图片组。且使用m个训练样本，代价函数将是：$$\\mathcal{J} = \\sum^{m}_{i=1} [\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 - \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2 + \\alpha ] $$ Triplet损失的相关内容来自Schroff等人2015年在论文[FaceNet: A unified embedding for face recognition and clustering]中提出的FaceNet，更细节可以参考论文内容。 神经风格转换神经风格迁移（Neural Style Tranfer）是将参考风格图像的风格转换到另一个输入图像中，如下图所示。 其中待转换的图片标识为C（Content），某种风格的图片为S（Style），转换后的图片为G（Generated）。 理解CNN要理解利用卷积网络实现神经风格转换的原理，首先要理解在输入图像数据后，一个深度卷积网络从中都学到了些什么。 2013年Zeiler和Fergus在论文[Visualizing and understanding convolutional networks]中提出了一种将卷积神经网络的隐藏层特征进行可视化的方法。 上图展示是一个AlexNet中的卷积、池化以及最后的归一化过程，以及实现隐藏层可视化的反卷积网络中的Unpooling、矫正以及反卷积过程。论文中将ImageNet 2012中的130万张图片作为训练集，训练结束后提取到的各个隐藏层特征如下图： 从中可以看出，浅层的隐藏单元通常学习到的是边缘、颜色等简单特征，越往深层，隐藏单元学习到的特征也越来越复杂。 实现过程实现神经风格转换，需要定义一个关于生成的图像$G$的代价函数$J(G)$，以此评判生成图像的好坏的同时，用梯度下降法最小化这个代价函数，而生成最终的图像。 $J(G)$由两个部分组成：$$J(G) = \\alpha J_{content}(C,G) + \\beta J_{style}(S,G)$$ 其中内容代价函数$J_{content}(C,G)$度量待转换的C和生成的G的相似度，风格代价函数$J_{style}(S,G)$则度量某风格的S和生成的G的相似度，用超参数$\\alpha$和$\\beta$来调整它们的权重。 将C、G分别输入一个预先训练好的卷积神经网络中，选择该网络中的某个中间层$l$，$a^{(C)[l]}$、$a^{(G)[l]}$表示C、G在该层的激活,则内容代价函数$J_{content}(C,G)$的表达式为：$$J_{content}(C,G) = \\frac{1}{2} \\mid \\mid(a^{(C)[l]} - a^{(G)[l]})\\mid \\mid^2$$ 定义风格代价函数$J_{style}(S,G)$前，首先提取出S的“风格”。通过之前的理解CNN内容，将S也输入那个预先训练好的卷积神经网络中，就可以将其所谓的“风格”定义为神经网络中某一层或者几个层中，各个通道的激活项之间的相关系数。如下图所示为网络中的某一层，假设其中前两个红、黄色通道分别检测出了下面对于颜色圈出的特征，则这两个通道的相关系数，就反映出了该图像所具有的“风格”。 选定网络中的大小为$i \\times j \\times k$的第$l$层，$a^{[l]}_{ijk}$表示k个通道的激活，则相关系数以一个Gram矩阵的形式表示为：$$\\mathcal{G}^{(S)[l]}_{kk’} = \\sum_{i=1}^{n_H^{[l]}} \\sum_{j=1}^{n_W^{[l]}} a^{(S)[l]}_{ijk} a^{(S)[l]}_{ijk’}$$ 对G，也表示出其在网络层中的相关系数：$$\\mathcal{G}^{(G)[l]}_{kk’} = \\sum_{i=1}^{n_H^{[l]}} \\sum_{j=1}^{n_W^{[l]}} a^{(G)[l]}_{ijk} a^{(G)[l]}_{ijk’}$$ 这样，第$l$层的风格代价函数表达式将是：$$J_{style}^{[l]}(S,G) = \\frac{1}{(2 n_C n_H n_W)^2} \\sum_k \\sum_{k’}(\\mathcal{G}^{(S)}_{kk’} - \\mathcal{G}^{(G)}_{kk’})^2$$ 一般将这个风格代价函数运用到每一层，可以获得更好的效果，此时的表达式为：$$J_{style}(S,G) = \\sum_{l} \\lambda^{[l]} J^{[l]}_{style}(S,G)$$ 其中$\\lambda$是一个用来设置每一层所占权重的超参数。 这样一种神经风格转换的实现方法，来自2015年Gatys等人发表的论文[A Neural Algorithm of Artistic Style]。 参考资料 吴恩达-卷积神经网络-网易云课堂 Andrew Ng-Convolutional Neural Networks-Coursera YOLO——基于回归的目标检测算法-csdn 非极大值抑制-csdn RCNN算法详解-csdn “看懂”卷积神经网-csdn 课程代码与资料-GitHub 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2018.02.12 完成初稿","link":"/2018/01/18/ML/deep_learning_8/"},{"title":"【Coursera】机器学习(1)：线性回归","text":"这里介绍机器学习的基本概念以及线性回归模型。 基本概念1959年，IBM科学家Arthur Samuel开发了一个跳棋程序。通过这个程序，塞缪尔驳倒了普罗维登斯提出的机器无法超越人类，像人类一样写代码和学习的模式。他创造了“机器学习（machine learning）”，并将它定义为“可以提供计算机能力而无需显式编程的研究领域”。 1998年，卡内基梅隆大学的Tom MitChell给出了一种更为形式化的定义：假设用P来估计计算机程序在某任务类T上的性能，若一个程序通过利用经验E在任务T上获得了性能改善，我们则称关于T和P，该程序对E进行了学习。 通常，大部分机器学习问题都可以划分为监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）两类： 监督学习：给定的数据集中已经包含了正确的输出结果，希望机器能够根据这些数据集学习一个模型，使模型能够对任意的输入，对其对应的输出做出一个好的预测。监督学习具体又可以分为： 回归（Regression）：将输入的变量映射到某个连续函数。例如，根据一些房子面积与其价格的对应数据，训练一个模型来预测某面积之下的房价： 分类（Classification）：将输入变量映射成离散的类别。例如，根据一些肿瘤大小与年龄的对应数据，训练一个模型来对良性、恶性肿瘤进行判断： 无监督学习：给定的数据集中不包含任何输出结果，希望机器通过算法自行分析而得出结果。无监督学习具体可以分为： 聚类（Clusterng）：将数据集归结为几个簇例如，将各种新闻聚合成一个个新闻专题。 非聚类（Non-clustering)例如，将鸡尾酒会上的音乐声和人声分离。 单变量线性回归了解一些基本概念后，这里通过解决如下问题，来学习机器学习中最基础的线性回归（linear regression）模型。 假如收集到如下“房价”因“房子面积”的变化而变化的数据： 要通过这些数据解决一个“房价预测”问题，即由任意给定的“房子面积”预测出相应的“房价”。显然，这是一个监督学习下的回归问题。 要解决该问题，我们就要根据已有的数据建立合适的机器学习模型。 符号约定这里首先对一些后面将反复用到的符号的含义约定如下： $m$：训练样本（training example）的数量 $x^{(i)}$：第$i$个输入变量（即“训练样本”） $y^{(i)}$：第$i$个输出变量（也称“目标变量”） $(x^{(i)},y^{(i)})$：第$i$个训练样本，当$i = 1，\\cdots , m$时称为一个训练集（training set） $X$：输入空间（input space，也称“样本空间”） $Y$：输出空间（output space，也称“标记空间”） $\\theta_i$：要通过学习得到的第$i$个参数 “房价预测”问题中，共有$m$个训练样本，输入变量$x$为“房子面积”，输出变量$y$是“真实房价”，且$x^{(1)} = 2104,y^{(1)} = 460$，$(x^{(1)}，y^{(1)})$组成第一个训练样本，$x^{(1)},\\cdots,x^{(m)}$形成样本空间$X$，$y^{(1)},\\cdots,y^{(m)}$则形成标记空间$Y$。 假设函数在解决监督学习问题，我们的目标是：给定一个训练集，希望从中学得一个由$X$到$Y$的假设函数（hypothesis function）$h(x)$，使得对于任意的输入$x$，都能通过$h(x)$准确预测出相应的输出$y$，如下图所示： 问题中，如果将假设函数简单地设定为一个线性函数：$$h_\\theta(x) = \\theta_0 + \\theta_1x $$那么我们的目标，就是根据给定的训练样本，获得假设函数中的两个参数值，而获得能过够通过房子面积预测房价的线性回归模型。 成本函数在模型的训练过程中，我们需要综合所有的训练样本，训练出表现最优的模型。对于习得的一个假设函数$h(x)$，我们用成本函数（cost function）来评估其预测的准确度，且评估的过程，是将假设函数的预测结果与对应的真实标记进行比较。均方误差（mean squared error）就是一种回归问题中常用的成本函数，其表达式为：$$E(h, D) = \\frac{1}{m} \\sum_{i=1}^m (h(x^{(i)}) - y^{(i)})^2$$ 根据成本函数的定义可知，模型预测的准确度越高，相应成本函数的值将越接近$0$。所以有了成本函数后，就有了训练模型过程中的目标–最小化成本函数。 由此，将“房价预测”问题的成本函数定义如下：$$J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$ 其中为方便之后的计算，上式中取均方误差的一半。该成本函数$J$是关于两个参数$\\theta_0$、$\\theta_1$的函数，其在空间坐标系的图像如下： 进一步画出该成本函数的等高线图（contour plot），可知一条等高线将对应一个训练过程中学习到的模型： 落在等高线包围的中心点上时，$J$将取得最小值，对应将获得最优的$h(x)$: 梯度下降通常采用梯度下降法（Gradient Denscent）来最小化成本函数，而得到假设函数中参数值。 在数学中，梯度指的是一个函数在某一点上一个向量，且该点上的方向导数沿着该向量取得最大值，即在该点处，函数沿着该向量的方向变化最快。由此，只要给成本函数预设一个初始值，使其沿梯度方向迭代下降，便能一步步将其最小化。更为直观的解释如下： 假设某个成本函数的图像如上，在图像上任取一点后，使其不断沿着梯度方向下降，最后定能落入一个局部的最小值上。注意到，当存在多个极小值时，选取的初始位置不同，梯度下降后将得到多种结果，这里暂且不考虑此问题。 要得到所谓的梯度，其实就是对函数进行求导。成本函数往往是个多元函数，对其求导的过程是求取其关于各参数的偏导数。对“房价预测”问题的成本函数$J$，就有其关于$\\theta_0$的偏导数$\\frac{\\partial}{\\partial \\theta_0} J$以及关于$\\theta_1$的偏导数$\\frac{\\partial}{\\partial \\theta_1} J$。 实现梯度下降的具体算法为：通过下式迭代更新参数值，直到它们收敛为止：$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_n) \\ \\ \\ j = 0, \\cdots, n$$ 其中的“:=”意为“赋值”，$\\alpha$是称为“学习率（learning rate）”，用以控制梯度下降时的“步伐”大小，它的预设值不能过大或小，训练过程中可以将它进行动态调整，使得越接近最小值点时下降的“步伐”越小，以准确地落到最小值点上。要注意，一次迭代过程中各参数的值是同时进行更新的。 例如“房价预测”问题中，$n$的值为1，且成本函数是一个凸函数，全局只有一个极小值点。训练时参数的更新过程将如下：$$temp0 := \\theta_0 - \\alpha \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1)$$ $$temp1 := \\theta_1 - \\alpha \\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1)$$ $$\\theta_0 := temp0$$ $$\\theta_1 := temp1$$其中，由初等函数的导数可得：$$\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})$$ $$\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)} $$训练该线性回归模型的过程，将如下动图所示： 注意到，使用梯度下降法训练该模型时，每一次学习过程都使用了全部的训练数据。采用这种方式进行梯度下降的过程，又称“批梯度下降（batch gredient descent）”。 训练结束后，能够根据“房子面积”而预测“房价”的一个简单的线性回归模型就建立好了。 多变量线性回归在解决前面的“房价预测”问题时，只由“房子面积”这一种特征（feature）来预测“房价”，建立的是一个单变量线性回归模型。然而我们直到，“房价”不单单因“房子面积”而异，往往还会受到“楼层数量”、“房间数量”等其他特征所影响。 考虑更多影响“房价”的房子特征，假如收集到了如上数据。我们将根据这些数据，建立一个多变量线性回归模型。 符号约定在原单变量模型的基础上，继续约定如下符号的含义： $n$：特征的数量 $x^{(i)}_j$：第$i$个训练样本的第$j$种特征的值 对上表中的训练数据，“房子面积”用$x_1$表示，“房间数量”用$x_2$表示，“楼层数”用$x_3$“表示，“房龄”则用$x_4$表示。每个训练样本中包含了$n=4$种特征，要使用一个由$x_j$组成的$n$维向量进行表示，如第2个训练样本表示为$x^{(2)}= \\begin{bmatrix} x^{(2)}_1 \\\\ x^{(2)}_2 \\\\ x^{(2)}_3 \\\\ x^{(2)}_4 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1416 \\\\ 3 \\\\ 2 \\\\ 40 \\\\ \\end{bmatrix}$。 特征缩放在训练机器学习模型的实际过程中，我们需要掌握一些技巧。特征放缩（feature scaling）便是其中之一，它可以有效地使梯度下降的过程变得更快。 所谓的特征放缩，是分别将训练数据中各个特征的值放缩到一个较小的统一区间内，该范围通常取$[-1,1]$，即使得$-1 \\leq x_i \\leq 1$。 如上图所示，进行特征放缩后，成本函数的等高线图将从椭圆变成圆形，而加快梯度下降的速度。 通常采用下式进行特征缩放：$$x_i := \\frac{x_i}{max(x_i)-min(x_i)} $$类似的还有均值归一化（mean normalization），它能使放缩后的均值为$0$，其表达式为：$$x_i := \\frac{x_i - mean(x_i)}{max(x_i)-min(x_i)} $$ 模型表示既然考虑了多种特征，那么假设函数也会变得更为复杂。多变量线性回归中，我们将假设函数设为：$$h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n$$ 上式看成两个$n+1$维列向量$x$、$\\theta$间的矩阵运算，可将它更为简洁地表示为：$$ h_\\theta(x) = \\theta^{T}x = [\\theta_0, \\theta_1, \\cdots , \\theta_n] \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\cdots \\\\ x_n \\\\ \\end{bmatrix} $$ 其中$x_0 = 1$。 由此，“房价预测”问题的假设函数为：$$h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_4 + \\theta_4x_4$$ $$ = [\\theta_0, \\theta_1, \\theta_2, \\theta_3, \\theta_4] \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ \\end{bmatrix} $$ 正规方程依然采用均方误差作为成本函数并使用梯度下降法进行模型训练的话，参数更新的过程将为：迭代运算下式，直到个参数值收敛为止: $$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1,\\cdots, \\theta_n) \\ \\ \\ j = 0, \\cdots, n $$ 该方法需要经过多次迭代运算，才能得到最优的参数值。对于线性回归模型，另外可采用正规方程法（normal equation）进行参数学习，而一次性计算出模型中各参数的最优值。 前面的式子中，成本函数的具体表达式为：$$ J(\\theta_0, \\theta_1,\\cdots, \\theta_n) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})$$ 要最小化该式，即求出使$J(\\theta_0, \\theta_1,\\cdots, \\theta_n)$最小的$\\theta_0, \\theta_1,\\cdots, \\theta_n$值，就要令函数$J$关于各参数$\\theta$的偏导等于$0$，如此得到的$\\theta$将使$J$取得最小值。 具体地，将有：$$\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1,\\cdots, \\theta_n) = \\frac{1}{m} \\sum_{i=1}^m ((h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}0) = 0$$ $$\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1,\\cdots, \\theta_n) = \\frac{1}{m} \\sum{i=1}^m ((h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}1) = 0$$ $$\\vdots$$ $$\\frac{\\partial}{\\partial \\theta_n} J(\\theta_0, \\theta_1,\\cdots, \\theta_n) = \\frac{1}{m} \\sum{i=1}^m ((h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}_n) = 0$$ 根据前面所述，有下式：$$x^{(i)}= \\begin{bmatrix} x^{(i)}_0 \\\\ x^{(i)}_1 \\\\ \\cdots \\\\ x^{(i)}_n \\\\ \\end{bmatrix} \\ \\ \\ i = 1,\\cdots,m $$ $$ y = \\begin{bmatrix} y^{(1)} \\\\ x^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\\\ \\end{bmatrix} ,\\ \\ \\ \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\\\ \\end{bmatrix} $$ $$h_\\theta(x^{(i)}) = \\theta^T(x^{(i)}) = (x^{(i)})^T\\theta$$ 此外，约定一个大小为$m \\times (n+1)$的矩阵$X$：$$X= \\begin{bmatrix} (x^{(1)})^T \\\\ (x^{(2)})^T \\\\ \\cdots \\\\ (x^{(m)})^T \\\\ \\end{bmatrix} = \\begin{bmatrix} x^{(1)}_0 &amp; x^{(1)}_1 &amp; \\cdots &amp; x^{(1)}_n\\\\ x^{(2)}_0 &amp; x^{(2)}_1 &amp; \\cdots &amp; x^{(2)}_n \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x^{(m)}_0 &amp; x^{(m)}_1 &amp; \\cdots &amp; x^{(m)}_n \\\\ \\end{bmatrix}$$ 由此，可将最小化成本函数时的累加过程转换为矩阵运算，例如有：$$\\frac{1}{m} \\sum_{i=1}^m ((h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}_0) = \\frac{1}{m}[x^{(1)}0, x^{(2)}0, \\cdots, x^{(m)}_0] \\begin{bmatrix} h_\\theta(x^{(1)}) - y^{(1)} \\\\ h{\\theta}(x^{(2)}) - y^{(2)} \\\\ \\vdots \\\\ h{\\theta}(x^{(m)}) - y^{(m)} \\\\ \\end{bmatrix} $$ 而：$$\\begin{bmatrix} h_{\\theta}(x^{(1)}) - y^{(1)} \\\\ h_{\\theta}(x^{(2)}) - y^{(2)} \\\\ \\vdots \\\\ h_{\\theta}(x^{(m)}) - y^{(m)} \\\\ \\end{bmatrix} = \\begin{bmatrix} (x^{(1)})^T \\\\ (x^{(2)})^T \\\\ \\vdots \\\\ (x^{(m)})^T\\\\ \\end{bmatrix} \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\\\ \\end{bmatrix} - \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\\\ \\end{bmatrix}$$ $$ = X\\theta - y$$ 则有：$$\\frac{1}{m} \\sum_{i=1}^m ((h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}_0) = \\frac{1}{m}[x^{(1)}0, x^{(2)}0, \\cdots, x^{(m)}_0] (X\\theta - y) = 0$$ $$\\frac{1}{m} \\sum{i=1}^m ((h{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}_1) = \\frac{1}{m}[x^{(1)}1, x^{(2)}1, \\cdots, x^{(m)}_1] (X\\theta - y) = 0$$ $$\\vdots $$ $$\\frac{1}{m} \\sum{i=1}^m ((h{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}_n) = \\frac{1}{m}[x^{(1)}_n, x^{(2)}_n, \\cdots, x^{(m)}_n] (X\\theta - y) = 0$$ 将上式进一步矩阵化有：$$\\frac{1}{m}\\begin{bmatrix} x^{(1)}_0 &amp; x^{(1)}_1 &amp; \\cdots &amp; x^{(1)}_n\\\\ x^{(2)}_0 &amp; x^{(2)}_1 &amp; \\cdots &amp; x^{(2)}_n \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x^{(m)}_0 &amp; x^{(m)}_1 &amp; \\cdots &amp; x^{(m)}_n \\\\ \\end{bmatrix}^T(X\\theta -y) = \\frac{1}{m}X^T(X\\theta - y) = 0$$ 并进一步整理上式，就能得到可一步计算出线性回归模型中$\\theta$值的正规方程：$$ \\theta = (X^TX)^{-1}X^Ty $$ 值得一提的时，上式中$X^TX$为奇异矩阵或非方阵时，它将不存在逆矩阵，造成该问题的原因可能是训练样本的特征类型中存在冗余特征，也可能是特征数量$n$大于等于训练样本数量$m$，要解决该问题可以某些特征或将样本进行正则化（regularization）。但在MATLAB中，始终可以用pinv()求得$X^TX$的逆矩阵或伪逆矩阵。 在“房价预测”中，有：$$X= \\begin{bmatrix} 1 &amp; 2104 &amp; 5 &amp; 1 &amp; 45 \\\\ 1 &amp; 1406 &amp; 3 &amp; 2 &amp; 40 \\\\ 1 &amp; 1534 &amp; 3 &amp; 2 &amp; 30 \\\\ 1 &amp; 852 &amp; 2 &amp; 1 &amp; 36 \\\\ \\end{bmatrix} y = \\begin{bmatrix} 460 \\\\ 232 \\\\ 315 \\\\ 178 \\\\ \\end{bmatrix}$$ 据此，就可以通过简单的矩阵运算而得到$\\theta$的值。 采用梯度下降法训练模型时： 需要调参（$alpha$） 需要多次迭代计算 当样本的特征（$n$）很多时也能保持较好的性能 除线性回归模型外还适用于逻辑回归等其他模型 而对正规方程： 不需要调参 一次性完成计算 当$n$比较大时，$(X^TX)^{-1}$的运算量较大，性能较差 只适用于线性回归模型因此，在训练模型的过程中要根据实际的情况，选择学习参数的方法。 编程作业见如下github链接，程序仅供参考： machine_learning-ex1-python 参考资料 Andrew Ng-Machine Learning-Coursera 吴恩达-机器学习-网易云课堂 机器学习简史-CSDN Normal equation公式推导-CSDN 掰开揉碎推导Normal Equation-知乎 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Machine Learning课程及上述书籍、博客资料，版权归各作者所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2019.03.18 完成初稿","link":"/2019/03/02/ML/machine_learning_1/"},{"title":"【Coursera】深度学习(11)：序列模型","text":"采用循环神经网络能够建立各种各样的序列模型（Sequence Model）。加入一些注意力机制，能够使这些序列模型更加强大。 Seq2Seq模型2014年Cho等人在论文[Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation]中首次提出了Seq2Seq（Sequence-to-Sequence）模型。从机器翻译到语音识别，这种模型能够在各种序列到序列的转换问题中得到应用。 一个Seq2Seq模型中可分成编码器（Encoder）和译码器（Decoder）两部分，它们通常是两个不同的神经网络。如下图是谷歌机器翻译团队的Sutskever等人2014年在论文[Sequence to Sequence Learning with Neural Networks]中提出的机器翻译模型： 上面法语中的词依次输入作为编码器的RNN的时间步中，这个RNN可以是GRU或LSTM。将编码器的最后输出作为译码器的输入，译码器也是一个RNN，训练译码器作出正确的翻译结果。 这种Enconder-Decoder的结构，也可以应用在图像标注（Image Caption）上。2014年百度研究所的毛俊骅等人在论文[DDeep Captioning with Multimodal Recurrent Neural Networks (m-RNN)]中提出了如下图中的结构： 上面将图像输入了一个作为编码器的AlexNet结构的CNN中，最后的Softmax换成一个RNN作为译码器，训练网络输出图像的标注结果。 另外两篇论文[Show and Tell: A Neural Image Caption Generator]和 [Deep Visual-Semantic Alignments for Generating Image Descriptions]中， 也提到了这种结构。 机器翻译用到的Seq2Seq模型中，译码器所做的工作与前面讲过的语言模型的采样过程类似，只不过在机器翻译中，用编码器的输出代替语言模型中的$0$作为译码器中第一个时间步的输入，如下图所示： 换句话说，机器翻译其实是一个输入为法语的条件下，希望以对应的英语作为输出的语言模型，所以机器翻译的过程也就相当于建立一个**条件语言模型（Conditional Language Model)**的过程。 采用大量的数据训练好一个机器翻译系统后，对于一个相同的句子，由于译码器进行的是随机采样过程，输出的可能会是多种或好或坏的结果： 所以对训练好的机器翻译系统，还需要加入一些算法，使其总是输出最好的翻译结果。 考虑直接使用CS中的贪心搜索（Greedy Search）算法，让译码器中每个时间步都取概率最大的那个词，得到的翻译结果还是会不尽人意。 集束搜索步骤Seq2Seq模型中，译码器的输出结果总是在RNN中采样后得到，造成模型训练完毕后，得到测试的结果参差不齐，集束搜索（Beam Search）算法能很好地解决这个问题。这里还是以机器翻译的例子来说明这种算法。 将集束搜索算法运用到机器翻译系统的第一步，是设定一个束长（Bean Width）$B$，它代表了译码器中每个时间步的预选单词数量。如下图中$B = 3$，则将第一个时间步中预测出的概率最大的$3$个词作为首词的预选词，同时保存它们的概率值大小$p(y^{\\langle 1 \\rangle} \\mid x)$: 如果第一步得到的三个预选词分别为“in”、“jane”和“September”，如下图所示，则第二步中，分别将三个预选词作为第一个时间步的预测结果$y^{\\langle 1 \\rangle}$输入第二个时间步，得到预测结果${\\hat y}^{\\langle 2 \\rangle}$，也就是条件概率值$p({\\hat y}^{\\langle 2 \\rangle} \\mid x, y^{\\langle 1 \\rangle})$: 根据条件概率公式，有：$$p(y^{\\langle 1 \\rangle}, {\\hat y}^{\\langle 2 \\rangle} \\mid x) = p(y^{\\langle 1 \\rangle} \\mid x) p({\\hat y}^{\\langle 2 \\rangle} \\mid x, y^{\\langle 1 \\rangle}) $$ 分别以三个首词预选词作为$y^{\\langle 1 \\rangle}$进行计算，将得到$30000$个$p(y^{\\langle 1 \\rangle}, {\\hat y}^{\\langle 2 \\rangle} \\mid x)$。之后还是取其中概率值最大的$B = 3$个，作为对应首词条件下的第二个词的预选词。比如第二个词的预选词分别是“in”为首词条件下的“September”，”jane”为首词条件下的“is”和“visits”，这样的话首词的预选词就只剩下了“in”和“jane”而排除了“September”。后面的过程以此类推，最后将输出一个最优的翻译结果。 优化总的来说，集束搜索算法所做的工作就是找出符合以下公式的结果：$$ arg\\ max\\ \\prod^{T_y}_{t = 1} p(y^{\\langle t \\rangle} \\mid x, y^{\\langle 1 \\rangle},…,y^{\\langle t-1 \\rangle})$$然而概率值都是小于$1$的值，多个概率值相乘后的结果的将会是一个极小的浮点值，累积到最后的效果不明显且在一般的计算机上达不到这样的计算精度。改进的方法，是取上式的$log$值并进行标准化：$$ arg\\ max\\ \\frac{1}{T^{\\alpha}_y} \\sum^{T_y}_{t = 1} log \\ p(y^{\\langle t \\rangle} \\mid x, y^{\\langle 1 \\rangle},…,y^{\\langle t-1 \\rangle})$$其中的$\\alpha$是一个需要根据实际情况进行调节的超参数。 与CS中的精确查找算法–广度优先查找（Breadth First Search，BFS）、深度优先查找（Depth First Search，DFS）算法不同，集束搜索算法运行的速度虽然很快，但是并不保证能够精确地找到满足$arg\\ max\\ p(y \\mid x)$的结果。 关于束长$B$的取值，较大的$B$值意味着同时考虑了更多的可能，最后的结果也可能会更好，但会带来巨大的计算成本；较小的$B$值减轻了计算成本的同时，也可能会使最后的结果变得糟糕。通常情况下，$B$值取一个$10$以下地值较为合适。还是要根据实际的应用场景，适当地选取。要注意的是，当$B = 1$时，这种算法就和贪心搜索算法没什么两样了。 错误分析在前面的结构化机器学习项目课程中，已经了解过错误分析。集束搜索是一种启发式（Heuristic）搜索算法，它的输出结果不是总为最优的。结合Seq2Seq模型与集束搜索算法构建的机器翻译等系统出错时，差错到底是出现在前者的RNN还是后者的算法中，还是需要通过一些手段，来进行错误分析。 例如对图中的法语，发现机器翻译的结果$\\hat y$与专业的人工翻译的结果$y^{*}$存在较大的差别。要找到错误的根源，首先将翻译没有差别的一部分“Jane visits Africa”分别作为译码器中其三个时间步的输入，得到第四个时间步的输出为“in”的概率$p(y^{*} \\mid x)$和“last”的概率$p(\\hat{y} \\mid x)$，比较它们的大小并分析： 若$p(y^{*} \\mid x) \\gt p(\\hat{y} \\mid x)$，说明是集束搜索时出现错误，没有选择到概率最大的词； 若$p(y^{*} \\mid x) \\le p(\\hat{y} \\mid x)$，说明是RNN模型的表现不够好，预测的第四个词为“in”的概率小于“last”。 分析过程中，可以建立一个如上所示的表格，提高错误查找效率。 机器翻译评估：BLEU指标BLEU（Bilingual Evaluation Understudy）是一种用来评估机器翻译质量的指标，它早在2002年由Papineni等人在论文[BLEU: a Method for Automatic Evaluation of Machine Translation]中提出。BLEU的设计思想与评判机器翻译好坏的思想是一致的：机器翻译的结果越接近专业的人工翻译，则评估的分值越高。 最原始的BLEU算法很简单：统计机器翻译结果中的每个单词在参考翻译中出现的次数作为分子，机器翻译结果的总词数作为分母。然而这样得到结果很容易出现错误。 如上图的例子中，机器翻译得到的结果是$7$个“the”，分母为$7$，每个“the”都在参考翻译中有出现，分子为$7$，得到翻译精确度为$1.0$，这显然是不对的。改进这种算法，将参考翻译中“the”出现的最高次数作为分子，机器翻译结果中“the”的出现次数作为分母，可得精度为$\\frac{2}{7}$。 上面的方法是一个词一个词进行统计，这种以一个单词为单位的集合统称为uni-gram（一元组）。以uni-gram统计得到的精度$p_1$体现了翻译的充分性，也就是逐字逐句地翻译能力。 两个单词为单位的集合则称为bi-gram（二元组），例如对以上机器翻译结果（$count$）及参考翻译（$count_{clip}$）以二元组统计有： bi-gram $count$ $count_{clip}$ the cat 2 1 cat the 1 0 cat on 1 1 on the 1 1 the mat 1 1 Count 6 4 根据上表，可得到机器翻译精度为$\\frac{4}{6}$。 以此类推，以n个单词为单位的集合为n-gram（多元组），采用n-gram统计的翻译精度$p_n$的计算公式为：$$p_n = \\frac{\\sum_{\\text{n-gram} \\in \\hat{y}} count_{clip}(\\text{n-gram})}{\\sum_{\\text{n-gram} \\in \\hat{y}} count(\\text{n-gram})}$$ 以n-gram统计得到的$p_n$体现了翻译的流畅度。将uni-gram下的$p_1$到n-gram下的$p_n$组合起来，对这$N$个值进行几何加权平均得到：$$p_{ave}=exp(\\frac{1}{N}\\sum_{i=1}^{N}log^{p_{n}})$$ 此外，注意到采用n-gram时，机器翻译的结果在比参考翻译短的情况下，很容易得到较大的精度值。改进的方法是设置一个最佳匹配长度（Best Match Length），机器翻译的结果未达到该最佳匹配长度时，则需要接受简短惩罚（Brevity Penalty，BP）：$$BP = \\begin{cases} 1, &amp; \\text{(MT_length $\\ge$ BM_length)} \\\\ exp(1 - \\frac{\\text{MT_length}}{\\text{BM_length}}), &amp; \\text{(MT_length $\\lt$ BM_length)} \\end{cases}$$最后得到BLEU指标为：$$ BLEU = BP \\times exp(\\frac{1}{N}\\sum_{i=1}^{N}log^{p_{n}}) $$ 注意力模型人工翻译一大段文字时，一般都是阅读其中的一小部分后翻译出这一部分，在一小段时间里注意力只能集中在一小段文字上，而很难做到把整段读完后一口气翻译出来。用Seq2Seq模型构建的机器翻译系统中，输出结果的BLEU评分会随着输入序列长度的增加而下降，其中的道理就和这个差不多。 2014年Bahdanau等人在论文[Neural Machine Translation by Jointly Learning to Align and Translate]中提出了注意力模型（Attention Model）。最初只是用这种模型对机器翻译作出改进，之后其思想也在其他领域也得到了广泛应用，并成为深度学习领域最有影响力的思想之一。 注意力模型中，网络的示例结构如下所示： 底层是一个双向循环神经网络，需要处理的序列作为它的输入。该网络中每一个时间步的激活$a^{\\langle t’ \\rangle}$中，都包含前向传播产生的和反向传播产生的激活：$$a^{\\langle t’ \\rangle} = ({\\overrightarrow a}^{\\langle t’ \\rangle}, {\\overleftarrow a}^{\\langle t’ \\rangle})$$ 顶层是一个“多对多”结构的循环神经网络，第$t$个时间步以该网络中前一个时间步的激活$s^{\\langle t-1 \\rangle}$、输出$y^{\\langle t-1 \\rangle}$以及底层的BRNN中多个时间步的激活$c$作为输入。对第$t$个时间步的输入$c$有：$$c = \\sum_{t’} {\\alpha}^{\\langle t,t’ \\rangle}a^{\\langle t’ \\rangle} $$ 其中的参数${\\alpha}^{\\langle t,t’ \\rangle}$意味着顶层RNN中，第$t$个时间步输出的$y^{\\langle t \\rangle}$中，把多少“注意力”放在了底层BRNN的第$t’$个时间步的激活$a^{\\langle t’ \\rangle}$上。它总有：$$\\sum_{t’} {\\alpha}^{\\langle t,t’ \\rangle} = 1$$ 为确保参数${\\alpha}^{\\langle t,t’ \\rangle}$满足上式，常用Softmax单元来计算顶层RNN的第$t$个时间步对底层BRNN的第$t’$个时间步的激活的“注意力”：$${\\alpha}^{\\langle t,t’ \\rangle} = \\frac{exp(e^{\\langle t,t’ \\rangle})}{\\sum_{t’=1}^{T_x} exp(e^{\\langle t,t’ \\rangle})} $$其中的$e^{\\langle t,t’ \\rangle}$由顶层RNN的激活$s^{\\langle t - 1 \\rangle}$和底层BRNN的激活$a^{\\langle t’ \\rangle}$一起输入一个隐藏层中得到的，因为$e^{\\langle t,t’ \\rangle}$也就是${\\alpha}^{\\langle t,t’ \\rangle}$的值明显与$s^{\\langle t \\rangle}$、$a^{\\langle t’ \\rangle}$有关，由于$s^{\\langle t \\rangle}$此时还是未知量，则取上一层的激活$s^{\\langle t-1\\rangle}$。在无法获知$s^{\\langle t-1 \\rangle}$、$a^{\\langle t’ \\rangle}$与$e^{\\langle t,t’ \\rangle}$之间的关系下，那就用一个神经网络来进行学习，如下图所示： 要注意的是，该模型的运算成本将达到$N^2$。此外，在2015年Xu等人发表的论文[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention]中，这种模型也被应用到了图像标注中。 应用语音识别在语音识别中，要做的是将输入的一段语音$x$转换为一段文字副本作为输出。曾经的语音识别系统都是采用人工设计出的音素（Phonemes）识别单元来构建，音素指的是一种语言中能区别两个词的最小语音单位。现在有了端对端深度学习，已经完美没有必要采用这种识别音素的方法实现语音识别。 采用深度学习方法训练语音识别系统的前提条件是拥有足够庞大的训练数据集。在学术界的研究中，3000小时的长度被认为是训练一个语音识别系统时，需要的较为合理的音频数据大小。而训练商用级别的语音识别系统，需要超过一万小时甚至十万小时以上的音频数据。 语音识别系统可以采用注意力模型来构建： 2006年Graves等人在论文[Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks]中提出了一种名为CTC（Connectionist Temporal Classification）损失函数计算方法，给语音识别系统的训练过程带来很大帮助。 由于输入的是音频数据，采用RNN建立的语音识别系统中将包含多个时间步，且整个网络中输出的数量往往是小于输入的数量的，也就是说不是每一个时间步都有对于的输出。而CTC的主要优点，是可对没有对齐的数据进行自动对齐。 如上图中，以一句意为图中下面的句子，长度为10s频率为100Hz的语音作为输入，则这段语音序列可分为1000个部分，分别输入RNN的时间步中，而RNN可能不会将1000个作为输出。 CTC损失计算方法允许RNN输出一个如图中所示的结果，允许以“空白（Blank）”作为输出的同时，也会识别出词之间存在的“空格（Space）”标记，CTC还将把未被“空白”分隔的重复字符折叠起来。 关于CTC的更多细节详见论文内容。 触发词检测触发词检测（Trigger Word Detection）现在已经被应用在各种语音助手以及智能音箱上。例如在Windows 10上能够设置微软小娜用指令“你好，小娜”进行唤醒，安卓手机上的Google Assistant则可以通过“OK，Google”唤醒。 想要训练一个触发词检测系统，同样需要有大量的标记好的训练数据。使用RNN训练语音识别系统实现触发词词检测的功能时，可以进行如下图所示的工作： 在以训练的语音数据中输入RNN中，将触发词结束后的一小段序列的标签设置为“$1$”，以此训练模型对触发词的检测。 参考资料 吴恩达-序列模型-网易云课堂 Andrew Ng-Sequence Model-Coursera sequence to sequence model小记-知乎专栏 seq2seq中的beam search算法过程-知乎专栏 机器翻译自动评估-BLEU算法详解 深度学习方法：自然语言处理中的Attention Model注意力模型-csdn 白话CTC算法讲解-csdn 课程代码与资料-GitHub 注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。 更新历史： 2018.03.11 完成初稿","link":"/2018/03/09/ML/deep_learning_11/"},{"title":"《C++ Primer》笔记","text":"地址：https://hugsy.top/cpp/#/cpp_primer/ 关于本文档是《C++ Primer 中文版（第 5 版）》的学习笔记，是基于applenod等人整理的学习仓库Cpp_Primer_Practice中已有的笔记，增删、整理而成，在此感谢他们的分享。 文档中的内容大都摘录自原书，算得上是整本书的浓缩版，建议配合原书使用。 如发现问题欢迎指出~ 参考 Cpp_Primer_Practice C++ Primer 中文版（第 5 版） 许可本文档由docsify动态生成，引用各种内容均来源于网络及上述参考中的各种资料，如有侵权，请及时联系删除。 文章全部采用CC BY-NC-SA 4.0许可协议，转载请注明出处。","link":"/2021/09/06/CS/cpp_primer_note/"},{"title":"【LeetCode】1.两数之和","text":"两数之和 描述给定一个整数数组nums和一个整数目标值target，请你在该数组中找出和为目标值target的那两个整数，并返回它们的数组下标。 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。 你可以按任意顺序返回答案。 示例 1： 输入：nums = [2,7,11,15], target = 9输出：[0,1]解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。 示例 2： 输入：nums = [3,2,4], target = 6输出：[1,2] 示例 3： 输入：nums = [3,3], target = 6输出：[0,1] 提示： 2 &lt;= nums.length &lt;= 104-109 &lt;= nums[i] &lt;= 109-109 &lt;= target &lt;= 109只会存在一个有效答案 进阶：你可以想出一个时间复杂度小于 O(n2) 的算法吗？ 解答 暴力破解 12345678vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) { for (int i = 0; i &lt; nums.size(); ++i) { for (int j = i + 1; j &lt; nums.size(); ++j) { if (target - nums[i] - nums[j] == 0) return {i, j}; } } return {};} 使用哈希表 123456789101112vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) { std::unordered_map&lt;int, int&gt; map; for (int i = 0; i &lt; nums.size(); ++i) { auto it = map.find(target - nums[i]); if (it == map.end()) { map[nums[i]] = i; } else { return {it-&gt;second, i}; } } return {};} 更新历史： 2022.02.07 完成初稿","link":"/2022/02/07/CS/leetcode_1/"}],"tags":[{"name":"CS","slug":"CS","link":"/tags/CS/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"EE","slug":"EE","link":"/tags/EE/"},{"name":"MCU","slug":"MCU","link":"/tags/MCU/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"Coursera","slug":"Coursera","link":"/tags/Coursera/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"Android","slug":"Android","link":"/tags/Android/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/tags/TensorFlow/"},{"name":"CPP","slug":"CPP","link":"/tags/CPP/"},{"name":"LeetCode","slug":"LeetCode","link":"/tags/LeetCode/"}],"categories":[{"name":"Docs","slug":"Docs","link":"/categories/Docs/"},{"name":"Notes","slug":"Notes","link":"/categories/Notes/"},{"name":"Tutorials","slug":"Tutorials","link":"/categories/Tutorials/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"Projects","slug":"Projects","link":"/categories/Projects/"}]}