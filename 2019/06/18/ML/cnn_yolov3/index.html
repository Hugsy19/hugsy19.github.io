<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>YOLOv3模型详解 - Cornfield Chase</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#B481BB"><meta name="application-name" content="Hugsy&#039;s Blog"><meta name="msapplication-TileImage" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><meta name="msapplication-TileColor" content="#B481BB"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hugsy&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="144x144" href="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><meta name="description" content="2015年华盛顿大学的Joseph Redmon等人提出的YOLO算法中，将目标检测直接作为回归问题来解决，而使用一个一体化的CNN实现了端到端（End to End）检测，为目标检测问题提供了另外一种解决思路的同时，基于深度学习的目标检测算法也自此有了单步和多步之分。到2018年，YOLO已经更新到了v3版本，这里对该模型进行分析。"><meta property="og:type" content="blog"><meta property="og:title" content="YOLOv3模型详解"><meta property="og:url" content="https://hugsy.top/2019/06/18/ML/cnn_yolov3/"><meta property="og:site_name" content="Cornfield Chase"><meta property="og:description" content="2015年华盛顿大学的Joseph Redmon等人提出的YOLO算法中，将目标检测直接作为回归问题来解决，而使用一个一体化的CNN实现了端到端（End to End）检测，为目标检测问题提供了另外一种解决思路的同时，基于深度学习的目标检测算法也自此有了单步和多步之分。到2018年，YOLO已经更新到了v3版本，这里对该模型进行分析。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411114709.png"><meta property="article:published_time" content="2019-06-17T16:00:00.000Z"><meta property="article:modified_time" content="2021-04-11T08:45:52.197Z"><meta property="article:author" content="Hugsy"><meta property="article:tag" content="ML"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411114709.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hugsy.top/2019/06/18/ML/cnn_yolov3/"},"headline":"YOLOv3模型详解","image":["https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411114709.png"],"datePublished":"2019-06-17T16:00:00.000Z","dateModified":"2021-04-11T08:45:52.197Z","author":{"@type":"Person","name":"Hugsy"},"description":"2015年华盛顿大学的Joseph Redmon等人提出的YOLO算法中，将目标检测直接作为回归问题来解决，而使用一个一体化的CNN实现了端到端（End to End）检测，为目标检测问题提供了另外一种解决思路的同时，基于深度学习的目标检测算法也自此有了单步和多步之分。到2018年，YOLO已经更新到了v3版本，这里对该模型进行分析。"}</script><link rel="canonical" href="https://hugsy.top/2019/06/18/ML/cnn_yolov3/"><link rel="alternate" href="/atom.xml" title="Cornfield Chase" type="application/atom+xml"><link rel="icon" href="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?7b65ce26b5ae8dae153d7b4d53214ba4";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/logo.svg" alt="Cornfield Chase" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/tags/CS/">CS</a><a class="navbar-item" href="/tags/EE/">EE</a><a class="navbar-item" href="/tags/ML/">ML</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411114709.png" alt="YOLOv3模型详解"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-06-17T16:00:00.000Z" title="6/18/2019, 12:00:00 AM">2019-06-18</time>发表</span><span class="level-item"><time dateTime="2021-04-11T08:45:52.197Z" title="4/11/2021, 4:45:52 PM">2021-04-11</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></span><span class="level-item">21 分钟读完 (大约3205个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">YOLOv3模型详解</h1><div class="content"><p>2015年华盛顿大学的Joseph Redmon等人提出的YOLO算法中，将目标检测直接作为回归问题来解决，而使用一个一体化的CNN实现了端到端（End to End）检测，为目标检测问题提供了另外一种解决思路的同时，基于深度学习的目标检测算法也自此有了单步和多步之分。到2018年，YOLO已经更新到了v3版本，这里对该模型进行分析。</p>
<span id="more"></span>

<h2 id="主干架构"><a href="#主干架构" class="headerlink" title="主干架构"></a>主干架构</h2><p>YOLOv3中用来对输入进行特征提取的主干架构为Darknet-53，它是YOLOv2中用到的Darknet-19的升级版本。正如其名，Darknet-53共包含53个卷积层，该网络在Darknet-19的基础上进一步引入了更多的残差结构，网络层数量虽然只有ResNet-101的一半，但它在性能上能够媲美ResNet-152的同时，运算速度也远超这两者。整个Darknet-53的网络结构如下表。</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411114755.png" alt="Darknet-53"></p>
<p>Darknet-53中主要包含卷积层（Convolutional）和残差块（Residual）这两种网络层。在每个卷积层中，使用各种卷积核完成卷积运算后，都进行了一次批标准化，之后再使用Leaky ReLU函数进行激活；表格中的各个方框即代表一类残差块，方框右边标识的是相应残差块的数量，每个残差块中均包含两个卷积层。它们的具体结构如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411114811.png" alt="Convolutional和Residual具体结构"></p>
<p>卷积过程中，除了一直使用$3\times3$大小的卷积核来提取特征外，还延续了Darknet-19中使用$1\times1$大小的卷积核来增加特征图维度的思想，两类卷积核的数量都随着网络层数的增加而呈$2$的幂次方增长。网络中没有引入全局平均池化层，而采用简单的增大卷积步幅策略来压缩特征。每次将卷积步幅从$1$增加$2$，特征图的大小就会成原来的一半，经过$5$次这样的下采样（Subsampled）操作后，最后输出的特征图大小被压缩到$13\times13$，即输入的$1/32$。</p>
<h2 id="多尺寸预测"><a href="#多尺寸预测" class="headerlink" title="多尺寸预测"></a>多尺寸预测</h2><p>利用主干结构提取到输入的特征图后，YOLOv3模型的后续检测流程下图：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411114827.png" alt="YOLOv3模型结构图"></p>
<p>图中的卷积层（Convolutional）与主干架构中用到的卷积层结构完全一致，每个卷积集（Convolutional Set）内则包含了$5$个这样的卷积层。在后续的处理过程中，用这些卷积层来进一步提取特征，以得到更高层次的抽象特征。卷积集的具体结构如下图：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411114840.png" alt="Convolutional Set"></p>
<p>在输入的尺寸为$416\times416$的情况下，Darknet-53的第三、四、五种残差块中，最后将分别提取到$52\times52$、$26\times26$、$13\times13$这三种尺寸的特征图。YOLOv3借鉴了FPN中的思想，使用了这三种大小的特征图来分别预测不同尺寸的包围盒。对于第五种残差块最后输出的大小为$13\times13$的特征图，由于该特征图中的各网格（像素点）具有最大（$32\times32$）的感受野，所以该特征图适合用来检测图像中包含的较大目标。将$13\times13$大小的特征图输入一个卷积集中，得到更高层次的抽象特征，再由结构图所示的右边分支，将特征输入一个$3\times3$的卷积层，就能得到第一组预测结果。</p>
<p>第四种残差块输出大小为$26\times26$的特征图，它具有中等大小（$16\times16$）的感受野，适合用来检测图像中包含的中等大小的目标。为充分利用提取到的各类特征，对$13\times13$的特征图经过卷积集后得到的特征利用插值算法进行上采样，而输出大小为$26\times26$的特征图。将该特征图与残差块输出的特征图堆叠（Concatenate）到一起，再输入一个卷积集中和一个$3\times3$的卷积层，就能得到第二组预测结果。同理，第三种残差块输出的特征图（$52\times52$）具有的感受野最小（$8\times8$），适合用来检测图像中包含的小目标。对前面得到的特征进行上采样后，与残差块输出的特征图堆叠在一起，再经过一系列的卷积层就能得到第三组预测结果。</p>
<h2 id="Anchor-Box机制"><a href="#Anchor-Box机制" class="headerlink" title="Anchor Box机制"></a>Anchor Box机制</h2><p>YOLOv3和YOLOv2一样，仍采用了Faster R-CNN中提出的Anchor Box机制从特征图中检测各类目标。Anchor Box机制相当于给模型提供了更多了先验知识，拥有了这些先验知识，模型只要学会预测真实包围盒各参量相对于Anchor Box的偏移大小，而非Bounding Box的各个具体参数，该机制使模型的学习过程容易了很多。此外，YOLOv2中还使用了K-Means聚类算法对训练集中真实Bounding Box的大小进行聚类运算，这样得到几种Anchor Box尺寸要比原Faster R-CNN手工选取的尺寸具有更优的先验性。</p>
<p>使用K-Means对真实Bounding Box进行聚类的过程中，为减小误差，采用各Bounding Box之间的交并比（IOU）来代替传统的欧几里得距离作为聚类标准，具体的距离度量公式为：$$d(box,centrid)=1-IOU(box,centrid)$$</p>
<p>在YOLOv2中，利用了上述度量标准，从$K=1$开始对COCO数据集中所有的Bounding Box进行了多次K-Means运算，进行各方面权衡后，最终采用了$K=9$时聚类结果，得到了$9$种尺寸的Anchor Box。YOLOv2中最后只输出一个张量，因此直接将这$9$个Anchor Box运用到特征图的每一个网格上，即特征图中的每一个网格最后都要输出$9$个Bounding Box预测结果。YOLOv3中依然采用这些尺寸的Bounding Box，但把其中最大尺寸的$3$个Bounding Box分配到了$13\times13$大小的特征图上，中等尺寸大小的$3$个分配给大小为$26\times26$特征图，剩下的$3$个小尺寸Anchor Box则分配到$52\times52$大小的特征图上。Anchor Box的具体大小及分配情况见下表：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411114856.png" alt="Anchor Box分配表"></p>
<p>如图下图所示，将多尺寸预测与Anchor Box机制结合，不同大小的物体分别由不同特征图中的Anchor Box负责检测，这有效提高了目标的检测精度。</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411114911.png" alt="各特征图中的Anchor Box检测范围"></p>
<h2 id="结果输出"><a href="#结果输出" class="headerlink" title="结果输出"></a>结果输出</h2><p>YOLOv3检测模型输出的是三个尺度不一的张量，需要把它们进行拆分，才能得到想要的检测结果。从前面的流程图可知，在输入大小为$416\times416$的情况下，输出张量分别包含$13\times13$、$26\times26$、$52\times52$个网格。除网格数量外，这些张量的结构都是相同的，这里仅以其中大小为$13\times13$张量为例进行分析。</p>
<p>如下图，由于各尺度的特征图都使用了$3$种不同的Anchor Box来实现包围盒回归，因此输出张量的各网格中均包含了$3$个Bounding Box预测结果，各预测结果均由Bounding Box的$4$个位置（$x$、$y$、$w$、$h$）、$1$个置信度评分（bbox score）以及$C$类目标的$C$个存在概率值（classes probs）组成。作者使用了包含$80$类常见物体的COCO数据集来训练YOLOv3，因此一个预测结果是一个$85$维的向量，整个张量的深度则为$255$。</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411114929.png" alt="输出张量的结构示意图"></p>
<p>和YOLOv2中一样，为使Anchor Box机制下的模型易于训练，训练样本中各Bounding Box的真实参数都使用了强约束方法进行编码。用$G_x$、$G_y$、$G_w$、$G_h$来表示某个Bouding Box的实际中心坐标、长度及宽度，则它们的编码结果$t_x$、$t_y$、$t_w$、$t_h$将由下列公式求得：$$t_x  = G_x - c_x$$ $$t_y  = G_y - c_y$$ $$t_w  =  \log⁡ \frac{G_w}{p_w}$$    $$t_h  =  \log⁡\frac{G_h}{p_h}$$</p>
<p>其中$c_x$、$c_y$是把输入的图像划分为$S\times S$个网格后，某个目标的中心点所在网格的左上角坐标；$p_w$、$p_h$则表示Anchor Box的大小。因此，编码后得到的$t_x$、$t_y$表示的是Bounding Box的中心坐标相对于其所在网格的偏移量，$t_h$、$t_w$则表示Bounding Box的大小相对于Anchor Box的放缩尺度。使用这些编码后的数据作为训练样本的标记，YOLOv3模型将通过这些样本，学会如何通过微调Anchor Box的大小或位置来得到真实的Bounding Box参数。</p>
<p>因此，从输出的张量中拆分出各个结果后，还需要对它们进行解码。用$b_x$、$b_y$、$b_w$、$b_h$表示某个网格中某个Bounding Box的实际参数，由前面的编码公式，它们与其对应的预测值$t_x$、$t_y$、$t_w$、$t_h$之间存在如下转换关系：$$b_x  = \sigma(t_x) + c_x$$ $$b_y =\sigma(t_y) + c_y$$ $$b_w  = p_w e^(t_w)$$ $$ b_h  = p_h e^(t_h)$$</p>
<p>其中，$\sigma$表示是Sigmoid函数，用它来把$t_x$、$t_y$的值缩放到$0$和$1$之间。对$t_x$、$t_y$放缩后的结果再加上对应网格的左上角坐标$c_x$、$c_y$，就能得到预测的实际中心坐标$b_x$、$b_y$，Anchor Box的宽度$p_w$及高度$p_h$分别乘以它们对应的放缩值则得到预测的Bounding Box 大小$b_w$、$b_h$。具体的转换过程如下图：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411114946.png" alt="包围盒参数解码过程"></p>
<p>对于输出张量中的Bounding Box的置信度评分以及类别概率，将它们拆分出来后只需要使用Sigmoid函数将它们的值约束在0到1的范围内即可。把输出的三个张量全部拆分后，将得到多达10647个Bounding Box预测结果。对于这些结果，需要采用非极大值抑制滤除其中无效或冗余的Bounding Box，才能得到最终的检测结果。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>由于均方误差的优化过程较为简单，因此所有的YOLO算法都采用了由多个均方误差组成的损失函数。在YOLOv3中，用到的损失函数如下：$$\mathcal{L}= \frac12 \sum_{i=1}^{10647} \lambda_\text{obj}\cdot[(2 - w_i \cdot h_i) \cdot \sum_{r\in(x,y,w,h)}(r_i-\hat{r_i})^2 + \sum_{c\in classes}(p_i(c) - \hat{p_i} (c))^2] +(C_i - \hat{C_i})^2 $$</p>
<p>整个损失函数可被分为Bounding Box位置损失（$x_i$、$y_i$、$w_i$、$h_i$）、置信度评分损失（$C_i$）以及分类损失（$p_i(c)$）这三大部分。只有当网格中存在某类目标时，$\lambda_{obj}$的值才为$1$，否则$\lambda_{obj}$的值为$0$，即对于不存在目标的网格，只计算其置信度评分损失。考虑到Bounding Box的大小会造成损失计算上的不均衡，损失函数中增加的额外项$(2 - w_i\cdot h_i)$可以增加小Bounding Box的位置损失，同时降低大Bounding Box的损失，从而解决该问题。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37541097/article/details/81214953#commentBox">YOLO v3网络结构分析-CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/leviopku/article/details/82660381">yolo系列之yolo v3【深度解析】-CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/49995236">史上最详细的Yolov3边框预测分析-知乎专栏</a></li>
</ol>
<hr>
<p>更新历史：</p>
<ul>
<li>2019.6.20 完成初稿</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>YOLOv3模型详解</p><p><a href="https://hugsy.top/2019/06/18/ML/cnn_yolov3/">https://hugsy.top/2019/06/18/ML/cnn_yolov3/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hugsy</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-06-18</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-04-11</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/ML/">ML</a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210410180700.png" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210410180901.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/11/04/CS/sql_tutorial/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">SQL基础教程</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/05/04/ML/cnn_alexnet/"><span class="level-item">AlexNet的实现与应用</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "b4b6cfd565bc7810c10c19ce73f6c9f0",
            repo: "Picbed",
            owner: "Hugsy19",
            clientID: "a321e46c668dc3d86c4f",
            clientSecret: "0ff9e81865b09e7511ef7ef17bb9596834b8fd95",
            admin: ["Hugsy19"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            proxy: "https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token",
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/logo.svg" alt="Cornfield Chase" height="28"></a><p class="is-size-7"><span>&copy; 2022 Hugsy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>