<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>机器学习(2)：Logistic回归 - Cornfield Chase</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#B481BB"><meta name="application-name" content="Hugsy&#039;s Blog"><meta name="msapplication-TileImage" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><meta name="msapplication-TileColor" content="#B481BB"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hugsy&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="144x144" href="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><meta name="description" content="这里主要介绍Logistic回归模型及正则化。"><meta property="og:type" content="blog"><meta property="og:title" content="机器学习(2)：Logistic回归"><meta property="og:url" content="https://hugsy.top/2019/03/21/ML/machine_learning_2/"><meta property="og:site_name" content="Cornfield Chase"><meta property="og:description" content="这里主要介绍Logistic回归模型及正则化。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/5cff98001678862269.jpg"><meta property="article:published_time" content="2019-03-20T16:00:00.000Z"><meta property="article:modified_time" content="2022-06-08T14:40:37.673Z"><meta property="article:author" content="Hugsy"><meta property="article:tag" content="ML"><meta property="article:tag" content="Coursera"><meta property="article:tag" content="Machine Learning"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/5cff98001678862269.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hugsy.top/2019/03/21/ML/machine_learning_2/"},"headline":"机器学习(2)：Logistic回归","image":["https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/5cff98001678862269.jpg"],"datePublished":"2019-03-20T16:00:00.000Z","dateModified":"2022-06-08T14:40:37.673Z","author":{"@type":"Person","name":"Hugsy"},"description":"这里主要介绍Logistic回归模型及正则化。"}</script><link rel="canonical" href="https://hugsy.top/2019/03/21/ML/machine_learning_2/"><link rel="alternate" href="/atom.xml" title="Cornfield Chase" type="application/atom+xml"><link rel="icon" href="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?7b65ce26b5ae8dae153d7b4d53214ba4";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/logo.svg" alt="Cornfield Chase" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/tags/CS/">CS</a><a class="navbar-item" href="/tags/EE/">EE</a><a class="navbar-item" href="/tags/ML/">ML</a><a class="navbar-item" href="/tags/ME/">ME</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/5cff98001678862269.jpg" alt="机器学习(2)：Logistic回归"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-03-20T16:00:00.000Z" title="3/21/2019, 12:00:00 AM">2019-03-21</time>发表</span><span class="level-item"><time dateTime="2022-06-08T14:40:37.673Z" title="6/8/2022, 10:40:37 PM">2022-06-08</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Note/">Note</a></span><span class="level-item">21 分钟读完 (大约3088个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">机器学习(2)：Logistic回归</h1><div class="content"><p>这里主要介绍Logistic回归模型及正则化。</p>
<span id="more"></span>
<h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>在分类问题中，预测的值是离散的。例如要实现一个垃圾邮件分类器，那么该分类器最后输出的结果应该只有两种–是垃圾邮件或者不是，而不像前面学习过的回归问题那样，输出的结果是与输入一一对应的连续值。</p>
<p>分类问题中，最简单的是<strong>二分类（binary classification）</strong>问题，其分类结果只有两种，一般选其中一类作为<strong>正类（positive class）</strong>，并使其标记$y = 1$，另一类则作为**反类（negative class)<strong>，使其标记$y = 0$，因而$y \in \lbrace0, 1\rbrace$。除此之外还有</strong>多分类（multi-classfication)**问题，可以将它转化成多个二分类问题后用二分类问题的解决方法进行解决。</p>
<p>对分类问题，可建立对应的Logistic回归模型来解决。由于历史原因，Logistic回归虽然称为“回归”，却是一种分类问题的学习算法，要注意将其区分。</p>
<h2 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h2><h3 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h3><p>分类与前面讲过的回归只是在输出的值上有所区别，考虑继续用线性回归算法来解决分类问题。</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411165914.png" alt="线性回归"></p>
<p>如上图中的肿瘤分类器，由训练样本拟合相应的线性回归模型后，可以在中间设定一个阈值，使预测结果高于该阈值的输出为正类，低于则输出为反类，以此起到分类的效果。然而该模型很容易受到一些特殊情况的影响，而产生图中的蓝线所示的偏差较大的结果，此外线性回归模型的输出值不一定都在$0$到$1$之间，与分类问题的要求不符。</p>
<p>延用线性回归中的假设函数$h_\theta(x) = \theta^Tx$，且采用<strong>sigmoid函数</strong>将其值约束在$0$到$1$的范围之内。Sigmoid函数又称Logistic函数，其表达式及图像如下：$$g(z) = \frac{1}{1 + e^{-z}}$$</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411165944.png" alt="sigmoid函数"></p>
<p>由sigmoid函数的图像可值，其值一直保持在$0$到$1$之间，由此有了Logistic回归的假设函数：$$h_\theta(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}$$</p>
<p>$h_\theta(x)$的值在此被赋予了新的含义，它表示的是给定参数为$\theta$、输入为$x$的条件下标签$y=1$的概率，用数学表达式表示即为：$$h_\theta(x) = P(y=1|x;\theta)$$</p>
<p>例如对训练好的肿瘤分类器输入肿瘤的大小$x$后，输出的$h_\theta(x) = 0.7$，就表示分类器预测这种大小的肿瘤有$70%$的概率是恶性的（即$y = 1$）。</p>
<h3 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h3><p>有了适用于分类的成本函数$h_\theta(x)$，还需要设定一个阈值作为分类的标准。注意到，sigmoid函数有以下性质：</p>
<ul>
<li>$z \ge 0$时，$0.5 \le g(z) \lt 1$</li>
<li>$z \lt 0$时，$0 \lt g(z) \lt 0.5$</li>
</ul>
<p>进而有：</p>
<ul>
<li>$\theta^Tx \ge 0$时，$0.5 \le h_\theta(x) \lt 1$</li>
<li>$\theta^Tx \lt 0$时，$0 \lt h_\theta(x) \lt 0.5$</li>
</ul>
<p>因此可将$0.5$作为阈值，即使得：$$y = \begin{cases} 1,  &amp; h_\theta(x) \ge 0.5 \\ 0, &amp; h_\theta(x) \lt 0.5 \end{cases}$$</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411170001.png" alt="决策边界"></p>
<p>例如，由上图左边的训练样本，得到了右边成本函数中的各参数值，那么当$0.5 \le h_\theta(x)$时，有：$$ \theta^Tx  = -3 + x_1 + x_2 \ge 0$$<br>即：$$x_1 + x_2 \ge 3$$</p>
<p>该不等式取的是分界线以上的部分，也就是正类所在的区域。同理$h_\theta(x) \lt 0.5$时，将取图中的反类。而图中的那条分界线，称为两类的<strong>决策边界（decision boundary）</strong>，其位置由参数$\theta$决定。</p>
<p>此外，sigmoid函数还可以用于非线性函数而得到复杂的分类问题的决策边界：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411170015.png" alt="非线性决策边界"></p>
<h3 id="成本函数"><a href="#成本函数" class="headerlink" title="成本函数"></a>成本函数</h3><p>Logistic回归中假设函数$h_\theta(x)$为非线性函数，如继续使用均方误差作为成本函数，得到的将是非凸（non-convex）函数，难以找到最优解。因此在Logistic回归中，我们另设成本函数如下：$$J(\theta) = \frac{1}{m} \sum_{i=1}^m Cost(h_\theta(x^{(i)}), y)$$<br>其中：$$Cost(h_\theta(x), y) = \begin{cases} - \log(h_\theta(x)),&amp; y = 1 \\ - \log(1-h_\theta(x)), &amp; y = 0 \end{cases}$$</p>
<p>当$y = 1$时，该函数的图像为：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411170030.png" alt="成本函数图像1"></p>
<p>此时$h_\theta(x)=1$时成本为$0$，且$h_\theta(x) \to 0$时，成本趋近$\infty$。</p>
<p>反之，当$y = 0$时函数的图像为：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411170042.png" alt="成本函数图像2"></p>
<p>此时$h_\theta(x)=0$时成本为$0$，且$h_\theta(x) \to 1$时，成本趋近$\infty$。</p>
<p>以上成本函数，可以用一个式子更为简洁地表示：<br>$$Cost(h_\theta(x), y)=-y\log h_\theta(x^{(i)}) - (1-y)\log(1 - h_\theta(x^{(i)}))$$</p>
<p>该式与原式完全相等，它又被称为<strong>交叉熵（cross entropy）</strong>损失函数。</p>
<p>由此，分类问题中常用的成本函数$J(\theta)$的表达式为：$$J(\theta) = -\frac{1}{m} \sum_{i=1}^m [y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)})\log(1 - h_\theta(x^{(i)}))]$$</p>
<p>该函数也可以通过概率统计中常用的最大似然估计法推导出来，具体过程见<a target="_blank" rel="noopener" href="http://binweber.top/2017/09/12/deep_learning_1/">深度学习(1)：Logistic回归</a>。</p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>有了成本函数后，依然采用梯度下降法来将其最小化，以学习Logistic回归模型中的参数值。</p>
<p>下面对该成本函数的导数进行推导：</p>
<p>为方便计算，成本函数中的$\log$默认以$\exp$为底。展开其中的几项，有：$$ \log h_\theta(x^{(i)}) = \log \frac{1}{1 + e^{-\theta^Tx^{(i)}}} = -\log(1 + e^{-\theta^Tx^{(i)}}) $$ $$ \log(1 - h_\theta(x^{(i)})) = \log \frac{e^{-\theta^Tx^{(i)}}}{1 + e^{-\theta^Tx^{(i)}}} = \log e^{-\theta^Tx^{(i)}} - \log (1 + e^{-\theta^Tx^{(i)}})$$ $$ = -\theta^Tx^{(i)} - \log (1 + e^{-\theta^Tx^{(i)}})$$</p>
<p>由此：$$J(\theta) = -\frac{1}{m} \sum_{i=1}^m [-y^{(i)} \log(1 + e^{-\theta^Tx^{(i)}}) - (1 - y^{(i)})(\theta^Tx^{(i)} + \log (1 + e^{-\theta^Tx^{(i)}}))]$$ $$ = -\frac{1}{m}\sum_{i=1}^m [y^{(i)}\theta^Tx^{(i)} - \theta^Tx^{(i)} - \log (1 + e^{-\theta^Tx^{(i)}})] $$ $$ = -\frac{1}{m}\sum_{i=1}^m [y^{(i)}\theta^Tx^{(i)} - (\log e^{\theta^Tx^{(i)}} + \log (1 + e^{-\theta^Tx^{(i)}}))] $$ $$ = \frac{1}{m}\sum_{i=1}^m [\log (1 + e^{\theta^Tx^{(i)}}) - y^{(i)}\theta^Tx^{(i)}] $$</p>
<p>对其求导，有：$$ \frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m}\sum_{i=1}^m [\frac{\partial}{\partial \theta_j} \log (1 + e^{\theta^Tx^{(i)}}) - \frac{\partial}{\partial \theta_j} (y^{(i)}\theta^Tx^{(i)})] $$ $$ =  \frac{1}{m}\sum_{i=1}^m \frac{x^{(i)}<em>je^{\theta^Tx^{(i)}}}{1 + e^{\theta^Tx^{(i)}}} - y^{(i)}x^{(i)}_j$$ $$ = \frac{1}{m}\sum</em>{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j$$</p>
<p>从而梯度下降更新参数值的过程为：$$\begin{aligned} &amp; \text{Repeat} \ \lbrace \\ &amp; \ \ \ \ \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\ &amp; \rbrace \end{aligned}$$</p>
<p>除了梯度下降以外，还存在一些常用的较复杂的优化算法，如<strong>共轭梯度法（onjugate gradient）</strong>、<strong>BFGS</strong>、<strong>L-BFGS</strong>等，它们都可以用来代替梯度下降法进行参数学习。</p>
<h2 id="预防过拟合"><a href="#预防过拟合" class="headerlink" title="预防过拟合"></a>预防过拟合</h2><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>在机器学习中，训练出来的模型经常存在下面几种情况：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411170105.png" alt="回归问题"></p>
<p>对上面的回归问题，最为理想的情况是中间所示的模型，另外两种情况则分别称为：</p>
<ul>
<li><strong>欠拟合（underfitting）</strong>：模型与样本间存在较大的<strong>偏差（bias）</strong>，如上面的左图</li>
<li><strong>过拟合（overfitting）</strong>：模型与样本间存在较大的<strong>方差（variance）</strong>，如上面的右图</li>
</ul>
<p>用Logistic回归解决分类问题时，这些情况也是存在的：</p>
<p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411170128.png" alt="分类问题"></p>
<p>其中，过拟合将导致模型的<strong>泛化（generalization）</strong>能力不够强，而它很容易在训练样本包含的特征种类过多而训练样本的数量却相对较少时发生，因此常用来预防过拟合的方法有：</p>
<ul>
<li>通过人工筛选或模型选择算法丢弃部分训练样本的特征</li>
<li><strong>正则化（regularization）</strong>成本函数</li>
</ul>
<p>下面重点讲解其中的第二种方法。</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210411170138.png" alt="过拟合"></p>
<p>上面的右图所示的回归模型明显存在过拟合，要解决该问题，最简单的做法就是想办法使模型中的参数$\theta_3$、$\theta_4$尽可能地接近于$0$，而使其近似地等于左图中最为理想的模型。</p>
<p>进一步地，我们知道，参数$\theta$的值都是通过最小化成本函数而求得的。那么要达到上述目的，可以考虑将这几个参数“捆绑”在成本函数上：$$ J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + 1000\theta_3^2 + 1000\theta_4^2$$</p>
<p>这样，在最小化成本函数时，参数$\theta_3$、$\theta_4$将受到“惩罚”而一同被最小化，以此达到防止过拟合的目的。所谓的正则化过程也与此类似。</p>
<p>建立机器学习模型时，学习到的参数值较小，就意味着假设函数会是一个相对“简单”的函数，过拟合也更不容易发生。正则化的思想即在于此，其具体做法是在成本函数后加入正则化项，对线性回归模型为：$$J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \sum_{i=1}^n \theta_j^2$$</p>
<p>对Logistic回归模型则为：$$J(\theta) = -\frac{1}{m} \sum_{i=1}^m [y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)})\log(1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m} \sum_{i=1}^n \theta_j^2$$</p>
<p>其中$\lambda$为正则化参数，需选取适当的值，其值过大容易导致欠拟合；$\sum_{i=1}^n \theta_j^2$是最常用的正则化项，它被称为<strong>L2范数</strong>，另有<strong>L0、L1范数</strong>。正则化保留了所有的特征，而通过使所有参数值最小化来防止过拟合。</p>
<p>对线性回归或Logistic回归模型，用梯度下降法最小化正则化后的成本函数的过程均为：$$\begin{aligned} &amp; \text{Repeat}\ \lbrace \\ &amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\ &amp; \ \ \ \  \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2…n\rbrace \\ &amp; \rbrace \end{aligned}$$</p>
<p>由于$\theta_0$的值恒为$1$，不需要将它正则化，所以迭代过程种分成了两步。</p>
<p>另外，采用正规方程直接求解线性回归模型中的参数$\theta$时，进行正规化的表达式为：$$\theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty$$<br>其中$L$是个大小为$(n+1)\times(n+1)$的矩阵：$$L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \\ &amp; 1 &amp; &amp; &amp; \\ &amp; &amp; 1 &amp; &amp; \\ &amp; &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; &amp; 1 \\ \end{bmatrix}$$</p>
<p>前面提到过正规方程种$X^TX$为奇异矩阵或非方阵时，它将不存在逆矩阵。对正规方程进行正规化后，就不会出现这种情况，$X^TX + \lambda \cdot L$将一定是可逆的。</p>
<h2 id="编程作业"><a href="#编程作业" class="headerlink" title="编程作业"></a>编程作业</h2><p>见如下github链接，程序仅供参考:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/Hugsy19/Machine_Learning/tree/master/Ng_Machine_Learning/ex2">ex2-python版</a></li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/">Andrew Ng-Machine Learning-Coursera</a></li>
<li><a target="_blank" rel="noopener" href="https://study.163.com/course/introduction/1004570029.htm">吴恩达-机器学习-网易云课堂</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/jasonzzj/article/details/52017438">交叉熵代价函数(损失函数)及其求导推导-CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/ustbbsy/article/details/82497872">牛顿法 拟牛顿法DFP BFGS L-BFGS的理解-CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/20924039/answer/131421690">机器学习中常常提到的正则化到底是什么意思？-知乎</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/zouxy09/article/details/24971995/">机器学习中的范数规则化-CSDN</a></li>
</ol>
<p>注：本文涉及的图片及资料均整理翻译自Andrew Ng的Machine Learning课程及上述书籍、博客资料，版权归各作者所有。翻译整理水平有限，如有不妥的地方欢迎指出。</p>
<hr>
<p>更新历史：</p>
<ul>
<li>2019.03.24 完成初稿</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>机器学习(2)：Logistic回归</p><p><a href="https://hugsy.top/2019/03/21/ML/machine_learning_2/">https://hugsy.top/2019/03/21/ML/machine_learning_2/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Hugsy</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-03-21</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2022-06-08</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/ML/">ML</a><a class="link-muted mr-2" rel="tag" href="/tags/Coursera/">Coursera</a><a class="link-muted mr-2" rel="tag" href="/tags/Machine-Learning/">Machine Learning</a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210410180700.png" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/20210410180901.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/04/13/ML/machine_learning/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">机器学习笔记</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/03/02/ML/machine_learning_1/"><span class="level-item">机器学习(1)：线性回归</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "aab07f6fbe72d99a8ca3df67034828dd",
            repo: "Picbed",
            owner: "Hugsy19",
            clientID: "a321e46c668dc3d86c4f",
            clientSecret: "0ff9e81865b09e7511ef7ef17bb9596834b8fd95",
            admin: ["Hugsy19"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            proxy: "https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token",
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://raw.githubusercontent.com/Hugsy19/Picbed/master/img/logo.svg" alt="Cornfield Chase" height="28"></a><p class="is-size-7"><span>&copy; 2022 Hugsy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>